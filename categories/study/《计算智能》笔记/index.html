<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
<title>《计算智能》笔记 - 陈燊的博客</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



    <meta name="description" content="最优化算法广度优先搜索总是在某一深度上先搜索所有节点，之后搜索下一个深度的节点，能保证一定可以得到最优解，但需要生成大量节点，并且有可能导致组合爆炸，搜索效率低。">
<meta name="keywords" content="神经网络,搜索,滤波,采样,卡尔曼滤波,时序推理,贝叶斯网络">
<meta property="og:type" content="article">
<meta property="og:title" content="《计算智能》笔记">
<meta property="og:url" content="http://chenshen.xyz/categories/study/《计算智能》笔记/index.html">
<meta property="og:site_name" content="陈燊的博客">
<meta property="og:description" content="最优化算法广度优先搜索总是在某一深度上先搜索所有节点，之后搜索下一个深度的节点，能保证一定可以得到最优解，但需要生成大量节点，并且有可能导致组合爆炸，搜索效率低。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyycdn9i4sj20g50lg423.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyycixq9q0j20cv0l2q3t.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyycfkhg0bj20dh0crgms.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyycsembmaj20aa0es754.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNbRwgy1fxc5w71tl7j30cj0140si.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNbRwgy1fxc6ykc6q6j30wg0h8di6.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyybvs2sqvj20s107in00.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyybw7c8f7j20rq0b50ww.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyybwg3omvj20rn0han4p.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyybsw5se5j20ss0boaff.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNbRwgy1fxa4qkjai6j30wd0h7juy.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNbRwgy1fxa4pysw4jj30fq083mxi.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNbRwgy1fxa4r35c8yj30v10ipn10.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNbRwgy1fxa4p5xb3qj30ge09gt9h.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNbRwgy1fxa4o7nso1j30960693yq.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNbRwgy1fxa4ogzk3dj30m209gkd4.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNbRwgy1fxb60s6q2sj30if0cbmyc.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNbRwgy1fxb614xpuwj30i20hzwgo.jpg">
<meta property="og:image" content="http://cos.name/wp-content/uploads/2013/01/mcmc-algo-1.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydgwd0jpj20b002s746.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydgfrea7j208901imwy.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydg7i77fj20kt02774b.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydhtyntej20dy02t74q.jpg">
<meta property="og:updated_time" content="2019-09-04T13:39:51.685Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《计算智能》笔记">
<meta name="twitter:description" content="最优化算法广度优先搜索总是在某一深度上先搜索所有节点，之后搜索下一个深度的节点，能保证一定可以得到最优解，但需要生成大量节点，并且有可能导致组合爆炸，搜索效率低。">
<meta name="twitter:image" content="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyycdn9i4sj20g50lg423.jpg">





<link rel="icon" href="/images/favicon.png">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    
    
    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    

    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.png" alt="《计算智能》笔记" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item" href="/">主页</a>
                
                <a class="navbar-item" href="/archives">归档</a>
                
                <a class="navbar-item" href="/categories">分类</a>
                
                <a class="navbar-item" href="/tags">标签</a>
                
                <a class="navbar-item" href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-9-tablet is-9-desktop is-9-widescreen has-order-2 column-main"><div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-01-15T06:56:44.000Z">2019-01-15</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/study/">学习</a>
                </div>
                
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                《计算智能》笔记
            
        </h1>
        <div class="content">
            <h1 id="最优化算法"><a href="#最优化算法" class="headerlink" title="最优化算法"></a>最优化算法</h1><h2 id="广度优先搜索"><a href="#广度优先搜索" class="headerlink" title="广度优先搜索"></a>广度优先搜索</h2><p>总是在某一深度上先搜索所有节点，之后搜索下一个深度的节点，能保证一定可以得到最优解，但需要生成大量节点，并且有可能导致组合爆炸，搜索效率低。</p>
<a id="more"></a>
<h2 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h2><p>总是扩展深度大的节点，直到找到目标节点。在存储空间上渐进最优，但是对于一棵无穷树，可能永远找不到最优解。搜索效率从一定程度上取决于运气，总体来说不高。</p>
<h2 id="启发式搜索"><a href="#启发式搜索" class="headerlink" title="启发式搜索"></a>启发式搜索</h2><p>通过合适的启发函数f(n)=g(n)+h(n),h(n)尽量取下界最大值，可以通过生成比较少的节点求得最佳路径，搜索效率高。</p>
<h2 id="alpha​-beta剪枝"><a href="#alpha​-beta剪枝" class="headerlink" title="alpha​-beta剪枝"></a>alpha​-beta剪枝</h2><p><strong>优点：</strong>节约了机器开销，减少搜索范围，提高了搜索效率。</p>
<p><strong>缺点：</strong>严重依赖于算法的寻找顺序。</p>
<h2 id="模拟退火算法"><a href="#模拟退火算法" class="headerlink" title="模拟退火算法"></a>模拟退火算法</h2><p><strong>优点：</strong></p>
<ul>
<li>具有摆脱局部最优解的能力，能够找到目标函数的全局最小点，已被证明有渐进收敛性；</li>
<li>简单、通用、易实现；</li>
<li>具有并行性。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对参数（如初始温度）的依赖性较强；</li>
<li>优化过程长，效率不高。</li>
</ul>
<p><strong>流程图：</strong></p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyycdn9i4sj20g50lg423.jpg" alt></p>
<h2 id="蚁群算法"><a href="#蚁群算法" class="headerlink" title="蚁群算法"></a>蚁群算法</h2><p><strong>优点：</strong></p>
<ul>
<li>在求解性能上，具有很强的鲁棒性和搜索较优解的能力；</li>
<li>蚁群算法是一种基于种群的进化算法，具有<strong>并行性</strong>；</li>
<li>搜索过程采用分布式计算方式，大大提高了算法的计算能力和运行效率。</li>
<li>蚁群算法很容易与多种启发式算法结合，以解决陷入局部最优的问题。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>蚁群算法中<strong>初始信息素匮乏</strong>；</li>
<li>收敛速度慢、<strong>易陷入局部最优</strong>；</li>
<li>蚁群算法复杂度高，需要的搜索时间较长。</li>
</ul>
<p><strong>流程图：</strong></p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyycixq9q0j20cv0l2q3t.jpg" alt></p>
<h2 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h2><p><strong>优点：</strong></p>
<ul>
<li>算法独立于求解域，具有快速随机的搜索能力；</li>
<li>群体搜索算法，具有潜在的<strong>并行性</strong>，鲁棒性高；</li>
<li>搜索使用评价函数启发，过程简单；</li>
<li>适合于求解复杂的优化问题。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>编程实现较复杂，需要对问题进行编码和解码；</li>
<li>算法参数多，会影响解的品质，而目前参数的选择基本是依靠经验；</li>
<li>容易产生<strong>早熟收敛</strong>的问题，易于陷入局部最优解；</li>
<li>算法对初始种群的选择有一定的依赖性，能够结合一些启发算法进行改进。</li>
</ul>
<p><strong>流程图：</strong></p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyycfkhg0bj20dh0crgms.jpg" alt></p>
<h2 id="粒子群算法"><a href="#粒子群算法" class="headerlink" title="粒子群算法"></a>粒子群算法</h2><p><strong>优点：</strong></p>
<ul>
<li>具有相当快的逼近最优解的速度；</li>
<li>个体充分利用自身经验和群体经验调整自身的最优解；</li>
<li>无集中约束控制，具有很强的鲁棒性。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>数学基础薄弱</strong>，不能严格证明它在全局最优点上的收敛性；</li>
<li>容易产生早熟收敛，陷入<strong>局部最优</strong>，主要归咎于种群在搜索空间中多样性的丢失；</li>
<li>由于缺乏精密搜索方法的配合 ，最优往往得不到最优解。</li>
</ul>
<p><strong>流程图：</strong></p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyycsembmaj20aa0es754.jpg" alt></p>
<h1 id="时序逻辑推理"><a href="#时序逻辑推理" class="headerlink" title="时序逻辑推理"></a>时序逻辑推理</h1><h2 id="滤波"><a href="#滤波" class="headerlink" title="滤波"></a>滤波</h2><script type="math/tex; mode=display">
P(X_t|Y_{1:t})</script><p>即根据现在及以前的所有测量数据，估计当前的状态。在雨伞那个例子中，根据目前为止过去进屋的人携带雨伞的所有观察数据，计算今天下雨的概率，这就是滤波。</p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><script type="math/tex; mode=display">
P(X_{t+k}|Y_{1:t}), k> 0</script><p>即根据现在及现在以前的所有测量数据，估计未来某个时刻的状态。在雨伞的例子中，根据目前为止过去进屋的人携带雨伞的所有观察数据，计算从今天开始若干天后下雨的概率，这就是预测。</p>
<h2 id="平滑"><a href="#平滑" class="headerlink" title="平滑"></a>平滑</h2><script type="math/tex; mode=display">
P(X_k|Y_{1:t}), 0 < k < t</script><p>即根据现在及现在以前的所有测量数据，估计过去某个时刻的状态。在雨伞的例子中，意味着给定目前为止过去进屋的人携带雨伞的所有观察数据，计算过去某一天的下雨概率。</p>
<h2 id="最可能解释"><a href="#最可能解释" class="headerlink" title="最可能解释"></a>最可能解释</h2><script type="math/tex; mode=display">
arg \max_{x_{1:t}}P(X_{1:t}|Y_{1:t}))</script><p>即给出现在及现在以前的所有测量数据，找到最能最可能生成这些测量数据的状态序列。例如，如果前三天每天都出现雨伞，但第四天没有出现，最有可能的解释就是前三天下雨了，而第四天没下雨。最可能解释也被称为解码问题，在语音识别、机器翻译等方面比较有用，最典型的方法是隐马尔可夫模型。</p>
<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><script type="math/tex; mode=display">
P(Y_{1:t}|X_{1:t}, \lambda)</script><p>这里面的$\lambda$是指模型。这个公式意味着在该模型下，给定到目前为止的状态序列，计算输出特定观测序列的可能性。这其实是个评估问题，可以评估模型的好坏，概率越高，意味着模型越能反映观测序列与状态序列之间的联系，模型就越好。</p>
<h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><p>学习$\lambda $，也即状态转移概率和观测概率</p>
<script type="math/tex; mode=display">
P(X_{t+1}|X_{t}), P(Y_t|X_t)</script><p>学习的目的是根据历史数据得到合理的模型，一般是根据一个目标函数，对模型进行迭代更新，例如使（5）中要计算的值最大便可以作为一个目标。</p>
<h2 id="前向递归算法"><a href="#前向递归算法" class="headerlink" title="前向递归算法"></a>前向递归算法</h2><p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fxc5w71tl7j30cj0140si.jpg" alt></p>
<p>应用于<strong>滤波</strong>问题。</p>
<h2 id="前向-后向算法"><a href="#前向-后向算法" class="headerlink" title="前向-后向算法"></a>前向-后向算法</h2><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxc6ykc6q6j30wg0h8di6.jpg" alt="屏幕快照 2018-11-18 下午2.18.58"></p>
<p>应用于<strong>平滑</strong>问题。</p>
<h2 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h2><script type="math/tex; mode=display">
m_{t+1} = P(e_{t+1}|X_{t+1})\max_{X_t}(P(X_{t+1}|X_t) m_{1:t})</script><p>应用于<strong>最大可能解释</strong>问题。</p>
<h1 id="精确推理"><a href="#精确推理" class="headerlink" title="精确推理"></a>精确推理</h1><h2 id="Noisy-OR模型"><a href="#Noisy-OR模型" class="headerlink" title="Noisy-OR模型"></a>Noisy-OR模型</h2><p>对于其中一个变量依赖于 k 个父节点的噪声逻辑关系，可以用 O(k)而不是 O(2k)个参数来描述其完全条件概率表。</p>
<h2 id="变量消元算法"><a href="#变量消元算法" class="headerlink" title="变量消元算法"></a>变量消元算法</h2><p>变量消元算法保存了中间计算的结果，避免了重复计算，大大提高了计算效率。</p>
<h2 id="动态贝叶斯网络"><a href="#动态贝叶斯网络" class="headerlink" title="动态贝叶斯网络"></a>动态贝叶斯网络</h2><h1 id="近似推理（离散型）"><a href="#近似推理（离散型）" class="headerlink" title="近似推理（离散型）"></a>近似推理（离散型）</h1><p>离散型的近似推理，即在贝叶斯网络下的近似推理。</p>
<blockquote>
<p>各种采样算法的<strong>一致性估计</strong>。</p>
</blockquote>
<h2 id="直接采样"><a href="#直接采样" class="headerlink" title="直接采样"></a>直接采样</h2><p><strong>直接采样</strong>可用于计算联合概率分布。</p>
<p>算法流程为：</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyybvs2sqvj20s107in00.jpg" alt></p>
<h2 id="拒绝采样"><a href="#拒绝采样" class="headerlink" title="拒绝采样"></a>拒绝采样</h2><p><strong>拒绝采样</strong>可用于计算条件概率。先通过直接采样生成一定量的样本，然后拒绝掉与证据变量不一致的样本，再计算剩余样本中查询变量所占的比例。</p>
<p>算法流程为：</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyybw7c8f7j20rq0b50ww.jpg" alt></p>
<h2 id="似然加权"><a href="#似然加权" class="headerlink" title="似然加权"></a>似然加权</h2><blockquote>
<p><a href="https://pubweb.eng.utah.edu/~mccully/cs5300lw/" target="_blank" rel="noopener">Likelihood Weighting</a></p>
</blockquote>
<p>回顾拒绝采样的算法过程，一个疑问就是为什么不在采样过程中就指定证据变量的值呢？这便是<strong>似然加权（重要性采样）</strong>所解决的问题。在似然加权采样过程中，如果变量位于证据变量的子节点，那么就在指定证据变量的值的概率下进行采样；如果变量是证据变量的父节点，由于在采样过程中无法实现知道证据变量的值，但是证据变量的值对父节点的概率分布是有影响的，因此我们在计算到证据节点时，需要将父节点采样的值对应的概率分布加入权重中。如果证据节点的条件概率高，说明父节点的取值是大概率的，反之则是小概率的。</p>
<p>算法流程为：</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4ly1fyybwg3omvj20rn0han4p.jpg" alt></p>
<h2 id="粒子滤波"><a href="#粒子滤波" class="headerlink" title="粒子滤波"></a>粒子滤波</h2><p>动态贝叶斯网络和马尔可夫模型可以相互转移。当给定证据变量想要计算隐含变量的概率时，我们可以通过“无限”复制时间片到动态贝叶斯网络中，然后再采用维特比算法等精确推理的方法去计算条件概率。但是，当网络的状态变量非常多的时候，即使维特比算法等精确推理的方法可以达到“常数级”的计算效率，但计算效率依旧不尽人意。</p>
<p>在复杂的动态贝叶斯网络下，可以采用似然加权的方法来近似得到条件概率。在传统的似然加权采样中，每次只产生一个样本，最后通过统计符合证据的样本频率来近似得到概率。替代地，我们可以一次性处理N个样本，并将其前向传播处理。但是似然加权采样存在的明显问题就是如果证据变量处在下游，那么算法的精度就会受损，前面的状态变量的采样无法从后面的证据变量得到指导。因此随着N个样本向后传播，就会有不少样本逐渐与证据变量的相似性变低。一个明显的做法就是去除掉与证据变量相似性低的样本，使得保留下来的样本大部分都是与证据变量相似性高的样本。</p>
<p>以上的整个过程，就是所谓的<strong>粒子滤波算法</strong>在动态贝叶斯网络中的应用。完整算法流程为：</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyybsw5se5j20ss0boaff.jpg" alt></p>
<blockquote>
<p>粒子滤波和维特比算法的计算复杂度对比</p>
</blockquote>
<h1 id="近似推理（连续型）"><a href="#近似推理（连续型）" class="headerlink" title="近似推理（连续型）"></a>近似推理（连续型）</h1><h2 id="直接采样-1"><a href="#直接采样-1" class="headerlink" title="直接采样"></a>直接采样</h2><p>直接采样的思想是，通过对均匀分布采样，实现对任意分布的采样。因为均匀分布采样好猜，我们想要的分布采样不好采，那就采取一定的策略通过简单采取求复杂采样。<br>假设y服从某项分布p(y)，其累积分布函数CDF为h(y)，有样本z~Uniform(0,1)，我们令 z = h(y)，即 y = h(z)^(-1)，结果y即为对分布p(y)的采样。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxa4qkjai6j30wd0h7juy.jpg" alt="314331-a158633099c6fd5c-2"></p>
<p>直接采样的核心思想在与CDF以及逆变换的应用。在原分布p(y)中，如果某个区域[a, b]的分布较多，然后对应在CDF曲线中，[h(a), h(b)]的曲线斜率便较大。那么，经过逆变换之后，对y轴（z）进行均匀分布采样时，分布多的部分（占据y轴部分多）对应抽样得到的概率便更大，</p>
<p><strong>局限性</strong></p>
<p>实际中，所有采样的分布都较为复杂，CDF的求解和反函数的求解都不太可行。</p>
<h2 id="拒绝采样-1"><a href="#拒绝采样-1" class="headerlink" title="拒绝采样"></a>拒绝采样</h2><p>拒绝采样是由一个易于采样的分布出发，为一个难以直接采样的分布产生采样样本的通用算法。既然 p(x) 太复杂在程序中没法直接采样，那么便一个可抽样的分布 q(x) 比如高斯分布，然后按照一定的方法拒绝某些样本，达到接近 p(x) 分布的目的。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxa4pysw4jj30fq083mxi.jpg" alt="25225434-fd6db018b45d4152a09ea1de2b5304ad"></p>
<p><strong>计算步骤</strong></p>
<p>设定一个方便抽样的函数 q(x)，以及一个常量 k，使得 p(x) 总在 k*q(x) 的下方。（参考上图）</p>
<ul>
<li>x 轴方向：从 q(x) 分布抽样得到 a；</li>
<li>y 轴方向：从均匀分布（0, k*q(a)) 中抽样得到 u；</li>
<li>如果刚好落到灰色区域： u &gt; p(a), 拒绝， 否则接受这次抽样；</li>
<li>重复以上过程。</li>
</ul>
<p><strong>局限性</strong></p>
<ul>
<li>拒绝了太多的样本！随着证据变量个数的增多，与证据e相一致的样本在所有样本中所占的比例呈指数级下降，所以对于复杂问题这种方法是完全不可用的。</li>
<li>难以找到合适的k*q(a)，接受概率可能会很低。</li>
</ul>
<h2 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h2><p><strong>重要性采样（似然加权）</strong>主要是用于求一个复杂分布p(x)的<strong>均值</strong>，最后并没有得到样本。 </p>
<p>重要性采样的思想是借助一个易于采样的简单分布q(x)，对这个简单分布q(x)所得到的样本全部接受。但是以此得到的样本肯定不满足分布p(x)，因此需要对每一个样本附加相应的重要性权重。在重要性采样中，以p(x0)/q(x0)的值作为每个样本的权重。这样，当样本和分布p(x)相近时，对应的权重大；与分布p(x)相差过多时，对应的权重小。这个方法采样得到的是带有重要性权重的服从q(z)分布的样本，这个权重乘以样本之后的结果其实就是服从p(z)分布的。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fxa4r35c8yj30v10ipn10.jpg" alt="314331-0096a433b005eb92-2"></p>
<p>通过上述公式，我们可以知道重要性采样可以用于近似复杂分布的均值。</p>
<h2 id="吉布斯采样"><a href="#吉布斯采样" class="headerlink" title="吉布斯采样"></a>吉布斯采样</h2><p>假设有一个例子：E：吃饭、学习、打球；时间T：上午、下午、晚上；天气W：晴朗、刮风、下雨。样本（E，T，W）满足一定的概率分布。现要对其进行采样，如：打球+下午+晴朗。</p>
<p>问题是我们不知道p(E,T,W)，或者说，不知道三件事的联合分布。当然，如果知道的话，就没有必要用吉布斯采样了。但是，我们知道三件事的条件分布。也就是说，p(E|T,W), p(T|E,W), p(W|E,T)。现在要做的就是通过这三个已知的条件分布，再用Gibbs sampling的方法，得到联合分布。<br>具体方法：首先随便初始化一个组合,i.e. 学习+晚上+刮风，然后依条件概率改变其中的一个变量。具体说，假设我们知道晚上+刮风，我们给E生成一个变量，比如，学习→吃饭。我们再依条件概率改下一个变量，根据学习+刮风，把晚上变成上午。类似地，把刮风变成刮风（当然可以变成相同的变量）。这样学习+晚上+刮风→吃饭+上午+刮风。同样的方法，得到一个序列，每个单元包含三个变量，也就是一个马尔可夫链。然后跳过初始的一定数量的单元（比如100个），然后隔一定的数量取一个单元（比如隔20个取1个）。这样sample到的单元，是逼近联合分布的。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxa4p5xb3qj30ge09gt9h.jpg" alt="25225719-116e5550e9fd448f894cddc6b6ab02b4"></p>
<h2 id="蓄水池采样"><a href="#蓄水池采样" class="headerlink" title="蓄水池采样"></a>蓄水池采样</h2><p>蓄水池抽样（Reservoir Sampling ），即能够在o（n）时间内对n个数据进行等概率随机抽取，例如：从1000个数据中等概率随机抽取出100个。另外，如果数据集合的量特别大或者还在增长（相当于未知数据集合总量），该算法依然可以等概率抽样。</p>
<p><strong>算法步骤：</strong></p>
<ul>
<li>先选取数据流中的前k个元素，保存在集合A中；</li>
<li>从第j（k + 1 &lt;= j &lt;= n）个元素开始，每次先以概率p = k/j选择是否让第j个元素留下。若j被选中，则从A中随机选择一个元素并用该元素j替换它；否则直接淘汰该元素；</li>
<li>重复步骤2直到结束，最后集合A中剩下的就是保证随机抽取的k个元素。</li>
</ul>
<h2 id="MCMC算法"><a href="#MCMC算法" class="headerlink" title="MCMC算法"></a>MCMC算法</h2><blockquote>
<p><a href="https://blog.csdn.net/pipisorry/article/details/51373090" target="_blank" rel="noopener">随机采样和随机模拟：吉布斯采样Gibbs Sampling</a></p>
<p><a href="https://blog.csdn.net/zongzi13545329/article/details/63685816" target="_blank" rel="noopener">MCMC算法学习总结</a></p>
<p><a href="https://blog.csdn.net/Dark_Scope/article/details/78937731" target="_blank" rel="noopener">【重点】采样方法（二）MCMC相关算法介绍及代码实现</a></p>
</blockquote>
<p><strong>马氏链收敛定理</strong></p>
<p><strong>马氏链定理：</strong>如果一个非周期马氏链具有转移概率矩阵P,且它的任何两个状态是连通的，那么$\lim<em>{p\to\infty}P</em>{ij}^n$存在且与i无关，记$\lim<em>{p\to\infty}P</em>{ij}^n = \pi(j)$，我们有：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxa4o7nso1j30960693yq.jpg" alt="20180604161544272"></p>
<p>其中$\pi = [\pi(1), \pi(2), … , \pi(j), …], \sum_{i=0}^{\infty}\pi_i = 1, \pi$称为马氏链的平稳分布。</p>
<p>所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的。</p>
<p><strong>说明：</strong></p>
<ol>
<li>该定理中马氏链的状态不要求有限，可以是有无穷多个的；</li>
<li>定理中的“非周期“这个概念不解释，因为我们遇到的绝大多数马氏链都是非周期的；</li>
</ol>
<p><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fxa4ogzk3dj30m209gkd4.jpg" alt="20180604161729263"></p>
<p><strong>细致平稳条件</strong></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxb60s6q2sj30if0cbmyc.jpg" alt="012132441751392"></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxb614xpuwj30i20hzwgo.jpg" alt="012132476599223"></p>
<blockquote>
<p><strong>针对一个新的分布，如何构造对应的转移矩阵？</strong></p>
<p>对于一个分布$\pi(x)$，根据<strong>细致平稳条件</strong>，如果构造的转移矩阵P满足$\pi(i)P<em>{ij} = \pi(j)P</em>{ji}$，那么$\pi(x)$即为该马氏链的平稳分布，因此可以根据这个条件去构造转移矩阵。</p>
<p>通常情况下，初始的转移矩阵$P$一般不满足细致平稳条件，因此我们通过引入接受率构造出新的转移矩阵$P’$，使其和$\pi(x)$满足细致平稳条件。由此，我们便可以用任何转移概率矩阵（均匀分布、高斯分布）作为状态间的转移概率。</p>
<p>如果我们假设状态之间的转移概率是相同的，那么在算法实现时，接收率可以简单得用$\pi(j)/\pi(i)$表示。</p>
</blockquote>
<h2 id="Metropolis-Hastings采样"><a href="#Metropolis-Hastings采样" class="headerlink" title="Metropolis-Hastings采样"></a>Metropolis-Hastings采样</h2><p>对于给定的概率分布p(x),我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布， 于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x), 那么我们从任何一个初始状态x0出发沿着马氏链转移, 得到一个转移序列 x0,x1,x2,⋯xn,xn+1⋯,， 如果马氏链在第n步已经收敛了，于是我们就得到了 π(x) 的样本xn,xn+1⋯。</p>
<p><img src="http://cos.name/wp-content/uploads/2013/01/mcmc-algo-1.jpg" alt></p>
<p>在马尔科夫状态链中，每一个状态代表一个样本$x_n$，即所有变量的赋值情况。</p>
<p>通过分析MCMC源码，可以知道：假设状态间的转移概率相同，那么下一个样本的采样会依赖于上一个样本。假设上一个样本所对应的原始分布概率$\pi(x)$很小，那么下一个样本的接受率很大概率为1；反之如果上一个样本的原始分布概率$\pi(x)$很大，那么下一个样本会有挺大概率被拒绝。这样的机制保证了生成的样本服从分布$\pi(x)$。</p>
<p>从上述分析可以看出，假如初始状态的样本对应的分布概率很小，那么在算法刚开始运行时所产生的样本（即使是分布概率很小的样本）很大可能都会被接收，从而使得算法刚开始运行时采样的样本不满足原始分布$\pi(x)$。只要算法采样到分布概率大的样本（<strong>此时即为收敛！</strong>），那么之后所采样得到的样本就会基本服从原始分布。当然，从初始状态遍历到分布概率大的状态时需要运行一段时间，这段过程即为收敛的过程。MCMC算法在收敛之后，保证了在分布概率$\pi(x)$大的地方产生更多的样本，在分布概率$\pi(x)$小的地方产生较少的样本。</p>
<p>一个马尔可夫链需要经过多次的状态转移过程采用达到一个稳定状态，这时候采样才比较接近真实的分布。此过程称为<em>burn in</em>。一般可通过丢弃前面的N个采样结果来达到burn in。</p>
<p><strong>疑问</strong></p>
<ul>
<li><p>MCMC的收敛是什么意思？这个过程中是什么参数会更新导致收敛？如何确定何时收敛？</p>
<p>收敛过程没有参数会更新，收敛的思想类似于大数定理。应用MCMC算法采样时，初始的样本量少，服从的分布可能和复杂分布$\pi(x)$相差挺远，但随着状态转移数的增加（转移矩阵P的应用），根据上述定理的证明，最终的样本分布会逐渐服从复杂分布$\pi(x)$。</p>
</li>
<li><p>$\pi$是每个状态所对应的概率分布吗？如果是的话，初始选定一个状态后，这个$\pi$如何设定？或则在MCMC证明过程中，初始$\pi$的概率分布如何设置？</p>
<p>在MCMC的证明过程中，$\pi$是每个状态所对应的概率分布。证明中所给定的初始$\pi$应该只是为了证明无论初始样本符合什么分布，在经过一定数量的转移之后，得到的样本会服从复杂分布$\pi (x)$，在实际代码实现中，不用对这个$\pi$进行设定。</p>
</li>
</ul>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p><strong>1. 网络初始化：</strong></p>
<p>初始化权重为(0,1)内的随机数，并给定α和$\eta$，α取(0.1,0.4)，$\eta$取0.9左右。误差精度值$\varepsilon$。</p>
<p><strong>2. 数据加载：</strong></p>
<p>提供训练样本集${x<em>n, y_n}</em>{n-1}^{N}$。</p>
<p><strong>3. 前向传播：</strong></p>
<p>计算网络的实际输出以及各隐层单元的状态（即前向过程）。</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydgwd0jpj20b002s746.jpg" alt></p>
<p>上述的$I <em> { \mathrm { p } ^ { i } } ^ { ( l ) }$即为本层的输入，$O </em> { j } ^ { ( l - 1 ) }$为上一层的输出，L、S代表两层的神经元个数，其实L==S,f(x)为阈值函数。</p>
<p><strong>4. 反向传播：</strong></p>
<p>反向计算误差，由以下公式计算完成：</p>
<p>对于输出层，误差计算公式如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydgfrea7j208901imwy.jpg" alt></p>
<p>对于隐含层，误差计算公式如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydg7i77fj20kt02774b.jpg" alt></p>
<p>即对于隐含层，其误差取决于上一层的误差值，K、M、J分别为第二层，第三层，第一层的神经元的个数，其实K=M=J.</p>
<p><strong>5. 更新权重：</strong></p>
<p>修改权重和阈值</p>
<p><img src="https://ws1.sinaimg.cn/large/a92fa7d4gy1fyydhtyntej20dy02t74q.jpg" alt></p>
<p><strong>6. 中止判断：</strong></p>
<p>判断输出层误差是否满足$\varepsilon $的要求，若满足则则停止训练，否则转向（3）,继续执行。</p>
<h2 id="Hebb学习（无监督）"><a href="#Hebb学习（无监督）" class="headerlink" title="Hebb学习（无监督）"></a>Hebb学习（无监督）</h2><p>Hebb学习规则与“条件反射”机理一致，并且已经得到了神经细胞学说的证实。 </p>
<p>巴甫洛夫的条件反射实验：每次给狗喂食前都先响铃，时间一长，狗就会将铃声和食物联系起来。以后如果响铃但是不给食物，狗也会流口水。<br>受该实验的启发，Hebb的理论认为在同一时间被激发的神经元间的联系会被强化。比如，铃声响时一个神经元被激发，在同一时间食物的出现会激发附近的另一个神经元，那么这两个神经元间的联系就会强化，从而记住这两个事物之间存在着联系。相反，如果两个神经元总是不能同步激发，那么它们间的联系将会越来越弱。<br><strong>Hebb学习律</strong>可表示为：</p>
<script type="math/tex; mode=display">
W_{ij}(t+1)=W_{ij}(t)+a⋅y_i⋅y_j</script><p>其中$W<em>{ij}$表示神经元j到神经元i的连接权，$y_i$与$y_j$表示两个神经元的输出，a是表示学习速率的常数，如果$y_i$与$y_j$同时被激活，即$y_i$与$y_j$同时为正，那么$w</em>{ij}$将增大。如果$y<em>i$被激活，而$y_j$处于抑制状态，即$y_i$为正$y_j$为负，那么$w</em>{ij}$将变小。</p>
<h2 id="Delta学习规则（有监督）"><a href="#Delta学习规则（有监督）" class="headerlink" title="Delta学习规则（有监督）"></a>Delta学习规则（有监督）</h2><p>Delta学习规则是一种简单的有监督学习算法，该算法根据神经元的实际输出与期望输出差别来调整连接权，其数学表示如下： </p>
<script type="math/tex; mode=display">
W_{ij}(t+1)=W_{ij}(t)+a⋅(d_i−y_i)x_j(t)</script><p>其中$W<em>{ij}$表示神经元j到神经元i的连接权，$d_i$是神经元i的期望输出，$y_i$是神经元i的实际输出，$x_j$表示神经元j状态，若神经元j处于激活态则$x_j$为1，若处于抑制状态则$x_j$为0或-1（根据激活函数而定）。a是表示学习速度的常数。假设$x_i$为1，若$d_i$比$y_i$大，那么$W</em>{ij}$将增大，若$d<em>i$比$y_i$小，那么$W</em>{ij}$将变小。 </p>
<p>Detla规则简单来讲就是：<strong>若神经元实际输出比期望输出大，则减少输入为正的连接的权重，增大所有输入为负的连接的权重。反之，则增大所有输入为正的连接权的权重，减少所有输入为负的连接权的权重。</strong></p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><p><strong>基本问题：</strong></p>
<script type="math/tex; mode=display">
\begin{array} { c l } { \min _ { \gamma , w , b } } & { \frac { 1 } { 2 } \| w \| ^ { 2 } + C \sum _ { i = 1 } ^ { m } \xi _ { i } } \\ { \text { s.t. } } & { y ^ { ( i ) } \left( w ^ { T } x ^ { ( i ) } + b \right) \geq 1 - \xi _ { i } , \quad i = 1 , \ldots , m } \\ { } & { \xi _ { i } \geq 0 , \quad i = 1 , \ldots , m } \end{array}</script><p><strong>对偶问题：</strong></p>
<script type="math/tex; mode=display">
\begin{array} { c l } { \max _ { \alpha } } & { W ( \alpha ) = \sum _ { i = 1 } ^ { m } \alpha _ { i } - \frac { 1 } { 2 } \sum _ { i , j = 1 } ^ { m } y ^ { ( i ) } y ^ { ( j ) } \alpha _ { i } \alpha _ { j } \left\langle x ^ { ( i ) } , x ^ { ( j ) } \right\rangle } \\ { \text { s.t. } } & { 0 \leq \alpha _ { i } \leq C , \quad i = 1 , \ldots , m } \\ { } & { \sum _ { i = 1 } ^ { m } \alpha _ { i } y ^ { ( i ) } = 0 } \end{array}</script>
        </div>
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/tags/kalman-filter/">卡尔曼滤波</a>, <a class="has-link-grey -link" href="/tags/search/">搜索</a>, <a class="has-link-grey -link" href="/tags/temporal-reasoning/">时序推理</a>, <a class="has-link-grey -link" href="/tags/filter/">滤波</a>, <a class="has-link-grey -link" href="/tags/neural-network/">神经网络</a>, <a class="has-link-grey -link" href="/tags/bayesian-network/">贝叶斯网络</a>, <a class="has-link-grey -link" href="/tags/samples/">采样</a>
                </div>
            </div>
        </div>
        
        
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/alipay.png" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/wechat.png" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/categories/computer-vision/计算机视觉顶尖期刊和会议/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">计算机视觉顶尖期刊和会议</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/categories/study/《高级人工智能》笔记/">
                <span class="level-item">《高级人工智能》笔记</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="comment-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.4.1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.4.1/dist/gitalk.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
    var gitalk = new Gitalk({
        clientID: 'e5eff8aa39f563a204e5',
        clientSecret: '8adc001af75f722b72813ab2916f65f6b7ba3da0',
        id: 'fed2f958dbbe4531acdb232b56e4b2d7',
        repo: 'chenshen03.github.io',
        owner: 'chenshen03',
        admin: "chenshen03"
    })
    gitalk.render('comment-container')
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    
                        <img class="image is-128x128 has-mb-6" src="/images/avatar.png" alt="陈燊">
                    
                    
                    <p class="is-size-4 is-block">
                        陈燊
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Master student of Computer Science
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Xiamen, China</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div style="cursor: pointer;" onclick="window.open('/archives','_self')">
                    <p class="heading">
                        文章
                    </p>
                    <p class="title has-text-weight-normal">
                        74
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div style="cursor: pointer;" onclick="window.open('/categories','_self')">
                    <p class="heading">
                        分类
                    </p>
                    <p class="title has-text-weight-normal">
                        8
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div style="cursor: pointer;" onclick="window.open('/tags','_self')">
                    <p class="heading">
                        标签
                    </p>
                    <p class="title has-text-weight-normal">
                        38
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/chenshen03">
                关注我</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Github" href="https://github.com/chenshen03">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Cnblogs" href="https://www.cnblogs.com/cslaker">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Weibo" href="https://www.weibo.com/cstobeno1">
                
                <i class="fab fa-weibo"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Facebook" href="https://www.facebook.com/chenshen03">
                
                <i class="fab fa-facebook-f"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Zhihu" href="https://www.zhihu.com/people/chenshen03">
                
                <i class="fab fa-zhihu"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
<div class="card widget" id="toc">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                目录
            </h3>
            <ul class="menu-list"><li>
        <a class="is-flex" href="#最优化算法">
        <span class="has-mr-6">1</span>
        <span>最优化算法</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#广度优先搜索">
        <span class="has-mr-6">1.1</span>
        <span>广度优先搜索</span>
        </a></li><li>
        <a class="is-flex" href="#深度优先搜索">
        <span class="has-mr-6">1.2</span>
        <span>深度优先搜索</span>
        </a></li><li>
        <a class="is-flex" href="#启发式搜索">
        <span class="has-mr-6">1.3</span>
        <span>启发式搜索</span>
        </a></li><li>
        <a class="is-flex" href="#alpha​-beta剪枝">
        <span class="has-mr-6">1.4</span>
        <span>alpha​-beta剪枝</span>
        </a></li><li>
        <a class="is-flex" href="#模拟退火算法">
        <span class="has-mr-6">1.5</span>
        <span>模拟退火算法</span>
        </a></li><li>
        <a class="is-flex" href="#蚁群算法">
        <span class="has-mr-6">1.6</span>
        <span>蚁群算法</span>
        </a></li><li>
        <a class="is-flex" href="#遗传算法">
        <span class="has-mr-6">1.7</span>
        <span>遗传算法</span>
        </a></li><li>
        <a class="is-flex" href="#粒子群算法">
        <span class="has-mr-6">1.8</span>
        <span>粒子群算法</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#时序逻辑推理">
        <span class="has-mr-6">2</span>
        <span>时序逻辑推理</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#滤波">
        <span class="has-mr-6">2.1</span>
        <span>滤波</span>
        </a></li><li>
        <a class="is-flex" href="#预测">
        <span class="has-mr-6">2.2</span>
        <span>预测</span>
        </a></li><li>
        <a class="is-flex" href="#平滑">
        <span class="has-mr-6">2.3</span>
        <span>平滑</span>
        </a></li><li>
        <a class="is-flex" href="#最可能解释">
        <span class="has-mr-6">2.4</span>
        <span>最可能解释</span>
        </a></li><li>
        <a class="is-flex" href="#评估">
        <span class="has-mr-6">2.5</span>
        <span>评估</span>
        </a></li><li>
        <a class="is-flex" href="#学习">
        <span class="has-mr-6">2.6</span>
        <span>学习</span>
        </a></li><li>
        <a class="is-flex" href="#前向递归算法">
        <span class="has-mr-6">2.7</span>
        <span>前向递归算法</span>
        </a></li><li>
        <a class="is-flex" href="#前向-后向算法">
        <span class="has-mr-6">2.8</span>
        <span>前向-后向算法</span>
        </a></li><li>
        <a class="is-flex" href="#维特比算法">
        <span class="has-mr-6">2.9</span>
        <span>维特比算法</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#精确推理">
        <span class="has-mr-6">3</span>
        <span>精确推理</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Noisy-OR模型">
        <span class="has-mr-6">3.1</span>
        <span>Noisy-OR模型</span>
        </a></li><li>
        <a class="is-flex" href="#变量消元算法">
        <span class="has-mr-6">3.2</span>
        <span>变量消元算法</span>
        </a></li><li>
        <a class="is-flex" href="#动态贝叶斯网络">
        <span class="has-mr-6">3.3</span>
        <span>动态贝叶斯网络</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#近似推理（离散型）">
        <span class="has-mr-6">4</span>
        <span>近似推理（离散型）</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#直接采样">
        <span class="has-mr-6">4.1</span>
        <span>直接采样</span>
        </a></li><li>
        <a class="is-flex" href="#拒绝采样">
        <span class="has-mr-6">4.2</span>
        <span>拒绝采样</span>
        </a></li><li>
        <a class="is-flex" href="#似然加权">
        <span class="has-mr-6">4.3</span>
        <span>似然加权</span>
        </a></li><li>
        <a class="is-flex" href="#粒子滤波">
        <span class="has-mr-6">4.4</span>
        <span>粒子滤波</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#近似推理（连续型）">
        <span class="has-mr-6">5</span>
        <span>近似推理（连续型）</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#直接采样-1">
        <span class="has-mr-6">5.1</span>
        <span>直接采样</span>
        </a></li><li>
        <a class="is-flex" href="#拒绝采样-1">
        <span class="has-mr-6">5.2</span>
        <span>拒绝采样</span>
        </a></li><li>
        <a class="is-flex" href="#重要性采样">
        <span class="has-mr-6">5.3</span>
        <span>重要性采样</span>
        </a></li><li>
        <a class="is-flex" href="#吉布斯采样">
        <span class="has-mr-6">5.4</span>
        <span>吉布斯采样</span>
        </a></li><li>
        <a class="is-flex" href="#蓄水池采样">
        <span class="has-mr-6">5.5</span>
        <span>蓄水池采样</span>
        </a></li><li>
        <a class="is-flex" href="#MCMC算法">
        <span class="has-mr-6">5.6</span>
        <span>MCMC算法</span>
        </a></li><li>
        <a class="is-flex" href="#Metropolis-Hastings采样">
        <span class="has-mr-6">5.7</span>
        <span>Metropolis-Hastings采样</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#神经网络">
        <span class="has-mr-6">6</span>
        <span>神经网络</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#反向传播算法">
        <span class="has-mr-6">6.1</span>
        <span>反向传播算法</span>
        </a></li><li>
        <a class="is-flex" href="#Hebb学习（无监督）">
        <span class="has-mr-6">6.2</span>
        <span>Hebb学习（无监督）</span>
        </a></li><li>
        <a class="is-flex" href="#Delta学习规则（有监督）">
        <span class="has-mr-6">6.3</span>
        <span>Delta学习规则（有监督）</span>
        </a></li><li>
        <a class="is-flex" href="#支持向量机">
        <span class="has-mr-6">6.4</span>
        <span>支持向量机</span>
        </a></li></ul></li></ul>
        </div>
    </div>
</div>

    
        
    
    
        <div class="column-right-shadow  ">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            标签云
        </h3>
        <a href="/tags/CUDA/" style="font-size: 10px;">CUDA</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/Leetcode/" style="font-size: 10px;">Leetcode</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/POJ/" style="font-size: 20px;">POJ</a> <a href="/tags/人脸识别/" style="font-size: 10px;">人脸识别</a> <a href="/tags/fourier-transform/" style="font-size: 10px;">傅里叶变换</a> <a href="/tags/dynamic-planning/" style="font-size: 14.29px;">动态规划</a> <a href="/tags/kalman-filter/" style="font-size: 10px;">卡尔曼滤波</a> <a href="/tags/cnn/" style="font-size: 10px;">卷积神经网络</a> <a href="/tags/hash/" style="font-size: 15.71px;">哈希</a> <a href="/tags/image-feature/" style="font-size: 10px;">图像特征</a> <a href="/tags/graph/" style="font-size: 12.86px;">图论</a> <a href="/tags/mean-shift/" style="font-size: 10px;">均值漂移</a> <a href="/tags/reinforcement-learning/" style="font-size: 10px;">强化学习</a> <a href="/tags/rnn/" style="font-size: 10px;">循环神经网络</a> <a href="/tags/sort/" style="font-size: 10px;">排序</a> <a href="/tags/search/" style="font-size: 17.14px;">搜索</a> <a href="/tags/svm/" style="font-size: 11.43px;">支持向量机</a> <a href="/tags/unsupervised-hash/" style="font-size: 12.86px;">无监督哈希</a> <a href="/tags/temporal-reasoning/" style="font-size: 11.43px;">时序推理</a> <a href="/tags/时序推理，隐马尔可夫/" style="font-size: 10px;">时序推理，隐马尔可夫</a> <a href="/tags/naive-bayes/" style="font-size: 11.43px;">朴素贝叶斯</a> <a href="/tags/kernel-function/" style="font-size: 10px;">核函数</a> <a href="/tags/gradient-descent/" style="font-size: 10px;">梯度下降</a> <a href="/tags/deep-hash/" style="font-size: 11.43px;">深度哈希</a> <a href="/tags/filter/" style="font-size: 12.86px;">滤波</a> <a href="/tags/supervised-hash/" style="font-size: 11.43px;">监督哈希</a> <a href="/tags/object-track/" style="font-size: 10px;">目标跟踪</a> <a href="/tags/neural-network/" style="font-size: 18.57px;">神经网络</a> <a href="/tags/particle-filter/" style="font-size: 10px;">粒子滤波</a> <a href="/tags/nlp/" style="font-size: 10px;">自然语言处理</a> <a href="/tags/semantic-segmentation/" style="font-size: 10px;">语义分割</a> <a href="/tags/bayesian-network/" style="font-size: 10px;">贝叶斯网络</a> <a href="/tags/greedy-lgorithm/" style="font-size: 17.14px;">贪心算法</a> <a href="/tags/edge-detection/" style="font-size: 10px;">边缘检测</a> <a href="/tags/samples/" style="font-size: 11.43px;">采样</a> <a href="/tags/zero-shot-learning/" style="font-size: 10px;">零样本学习</a>
    </div>
</div>

        
            
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                分类
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/study/">
            <span class="level-start">
                <span class="level-item">学习</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">5</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/technique/">
            <span class="level-start">
                <span class="level-item">技术</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/machine-learning/">
            <span class="level-start">
                <span class="level-item">机器学习</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">11</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/deep-learning/">
            <span class="level-start">
                <span class="level-item">深度学习</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">10</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/algorithms/">
            <span class="level-start">
                <span class="level-item">算法与数据结构</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">23</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/computer-vision/">
            <span class="level-start">
                <span class="level-item">计算机视觉</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">10</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/software-engineering/">
            <span class="level-start">
                <span class="level-item">软件工程</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">6</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/essay/">
            <span class="level-start">
                <span class="level-item">随笔</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">8</span>
            </span>
        </a></li>
            </ul>
        </div>
    </div>
</div>
        
            
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.png" alt="《计算智能》笔记" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 陈燊&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv">
                共<span id="busuanzi_value_site_uv">0</span>个访客
                </span>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="GitHub" href="https://github.com/chenshen03">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/js/clipboard.js" defer></script>
    

    
    
    

    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>