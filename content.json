{"pages":[{"title":"关于","text":"我是陈燊，来自福建泉州，喜欢篮球、音乐、跑步和旅行，完美主义者，强迫癌晚期。 目前在厦门大学计算机专业读研，研究方向是计算机视觉和哈希检索。 科研小萌新一枚，希望各位大佬多多指教。 陈燊 研究生（计算机科学系） 厦门大学 信息科学与技术学院 MAC实验室 QQ: 675121663 Mobile: 15606911909 Email: chenshen.fj@foxmail.com 福建省厦门大学海韵校区行政楼7楼MAC实验室 邮编：351005","link":"/about/index.html"}],"posts":[{"title":"2018-2019学年研一上学期总结","text":"请输入博文的阅读密码:： Incorrect Password! No content to display! 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","link":"/categories/essay/2018-2019学年研一上学期总结/"},{"title":"CNNH","text":"[AAAI 2014] Supervised Hashing via Image Representation Learning [paper] [code]Rongkai Xia , Yan Pan, Hanjiang Lai, Cong Liu, Shuicheng Yan. Overcome之前的哈希方法，大都使用手工的图像特征（如GIST等）作为图像的特征表达， 但是这些手工特征是采用无监督的方式提取的，难以很好得保存原始图片的语义信息。而深度深度神经网络可以很好得表达图像特征信息，因此作者便提出了基于深度的哈希检索方法——CNNH。 Contribute 第一个采用深度神经网络解决哈希检索问题； 采用coordinate descent method学习数据集的近似哈希编码，效率高，计算速度快； 在学习哈希函数的同时可以得到图像的特征表达。 Algorithm论文中，作者提出了一种监督哈希方法——CNNH，可以同时学习到图像的特征表达以及哈希函数。 CNNH具有两个阶段，Stage 1将相似矩阵分解为低维的哈希矩阵H，得到每个样本对应的哈希编码。但是这个过程并没有学习到哈希函数，因此无法对新的图像进行哈希编码；Stage 2利用Stage 1得到的哈希编码，以及每个样本对应的类别标签作为ground-truth来训练网络，从而得到哈希函数。假设Stage 1的哈希编码有r个比特位，样本的类别数有c个，那么网络输出层的节点就共有(r + c)个输出节点。 CNNH的输入为原始的图片信息，假设网络已经训练完毕后，对于一张新的测试图片，CNNH输出层的前r个节点就是该图片所对应的哈希编码，后c个节点则表示了图片的类别信息。 Stage 1根据类别标签获得相似矩阵S，根据KSH中的结论，$H{i·}$和$H{j·}$的汉明距离与内积$H{i·}H{j·}^T $是一一对应的，因此可以得到如下目标优化函数： 其中，为了解决优化问题，令$H ∈ [−1,1]^{n×q}$&gt;。 之后，通过coordinate descent method方法每次单独对$H{i,j}$进行更新。为了确保每次更新的$H{ij}$不超过[-1, 1]的范围，对更新步长d加入以下操作。这个操作确保了在对$H{ij}$更新时，值不会超过[-1, 1]，如果d小于$H{ij}$与边界的距离，则取d，如果大于则取-1或1。 算法流程如下： Stage 2Stage 2利用Stage 1得到的哈希编码，以及每个样本对应的类别标签作为ground-truth来训练神经网络。 CNNH网络具有三个卷积层，分别有32, 64, 128 filters。输出层有（r + c）个节点，其中r为哈希编码的比特数，c为数据集的类别数。类别标签的引入使得网络具有了迁移学习的能力，能够更好得学习到图像的特征表达。在训练完毕后，全连接层所对应的值就是每张图片的特征向量。","link":"/categories/computer-vision/CNNH/"},{"title":"Iterative Quantization，ITQ","text":"Abstract针对大规模的图像检索问题，论文提出了一个高效的ITQ算法。该算法先将中心化后的数据映射到超立方体的顶点上，再通过优化过程寻找一个旋转矩阵，使得数据点经过旋转后，与超立方体的顶点数据具有最小的量化误差。ITQ算法涉及到了multi-class spectral clustering（不懂）以及Orthogonal Procrustes problem，且可以通过PCA（无监督）或CCA（监督）的方法事先对数据进行降维。该方法的实验结果优于大部分start-of-the-art方法。 Introduction一个高效的二值编码学习方法应具有以下特点：（1）码长足够短，内存才不会占用过大；（2）应该具有局部敏感性，即原始空间相似的两个数据点在二值空间也应具有较近的汉明距离；（3）学习和查询过程效率也足够高。 在很多哈希方法中，初始的操作都是对数据进行PCA降维。但是每个特征维度所具备的方差是不同的，高方差的特征方向往往具有更多的信息，如果对所有方向都进行相同的编码（亦或要求向量间正交），那么算法有时会具有更差的表现。SSH算法通过放松对hash函数的正交限制得到了不错的表现，而在ITQ算法中并没有显式得对hash函数添加正交限制。算法初始也是进行PCA降维，然后随机初始化一个旋转矩阵，通过最小化量化误差的过程寻找到矩阵矩阵$R^*$，从而得到最终的映射函数。 Unsupervised Code Learning接下来对文章用到的符号表示进行声明： $X∈R^{n*d}$，为数据矩阵，并且对其进行中心化预处理0 $B = sgn(XW)$，W为映射函数，sgn为符号函数，B表示X经过映射后的数据，在二值超立方体上的映射的数据。 令$V = XW$，算法的目标是最小化量化误差$||sgn(V) - V||$，并令$W^$表示最优解。假设R为旋转矩阵，因为旋转矩阵只改变方向不改变映射关系，因此$W = W^ R$依然为算法的最优解。据此可得到量化损失函数为： Q(B,R) = |B - VR|_F^{2}-------(1)在实验中，发现对旋转矩阵R进行随机初始化，可以得到不错的效果。接下来，我们利用ITQ算法进行k（通常为50）次迭代，迭代过程分为两个步骤： 固定R，对B进行更新 通过对$Q(B,R)$公式的推导，我们得到最小化$Q(B,R)$的过程等同于最大化$tr(BR^TV^T) = \\sum{i=1}^n \\sum{j=1}^cB{ij}V^*{ij}$，其中$V^* = VR$。显然，因为B的取值只有-1和1，为使得该式最大，应使得V为正值时对应的B为1，V为负值时对应的B为-1。此外，对原始数据$X$的尺度进行改变不会影响到B或R的最优值，因此我们实现对数据进行中心化处理对实验结果没有影响。 固定B，对R进行更新 在此情况下，目标公式（1）就变成了典型的Orthogonal Procrustes problem，该问题的求解过程可以参考wiki百科。在本文中，利用SVD分解，使得$B^TV = S\\Omega S^T$，再令$R = SS^T$，便可求解。 Evaluation of Unsupervised Code Learning数据集 CIFAR：包括64800个数据，11类。 Tiny image dataset：包括580000个数据，分别对应388个Internet search key words。是以二进制文件形式存储的，每个数据文件包括图片本身、与图片相关的元数据（文件名，使用的搜索引擎，排名等）、每个图片的Gist描述符。 评估方案 第一个评估方法使用欧几里得近邻。在该方法中没有用到类标签的信息。对于每一个查询点，将其与其他数据点的距离进行升序排序，对于距离最近的前50个数据点，只要距离小于一个事先设定的阙值，便认为查询的结果是正确的，否则即为错误的查询结果。然后通过计算错误和正确的比例来得到精度和召回率。 第二个评估方法使用类标签。对每个查询点，得到对应top500的图像数据，然后通过计算top500中类标签和查询点相同所占的比例得到精度，再结合数据集中和查询点标签相同的所有数据点的数量得到召回率。 论文比较了PCA-RR、PCA-ITQ等与其他baseline方法在两种评估尺度下的表现： 在CIFAR下的实验结果表明，在两种评估方案下，PCA-ITQ算法的表现基本都优于其他baseline。除了在256-bits时，SKLSH在第一种量度下的表现最好，但是SKLSH在第二种量度下的表现却很差。由此可以看出基于PCA的方法可以很好得保留原始数据的语义一致性。 显然，无监督学习的方法（目标函数直接对距离进行优化）在第一种评估尺度下表现优于监督学习的方法，而在第二种评估尺度下，监督学习有效得利用了类标签的信息，因此表现普遍优于无监督学习方法。因此实验结果表明，欧式距离更近并不等同于更一致的语义信息。 Leveraging Label InformationRR和ITQ可以利用任何基于正交的投影方法。PCA利用一种无监督的方法进行降维，而CCA结合了数据中的标签进行，进行有监督得降维，从而得到了更好的实验结果。 因为CIFAR的类标签时人为标注的，较为“clean”，而Tiny image dataset的数据是互联网自动产生的，较为“noisy”。从实验结果可以看出，利用clean数据训练的CCA-ITQ具有最优的表现，而利用noisy数据训练的CCA-ITQ同样也比PCA-ITQ得到了很大的提升。","link":"/categories/computer-vision/Iterative-Quantization，ITQ/"},{"title":"LeetCode-155","text":"题目Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) — Push element x onto stack. pop() — Removes the element on top of the stack. top() — Get the top element. getMin() — Retrieve the minimum element in the stack. Example: 12345678MinStack minStack = new MinStack();minStack.push(-2);minStack.push(0);minStack.push(-3);minStack.getMin(); --&gt; Returns -3.minStack.pop();minStack.top(); --&gt; Returns 0.minStack.getMin(); --&gt; Returns -2. 思路&amp;心得 题意大致就是模拟栈的实现，这题比较出戏的一点就是要求在线性时间内获得栈中的最小值。一共有两个方法，都是比较巧妙的。 方法一： 维护两个自己构造的”栈“对象， 在这里为两个数组stack和sm,其中sm的栈顶为当前stack中最小元素； 在push时要判断sm是否为空，如果为空或者非空但是栈顶元素大于等于插入值的 需要在sm中插入x； 在pop时，s的元素被删除了，那么sm中的也应该被删除； 通过这些操作维护sm能很巧妙在O（1）复杂度得到最小值。 方法二： 栈中的每个对象为(x, y)，其中x为元素值，y为当前对象处于栈顶时，栈中的最小元素； 在push的时候，令y为要push的值x和当前栈中最小值两个中的最小值； 在top和getMin方法中，直接根据需要返回栈顶的第一个或第二个元素即可。 代码方法一： 1234567891011121314151617181920212223242526272829303132333435363738class MinStack(object): def __init__(self): \"\"\" initialize your data structure here. \"\"\" self.stack = [] self.sm = [] def push(self, x): \"\"\" :type x: int :rtype: void \"\"\" self.stack.append(x) if not self.sm or self.sm[-1] &gt;= x: self.sm.append(x) def pop(self): \"\"\" :rtype: void \"\"\" if self.sm[-1] == self.stack[-1]: self.sm.pop() self.stack.pop() def top(self): \"\"\" :rtype: int \"\"\" return self.stack[-1] def getMin(self): \"\"\" :rtype: int \"\"\" return self.sm[-1] 方法二： 1234567891011121314151617181920212223242526272829303132333435363738394041424344class MinStack(object): def __init__(self): \"\"\" initialize your data structure here. \"\"\" self.stack = [] def push(self, x): \"\"\" :type x: int :rtype: void \"\"\" if not (self.stack): self.stack.append((x,x)) else: self.stack.append((x, min(x, self.stack[-1][1]))) def pop(self): \"\"\" :rtype: void \"\"\" if self.stack: self.stack.pop() else: return None def top(self): \"\"\" :rtype: int \"\"\" if self.stack: return self.stack[-1][0] return None def getMin(self): \"\"\" :rtype: int \"\"\" if self.stack: return self.stack[-1][1] return None","link":"/categories/algorithms/LeetCode-155/"},{"title":"Locality Sensitive Hashing, LSH","text":"LSH局部敏感（Locality Senstitive）：即空间中距离较近的点映射后发生冲突的概率高，空间中距离较远的点映射后发生冲突的概率低。 局部敏感哈希的基本思想类似于一种空间域转换思想，LSH算法基于一个假设，如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的它们也具有很高的相似度；相反，如果它们本身是不相似的，那么经过转换后它们应仍不具有相似性。 假设一个局部敏感哈希函数具有10个不同的输出值，而现在我们具有11个完全没有相似度的数据，那么它们经过这个哈希函数必然至少存在两个不相似的数据变为了相似数据。从这个假设中，我们应该意识到局部敏感哈希是相对的，而且我们所说的保持数据的相似度不是说保持100%的相似度，而是保持最大可能的相似度。 对于局部敏感哈希保持最大可能的相似度的这一点，我们也可以从数据降维的角度去考虑。数据对应的维度越高，信息量也就越大，相反，如果数据进行了降维，那么毫无疑问数据所反映的信息必然会有损失。哈希函数从本质上来看就是一直在扮演数据降维的角色。 Min-Hash定义：特征矩阵按行进行一个随机的置换后，第一个列值为1的行的行号。 对于两个数据$C_1$和$C_2$，在Min-Hashing方法中，hash值相等的概率等于这两个数据降维前的Jaccard相似度（两个集合的交比两个集合的并）。用公式描述即： Pr[h_\\pi(C_1) = h_\\pi (C_2)] = sim(C_1, C_2)每一个置换等同于一个hash函数，多个置换构成一个hash函数族。假设我们拥有n个hash函数，要求在原始空间相似的两个数据在hash之后得到的n个值均相等的条件过于苛刻，所得到的精确率是很高，但是同样的召回率也会非常低。因此，我们放松了要求，在n个hash函数划分为b个hash函数族，只要两个数据在某一个hash函数族的值均相等，就认为这两个数据相似。 在上述定义下，两个数据在低维空间相似的概率为：$1−(1−s^r)^b$。解释如下： 对于两个数据的任意一个函数族来说，这两个函数族值相同的概率是：$s^r$，其中s∈[0,1]是这两个文档的相似度d，r是函数族中的函数个数。 也就是说，这两个函数族不相同的概率是$1−s^r$ 这两个文档一共存在b个函数族，这bb个函数族都不相同的概率是$(1−s^r)^b$ 所以说，这b个函数族至少有一个相同的概率是$1-(1−s^r)^b$ 以上过程可以为一个简单的AND-OR逻辑，这个逻辑同样也应用于下述基于p稳定分布的LSH中。 E2LSH：p稳定分布定义：对于一个实数集R上的分布D，如果存在P&gt;=0，对任何n个实数v1,…,vn和n个满足D分布的变量X1,…,Xn，随机变量$\\sum_iv_ix_i$和$(\\sum_i|v_i|^{p})^{1/p}x$有相同的分布，其中$x$是服从D分布的一个随机变量，则称D为一个p稳定分布。 利用p稳定分布可以有效的近似高维特征向量，并在保证度量距离的同时，对高维特征向量进行降维，其关键思想是，产生一个d维的随机向量$X$，随机向量$X$中的每一维随机、独立得从p稳定分布中产生。对于一个d维的特征向量$V$，如定义，随机变量$X.V$具有和$(\\sum_i|v_i|^{p})^{1/p}X$一样的分布，因此可以用$X.V$表示向量$V$来估算$||V||_p$ 。 p-稳定 LSH通过涉入p稳定分布和点积的概念，实现了LSH算法在欧几里得空间下的直接应用，而不需要嵌入Hamming空间。p-stable LSH中，度量是欧几里得空间下的lp准则，即向量v1与v2的距离定义为||v1-v2||p，然后通过设定的哈希函数将原始点映射到直线的等长线段上，每条线段便相当于一个哈希桶，与LSH方法类似，距离较近的点映射到同一哈希桶（线段）中的概率大，距离较远的点映射到同一哈希桶中的概率小，正好符合局部敏感的定义。 hash函数：p稳定分布下的hash函数为$h_{x, b} (v) = \\left \\lfloor \\frac{x.v + b}{w} \\right \\rfloor$，用于将d维的特征向量映射到整数集。其中$x$为d维向量，每一维都独立取自于p稳定分布，b为[0，w]范围内的随机数。其作用效果图如下： 哈希表的设计：将哈希过后的向量直接存入hash表，占用内存又不便于查找。因此论文定义了额外两个hash函数： h_1(x_1, x_2, ..., x_k) = ((\\sum_{i=1}^{k} r_i a_i) mod C) mod tableSize h_2(x_1, x_2, ..., x_k) = ((\\sum_{i=1}^{k} r_i a_i) mod C)其中，h1的值作为哈希表索引，h2的值作为链表中的关键字，$r_1$和$r_2$为随机整数，$C$的取值为$2^{32} - 5$。 缺点LSH 典型的基于概率模型生成索引编码的结果并不稳定。虽然编码位数增加，但是查询准确率的提高确十分缓慢； 需要大量的存储空间，不适合于大规模数据的索引。 E2LSH E2LSH方法的目标是保证查询结果的准确率和查全率，并不关注索引结构需要的存储空间的大小； E2LSH使用多个索引空间以及多次哈希表查询，生成的索引文件的大小是原始数据大小的数十倍甚至数百倍。 疑问$\\sum_iv_ix_i$和$(\\sum_i|v_i|^{p})^{1/p}X$在满足相同分布的前提下，有什么特点？或则说如果$X$不符合p稳定分布，对结果有什么影响？ 向量距离的度量可以用范数表示，假设向量$V = V_1 - V_2$，那么$||V||_p$即表示向量$V_1$和$V_2$的原始空间下的p范数距离； 点乘的几何意义表示一个向量在另一个向量下的投影表示，因此$V_1.X - V_2.X = X.V = \\sum_iv_ix_i$表示的是向量$V_1$和$V_2$在向量$X$下投影的距离； 在$X$满足p稳定分布的前提下，可知（1）和（2）下的两个距离具有相同的分布，即满足了局部敏感的特点。 参考 LSH(Locality Sensitive Hashing)原理与实现 LSH和p-stable LSH LSH系列一：p稳定分布LSH算法初探 利用Minhash和LSH寻找相似的集合 【代码】min-hash和E2LSH","link":"/categories/computer-vision/Locality Sensitive Hashing, LSH/"},{"title":"Linux系统知识汇总","text":"系统相关静态IP地址配置 Ubuntu配置和修改IP地址 硬盘分区和挂载 parted分区和挂载及非交互式操作 Linux内核升级和降级内核升级 Linux升级内核的正确姿势 内核降级 Ubuntu 16.04 内核降级 清理缓存cache ubuntu 手动释放缓存 (清理内存cache) JupyterLab远程服务器访问 Ubuntu服务器JupyterNotebook配置与远程连接 多kernel设置 Jupyter lab 安装及多kernel配置 JuputerLab扩展 JupyterLab插件 显卡驱动cudnn安装 Ubuntu16.04上安装cudnn5.1详细教程 CUDA安装及解决图形界面冲突 Ubuntu16.04安装Nvidia显卡驱动 CUDA和cudnn版本查看cuda 版本cat /usr/local/cuda/version.txt cudnn 版本cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Tensorflow与CUDA、cudnn版本对应关系 Tensorflow不同版本要求与CUDA及CUDNN版本对应关系 实用工具zsh shell的最佳选择——zsh zsh全程指南 Aria2 ubuntu安装配置aria2 tldr linux 终端工具之tldr 远程桌面 Ubuntu 16.04 远程桌面 TODO [ ] 将所有博客内容整理到该博客下","link":"/categories/technique/Linux系统知识汇总/"},{"title":"MDP，马尔可夫决策","text":"离散状态的马尔科夫决策奖励因子r在马尔科夫决策中，有个奖励因子r，在计算总期望价值的时候，奖励因子r的次方数会逐步增加。对于这个的解释可以理解为：今天的一元钱在明天一般都会贬值。所以当某个状态s较晚到达时，要控制奖励因子使得获得的价值减少。 Bellman方程 V^{\\pi} = R(s) + \\gamma \\sum_{s^{'}\\epsilon S } P_{s\\pi(s)}(s^{'})V^\\pi (s^{'}) 假设有n个状态，则可以列出n个Bellman方程，且共含有n个未知量，那么就可以通过解这个线性方程组得到每个状态下对应的价值函数的值； 值迭代 政策迭代 对比假如状态有n个，政策迭代在计算时，需要计算含有n个方程和n个变量的方程组，当n≤1000时，政策迭代比较适用，当n&gt;1000时，值迭代效率会更高。 因为在政策迭代中，需要求解Bellman方程，当状态数变多时，就需要求解同等数量的方程，这是一个相当大的计算负荷，因此此时使用值迭代会更好。 转移概率和奖励因子的获取在实际情况中，对于MDP的五元组，转移概率常常是未知的。我们可以通过统计在每个状态下打到某个状态的转移次数来得到近似的转移概率。此外，有时状态s1无法转移到状态s2，为了避免出现0除的情况，可以用 1 / |S|替代其概率。 if R is unknown, we can also pick our estimate of the expected immediate reward R(s) in state s to be the average reward observed in state s. MDP求解全过程 连续状态的马尔可夫决策对于连续型的状态，可以设定一定量的区间，使其离散化，将连续型的MDP变成离散型的MDP来解决。但是离散化通常而言表现都不是很好，数据分布的多样性被消除了，因此就无法学习到更深层次的数据的潜在信息。离散化还可能导致出现维数灾难。 拟合值迭代算法 近似政策迭代算法","link":"/categories/machine-learning/MDP，马尔可夫决策/"},{"title":"Network in Network","text":"论文：Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013. 1×1卷积作用：1×1卷积核可以起到一个跨通道聚合的作用，所以进一步可以起到降维（或者升维）的作用，起到减少参数的目的。 比如当前层为 x*x*m即图像大小为x*x，特征层数为m，然后如果将其通过1×1的卷积核，特征层数为n，那么只要n&lt;m这样就能起到降维的目的，减少之后步骤的运算量（当然这里不太严谨，需要考虑1×1卷积核本身的参数个数为m×n个）。换句话说，如果使用1x1的卷积核，这个操作实现的就是多个feature map的线性组合，可以实现feature map在通道个数上的变化。 而因为卷积操作本身就可以做到各个通道的重新聚合的作用，所以1×1的卷积核也能达到这个效果。 MLP卷积层一般来说，如果我们要提取的一些潜在的特征是线性可分的话，那么对于线性的卷积运算来说这是足够了。然而一般来说我们所要提取的特征一般是高度非线性的。在传统的CNN中，也许我们可以用超完备的滤波器，来提取各种潜在的特征。比如我们要提取某个特征，于是就用了一大堆的滤波器，把所有可能的提取出来，这样就可以把想要提取的特征也覆盖到，然而这样存在一个缺点，那就是网络太恐怖了，参数太多了。 CNN高层特征其实是低层特征通过某种运算的组合。于是作者就根据这个想法，提出在每个局部感受野中进行更加复杂的运算，提出了对卷积层的改进算法：MLP卷积层。MLP层可以看成是每个卷积的局部感受野中还包含了一个微型的多层网络 Maxout层原先为： 现在为： 全局均值池化传统的卷积神经网络卷积运算一般是出现在低层网络。对于分类问题，最后一个卷积层的特征图通过量化然后与全连接层连接，最后在接一个softmax逻辑回归分类层。这种网络结构，使得卷积层和传统的神经网络层连接在一起。我们可以把卷积层看做是特征提取器，然后得到的特征再用传统的神经网络进行分类。 然而，全连接层因为参数个数太多，往往容易出现过拟合的现象，导致网络的泛化能力不尽人意。于是Hinton采用了Dropout的方法，来提高网络的泛化能力。 本文提出采用全局均值池化的方法，替代传统CNN中的全连接层。与传统的全连接层不同，我们对每个特征图一整张图片进行全局均值池化，这样每张特征图都可以得到一个输出。这样采用均值池化，连参数都省了，可以大大减小网络，避免过拟合，另一方面它有一个特点，每张特征图相当于一个输出特征，然后这个特征就表示了我们输出类的特征。这样如果我们在做1000个分类任务的时候，我们网络在设计的时候，最后一层的特征图个数就要选择1000。 参考 关于CNN中1×1卷积核和Network in Network的理解 深度学习（二十六）Network In Network学习笔记","link":"/categories/deep-learning/Network-in-Network/"},{"title":"PCA，主成分分析","text":"介绍主方向的概念是什么？为什么降低维度的方法是使方差最大化？ 假设某两个特征之间成线性关系，在二维平面上的表示就是数据点呈线性分布，那么可以通过将数据在主方向上进行投影，得到一个一维的数据，这个一维的数据保留了原始数据大部分的信息. 两个特征之间成线性关系，但是由于一些噪声的影响，所以数据分布并不严格处在一条直线上面。所谓的主方向就是要找到一个向量使得这个向量和预想中的直线大致符合。然后这样的一个向量应该满足什么要求呢？显然，应当使得所有的数据点到这个向量（直线）的距离总和最小，在数学表达上的体现就是尽可能使得每个点和主方向向量的协方差$x^{T}*u / m$最大，因为协方差变大时，x和u越正相关，那么x自然就距离直线更近。（有疑问） 通过拉格朗日方法求解，可知向量u即为$\\sum $矩阵的特征向量，假设需要降维到k惟，那么就要选择特征值大的前k的特征向量。而为什么选用特征值大的向量？因为特征值越大，说明对应的特征向量代表了这个矩阵的主要特征（主要方向）。 解出来的特征向量个数为多少？如何知道k的具体大小？ $\\sum $矩阵为实对称矩阵，因此特征向量两两正交，且特征向量个数一定有n个。 奇异值分解奇异值分解的含义是，把一个矩阵A看成线性变换（当然也可以看成是数据矩阵或者样本矩阵），那么这个线性变换的作用效果是这样的，我们可以在原空间找到一组标准正交基V，同时可以在像空间找到一组标准正交基U，我们知道，看一个矩阵的作用效果只要看它在一组基上的作用效果即可，在内积空间上，我们更希望看到它在一组标准正交基上的作用效果。而矩阵A在标准正交基V上的作用效果恰好可以表示为在U的对应方向上只进行纯粹的伸缩！这就大大简化了我们对矩阵作用的认识，因为我们知道，我们面前不管是多么复杂的矩阵，它在某组标准正交基上的作用就是在另外一组标准正交基上进行伸缩而已。","link":"/categories/machine-learning/PCA，主成分分析/"},{"title":"NINH","text":"[CVPR 2015] Simultaneous Feature Learning and Hash Coding with Deep Neural Networks [paper] Hanjiang Lai, Yan Pan, Ye Liu, Shuicheng Yan. Overcome 在大多数哈希方法中，图像都用手工特征表示，这些特征不能很好得保存原始图片的语义信息。 哈希方法大都分为编码和量化两个过程，基于手工向量的优化结果可能难以同时兼容编码和量化的过程，从而造成子优化问题。 CNNH是一个two-stage的深度哈希方法，利用神经网络同时学习了哈希函数和图像特征表达。在stage 1时，CNNH得到了近似哈希编码，并在stage 2利用这些近似哈希编码来训练得到图像的特征表达，但是这些训练得到的图像特征无法反过来指导近似哈希编码的训练，以得到更好的编码。 Contribute 提出了一种 “one-stage”的监督哈希检索方法，可以同时得到图像特征和哈希编码，并且特征信息和哈希编码可以相互改善，从而得到更好的结果。 设计了一种triplet ranking loss，可以很好得保存图像语义的相似信息。 AlgorithmNINH方法分为三个步骤：（1）通过一个卷积网络得到中间图像特征；（2）通过divide-and-encode模块得到每张图片的哈希编码；（3）得到图像对的triplet ranking loss，再经由反向传播更新网络。 Triplet Ranking Loss and Optimization输入为图像对$(I, I^-, I^+)$，其中$I$为查询图像，$I^-$为与$I$不相似的图像，$I^+$为与$I$相似的图像。这种输入形式可以更好得表达相似信息“与$I^-$相比，图像$I$与$I^+$更相似”。Triplet ranking hinge loss定义为: 为了便于优化，加入松弛技巧——将Hamming norm替换为L2 norm。得到新的损失函数如下： 损失函数关于$F(I)、F(I^-)、F(I^+)$的导数为： 因为损失函数的求导过程非常简单，因此可以容易结合进神经网络的反向传播过程中。 Shared Sub-Network with Stacked Convolution LayersShared Sub-Network使用相同的神经网络结构以及相同的参数得到triplet图像对中的每一张图像的特征表达。网络结构和参数如下表所示： 在该部分，一种替代的策略是对于图像$I$使用单独的网络结构和参数，而图像$I^-$和$I^+$使用相同的网络结构和参数。但是论文中的实验证明，对三个图像使用相同的网络结构和参数的表现会由于这种替代策略。 Divide-and-Encode ModuleDivide-and-Encode（DAE）通过Shared Sub-Network得到每张图片的特征向量后，将这个向量切片成r个部分，每个部分再通过全连接层连接到一个节点，最后通过sigmoid函数转化为[0, 1]之内的值。通过上述步骤，便可以得到一张图片r比特的哈希编码。在这个过程中，因为每个哈希编码是由特征向量的子部分得到的，哈希编码之间的冗余性少，有利于图像的检索。 这个部分另一种策略简称为FC，将整个特征向量通过全连接层直接连接到r个节点，再借由sigmoid函数得到图片的哈希编码。显然这种策略下，每一位哈希编码都利用到了整个特征向量的信息，容易造成冗余。论文的实验也证明了DAE的表现会由于FC。","link":"/categories/computer-vision/NINH/"},{"title":"POJ-1017","text":"题目DescriptionA factory produces products packed in square packets of the same height h and of the sizes 11, 22, 33, 44, 55, 66. These products are always delivered to customers in the square parcels of the same height h as the products have and of the size 6*6. Because of the expenses it is the interest of the factory as well as of the customer to minimize the number of parcels necessary to deliver the ordered products from the factory to the customer. A good program solving the problem of finding the minimal number of parcels necessary to deliver the given products according to an order would save a lot of money. You are asked to make such a program. InputThe input file consists of several lines specifying orders. Each line specifies one order. Orders are described by six integers separated by one space representing successively the number of packets of individual size from the smallest size 11 to the biggest size 66. The end of the input file is indicated by the line containing six zeros. OutputThe output file contains one line for each line in the input file. This line contains the minimal number of parcels into which the order from the corresponding line of the input file can be packed. There is no line in the output file corresponding to the last ``null’’ line of the input file. Sample Input0 0 4 0 0 17 5 1 0 0 00 0 0 0 0 0 Sample Output21 思路&amp;心得 题目大意为共有1 1、2 2…6 6的产品各a[i]个，包装袋大小固定为6 6，问最少最要多少个包装袋，可以把每个订单中所有产品包装起来。 这个题目有点类似硬币问题，在选择时从最大的6 6的产品开始依次往小进行计算。对于6 6、5 5、4 4的产品，各需要包装袋a[6]、a[5]、a[4]个，其中放置5 5产品的包装袋可以额外装11个1 1的产品，放置4 4产品的可以额外装5个2 2的产品；对于3 3的产品比较特殊，对4取模后，根据余数0、1、2、3分四种情况进行判断，每次选择时尽可能得装入更多的2 2的产品再装入1 1的产品；对于2 2和1 * 1的产品判断较为简单，不再多说。 在做这题时刚开始有点看不懂题意，便看了下discuss区，发现所有人都说这题非常非常难以及细节很多，导致不敢轻易下手。但是思路想清，WA了一两次之后，便AC了，好像也没想象中的那么难。 在网上观摩到大神的极短代码，算法思想非常精辟，特另附上，以供学习。 代码我的渣解法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include&lt;cstdio&gt;int a[7];int ans;void solve() { ans += (a[4] + a[5] + a[6]); a[1] -= a[5] * 11; a[2] -= a[4] * 5; if (a[2] &lt; 0) { a[1] += a[2] * 4; } ans += (a[3] / 4 + 1); switch (a[3] % 4) { case 0: { ans --; break; } case 1: { if (a[2] &gt; 0) { a[2] -= 5; if (a[2] &lt; 0) { a[1] += a[2] * 4; } a[1] -= 7; } else { a[1] -= 27; } break; } case 2: { if (a[2] &gt; 0) { a[2] -= 3; if (a[2] &lt; 0) { a[1] += a[2] * 4; } a[1] -= 6; } else { a[1] -= 18; } break; } case 3: { if (a[2] &gt; 0) { a[2] -= 1; a[1] -= 5; } else { a[1] -= 9; } break; } } if (a[2] &gt; 0) { ans += a[2] / 9; if (a[2] % 9 &gt; 0) { ans ++; a[1] -= (9 - a[2] % 9) * 4; } } if (a[1] &gt; 0) { ans += (a[1] + 35) /36; } printf(&quot;%d\\n&quot;, ans);}int main () { while (1) { ans = 0; for (int i = 1; i &lt;= 6; i ++) { scanf(&quot;%d&quot;, &amp;a[i]); } if (!a[1] &amp;&amp; !a[2] &amp;&amp; !a[3] &amp;&amp; !a[4] &amp;&amp; !a[5] &amp;&amp; !a[6]) break; solve(); } return 0;} 大神的精辟解法： 123456789101112131415161718192021#include&lt;stdio.h&gt;int main(){ int n,a,b,c,d,e,f,x,y; int u[4]={0,5,3,1}; while(1) { scanf(\"%d%d%d%d%d%d\",&amp;a,&amp;b,&amp;c,&amp;d,&amp;e,&amp;f); if(a==0&amp;&amp;b==0&amp;&amp;c==0&amp;&amp;d==0&amp;&amp;e==0&amp;&amp;f==0) break; n=d+e+f+(c+3)/4; y=5*d+u[c%4];//在已有n个的情况下，能装下y个2*2的 if(b&gt;y) n+=(b-y+8)/9;//把多的2*2的弄进来 x=36*n-36*f-25*e-16*d-9*c-4*b; if(a&gt;x) n+=(a-x+35)/36;//把1*1的弄进来 printf(\"%d\\n\",n); } return 0;}","link":"/categories/algorithms/POJ-1017/"},{"title":"POJ-1862 简单哈夫曼","text":"题目DescriptionOur chemical biologists have invented a new very useful form of life called stripies (in fact, they were first called in Russian - polosatiki, but the scientists had to invent an English name to apply for an international patent). The stripies are transparent amorphous amebiform creatures that live in flat colonies in a jelly-like nutrient medium. Most of the time the stripies are moving. When two of them collide a new stripie appears instead of them. Long observations made by our scientists enabled them to establish that the weight of the new stripie isn’t equal to the sum of weights of two disappeared stripies that collided; nevertheless, they soon learned that when two stripies of weights m1 and m2 collide the weight of resulting stripie equals to 2sqrt(m1m2). Our chemical biologists are very anxious to know to what limits can decrease the total weight of a given colony of stripies. You are to write a program that will help them to answer this question. You may assume that 3 or more stipies never collide together. InputThe first line of the input contains one integer N (1 &lt;= N &lt;= 100) - the number of stripies in a colony. Each of next N lines contains one integer ranging from 1 to 10000 - the weight of the corresponding stripie. OutputThe output must contain one line with the minimal possible total weight of colony with the accuracy of three decimal digits after the point. Sample Input3723050 Sample Output120.000 思路&amp;心得 很水的一道题。。。类似哈夫曼树的构造过程，可以用排序做，也可以用优先队列做。 代码1.排序12345678910111213141516171819202122232425262728#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;algorithm&gt;using namespace std;const int MAX_N = 105;int N;float num[MAX_N], s;void solve() { for (int i = 0; i &lt; N; i ++) { scanf(&quot;%f&quot;, &amp;num[i]); } sort(num, num + N); s = num[N - 1]; for (int i = N - 2; i &gt;= 0; i --) { s = 2 * sqrt(s * num[i]); } printf(&quot;%.3f\\n&quot;, s);} int main() { while (~scanf(&quot;%d&quot;, &amp;N)) { solve(); } return 0;} 2.优先队列1234567891011121314151617181920212223242526272829303132#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;queue&gt;using namespace std;int N;float num, t1, t2;priority_queue&lt;float&gt; que;void solve() { while (N --) { scanf(\"%f\", &amp;num); que.push(num); } while (que.size() &gt; 1) { t1 = que.top(); que.pop(); t2 = que.top(); que.pop(); num = 2 * sqrt(t1 * t2); que.push(num); } num = que.top(); que.pop(); printf(\"%.3f\\n\", num);} int main() { while (~scanf(\"%d\", &amp;N)) { solve(); } return 0;}","link":"/categories/algorithms/POJ-1862-简单哈夫曼/"},{"title":"POJ-1328 区间问题","text":"题目DescriptionAssume the coasting is an infinite straight line. Land is in one side of coasting, sea in the other. Each small island is a point locating in the sea side. And any radar installation, locating on the coasting, can only cover d distance, so an island in the sea can be covered by a radius installation, if the distance between them is at most d. We use Cartesian coordinate system, defining the coasting is the x-axis. The sea side is above x-axis, and the land side below. Given the position of each island in the sea, and given the distance of the coverage of the radar installation, your task is to write a program to find the minimal number of radar installations to cover all the islands. Note that the position of an island is represented by its x-y coordinates. Figure A Sample Input of Radar Installations InputThe input consists of several test cases. The first line of each case contains two integers n (1&lt;=n&lt;=1000) and d, where n is the number of islands in the sea and d is the distance of coverage of the radar installation. This is followed by n lines each containing two integers representing the coordinate of the position of each island. Then a blank line follows to separate the cases. The input is terminated by a line containing pair of zeros OutputFor each test case output one line consisting of the test case number followed by the minimal number of radar installations needed. “-1” installation means no solution for that case. Sample Input3 21 2-3 12 1 1 20 2 0 0 Sample OutputCase 1: 2Case 2: 1 思路&amp;心得 贪心算法与区间问题：以每个岛屿的坐标为中心，d为半径构造圆，该圆与X轴的两个交点即构成一个区间，若在这个区间上的任何雷达均可扫描到该岛屿。通过对n个岛屿进行处理，可得到n个区间，则问题转化成区间问题。 每个区间a[i]有两个端点：first和second，对区间数组a按second进行升序排序，然后从左向右扫描，对于每一个区间a[i]，若a[i].first小于之前选择的second的值，则不做任何处理，知道找到大于second的区间，然后进行下一个循环。 在数据输入的时候可进行特判，如若有岛屿的Y坐标大于d或则Y坐标&lt;0或则d&lt;0，则输入完成后直接返回-1即可。 数据定义记得用浮点型进行定义。 PS:在做贪心问题时，务必确定所做的贪心选择的正确性，在做这题时因为一开始的方向就是错误的，导致浪费了很多时间。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;algorithm&gt;#define MAX_SIZE 1005using namespace std;typedef pair&lt;double, double&gt; P;P a[MAX_SIZE];int n, ans;double x, y, d;bool cmp(P a, P b) { if (a.second &lt; b.second) return true; else return false;}int solve() { bool flag = true; double end; ans = 0; for (int i = 0; i &lt; n; i ++) { scanf(\"%lf %lf\", &amp;x, &amp;y); if (y &gt; d || y &lt; 0) flag = false; if (flag) { a[i].first = x - sqrt(d * d - y * y); a[i].second = x + sqrt(d * d - y * y); } } if (!flag || d &lt; 0) return -1; sort(a, a + n, cmp); for (int i = 0; i &lt; n; i ++) { if (flag) { end = a[i].second; ans ++; flag = false; continue; } if (a[i].first &gt; end) { flag = true; i --; } } return ans;}int main() { int step = 1; while (~scanf(\"%d %lf\", &amp;n, &amp;d)) { if (!n &amp;&amp; !d) break; printf(\"Case %d: %d\\n\", step ++, solve()); getchar(); } return 0;}","link":"/categories/algorithms/POJ-1328-区间问题/"},{"title":"POJ-2229","text":"题目DescriptionFarmer John commanded his cows to search for different sets of numbers that sum to a given number. The cows use only numbers that are an integer power of 2. Here are the possible sets of numbers that sum to 7:1) 1+1+1+1+1+1+12) 1+1+1+1+1+23) 1+1+1+2+24) 1+1+1+45) 1+2+2+26) 1+2+4Help FJ count all possible representations for a given integer N (1 &lt;= N &lt;= 1,000,000). InputA single line with a single integer, N. OutputThe number of ways to represent N as the indicated sum. Due to the potential huge size of this number, print only last 9 digits (in base 10 representation). Sample Input17 Sample Output16 思路&amp;心得 DP问题，先讨论下起始情况：当n = 1的时候，只有1种；当n = 2的时候，有两种； 当n为奇数时，将n - 1的所有情形都加上1即可以得到n时的个数。dp[n] = dp[n - 1]； 当n为偶数时，又分为两种情况：若式子含有1，则这种情况的个数为dp[n - 1]；若式子不包含1，即全都是偶数，则将所有加数都除以2，可得到个数为dp[n / 2]的值； 题目中要求的输出为实际值的最后九位数字，所以每次计算时要对1000000000取模。 代码12345678910111213141516171819202122#include&lt;stdio.h&gt;const int MAX_N = 1000005;const int mod = 1000000000;int dp[MAX_N];int f(int n) { for (int i = 3; i &lt;= n; i ++) { if (i &amp; 1) dp[i] = dp[i - 1]; else dp[i] = (dp[i - 1] + dp[i / 2] ) % mod; } return dp[n];}int main() { int N; dp[1] = 1, dp[2] = 2; while (~scanf(\"%d\", &amp;N)) { printf(\"%d\\n\", f(N)); } return 0;}","link":"/categories/algorithms/POJ-2229/"},{"title":"POJ-2376 区间问题","text":"题目DescriptionFarmer John is assigning some of his N (1 &lt;= N &lt;= 25,000) cows to do some cleaning chores around the barn. He always wants to have one cow working on cleaning things up and has divided the day into T shifts (1 &lt;= T &lt;= 1,000,000), the first being shift 1 and the last being shift T. Each cow is only available at some interval of times during the day for work on cleaning. Any cow that is selected for cleaning duty will work for the entirety of her interval.Your job is to help Farmer John assign some cows to shifts so that (i) every shift has at least one cow assigned to it, and (ii) as few cows as possible are involved in cleaning. If it is not possible to assign a cow to each shift, print -1. Input* Line 1: Two space-separated integers: N and T* Lines 2..N+1: Each line contains the start and end times of the interval during which a cow can work. A cow starts work at the start time and finishes after the end time. Output* Line 1: The minimum number of cows Farmer John needs to hire or -1 if it is not possible to assign a cow to each shift. Sample Input12343 101 73 66 10 Sample Output12 HintThis problem has huge input data,use scanf() instead of cin to read data to avoid time limit exceed.INPUT DETAILS:There are 3 cows and 10 shifts. Cow #1 can work shifts 1..7, cow #2 can work shifts 3..6, and cow #3 can work shifts 6..10.OUTPUT DETAILS:By selecting cows #1 and #3, all shifts are covered. There is no way to cover all the shifts using fewer than 2 cows. 思路&amp;心得 利用贪心算法，先按开始时间将所有数据进行排序，然后每次选择结束时间最大的即可 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;cstdio&gt;#include&lt;algorithm&gt;#define MAX_N 25005using namespace std;typedef pair&lt;int, int&gt; P;int N, T;int cnt;P a[MAX_N];bool cmp(P a, P b) { if (a.first &lt; b.first) return true; else return false;} int solve() { sort(a, a + N, cmp); bool found = false; int begin = 0, end = 0; for (int i = 0; i &lt; N; i ++) { if (a[i].first &lt;= begin + 1) { if (!found) found = true; if (a[i].second &gt; end) { end = a[i].second; if (end == T) return ++ cnt; } continue; } if (!found ) return -1; begin = end; found = false; cnt ++; i --; } return -1;}int main() { scanf(\"%d %d\", &amp;N, &amp;T); cnt = 0; for (int i = 0; i &lt; N; i ++) { scanf(\"%d %d\", &amp;a[i].first, &amp;a[i].second); } printf(\"%d\\n\", solve()); return 0;}","link":"/categories/algorithms/POJ-2376-区间问题/"},{"title":"POJ-2385","text":"题目DescriptionIt is a little known fact that cows love apples. Farmer John has two apple trees (which are conveniently numbered 1 and 2) in his field, each full of apples. Bessie cannot reach the apples when they are on the tree, so she must wait for them to fall. However, she must catch them in the air since the apples bruise when they hit the ground (and no one wants to eat bruised apples). Bessie is a quick eater, so an apple she does catch is eaten in just a few seconds. Each minute, one of the two apple trees drops an apple. Bessie, having much practice, can catch an apple if she is standing under a tree from which one falls. While Bessie can walk between the two trees quickly (in much less than a minute), she can stand under only one tree at any time. Moreover, cows do not get a lot of exercise, so she is not willing to walk back and forth between the trees endlessly (and thus misses some apples). Apples fall (one each minute) for T (1 &lt;= T &lt;= 1,000) minutes. Bessie is willing to walk back and forth at most W (1 &lt;= W &lt;= 30) times. Given which tree will drop an apple each minute, determine the maximum number of apples which Bessie can catch. Bessie starts at tree 1. Input Line 1: Two space separated integers: T and W Lines 2..T+1: 1 or 2: the tree that will drop an apple each minute. Output Line 1: The maximum number of apples Bessie can catch without walking more than W times. Sample Input7 22112211 Sample Output6 HintINPUT DETAILS: Seven apples fall - one from tree 2, then two in a row from tree 1, then two in a row from tree 2, then two in a row from tree 1. Bessie is willing to walk from one tree to the other twice. OUTPUT DETAILS: Bessie can catch six apples by staying under tree 1 until the first two have dropped, then moving to tree 2 for the next two, then returning back to tree 1 for the final two. 思路&amp;心得 定义dp[i][j]为在第i秒，移动j次获得的最大苹果数。在零时刻时，所有的dp项均为0。 当j为0时，有dp[i][j] = dp[i - 1][j] 当j不为0时，有dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - 1])，即在 i - 1 时刻时，均有两种选择，一种选择移动，一种选择不移动。 注意题目并不是在移动越多次能获得越多苹果。 代码12345678910111213141516171819202122232425262728#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;const int MAX_T = 1005;const int MAX_W = 35;int main() { int dp[MAX_T][MAX_W]; int num[MAX_T]; int T, W; scanf(\"%d %d\", &amp;T, &amp;W); for (int i = 1; i &lt;= T; i ++) { scanf(\"%d\", &amp;num[i]); } for (int i = 1; i &lt;= T; i ++) { for (int j = 0; j &lt;= W, j &lt;= i; j ++) { if (j == 0) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - 1]); if ((num[i] - (j &amp; 1) == 1)) dp[i][j] ++; } } int ans = dp[T][0]; for (int i = 1; i &lt;= W; i ++) { ans = max(ans, dp[T][i]); } printf(\"%d\\n\", ans); return 0;}","link":"/categories/algorithms/POJ-2385/"},{"title":"POJ-2718 全排列+暴力","text":"题目DescriptionGiven a number of distinct decimal digits, you can form one integer by choosing a non-empty subset of these digits and writing them in some order. The remaining digits can be written down in some order to form a second integer. Unless the resulting integer is 0, the integer may not start with the digit 0. For example, if you are given the digits 0, 1, 2, 4, 6 and 7, you can write the pair of integers 10 and 2467. Of course, there are many ways to form such pairs of integers: 210 and 764, 204 and 176, etc. The absolute value of the difference between the integers in the last pair is 28, and it turns out that no other pair formed by the rules above can achieve a smaller difference. InputThe first line of input contains the number of cases to follow. For each case, there is one line of input containing at least two but no more than 10 decimal digits. (The decimal digits are 0, 1, …, 9.) No digit appears more than once in one line of the input. The digits will appear in increasing order, separated by exactly one blank space. OutputFor each test case, write on a single line the smallest absolute difference of two integers that can be written from the given digits as described by the rules above. Sample Input1210 1 2 4 6 7 Sample Output128 思路&amp;心得 next_permutation()函数的用法：注意若要得到全排列，则数组应该为有序的 利用dfs + 回溯，再借以辅助数据visit可以找出一组数据的多种组合 代码12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;cstdio&gt;#include&lt;algorithm&gt;#define MAX 99999using namespace std;int nums[11];int t, len;char ch;void solve() { while (t --) { len = 0; while (1) { scanf(&quot;%d%c&quot;, &amp;nums[len ++], &amp;ch); if (ch == &apos;\\n&apos;) break; } if (len == 2) { printf(&quot;%d\\n&quot;, abs(nums[0] - nums[1])); continue; } int num1, num2, ans = MAX; int mid = len / 2; do { num1 = nums[0], num2 = nums[mid]; if (!num1 || !num2) continue; for (int i = 1; i &lt; mid; i ++) { num1 = num1 * 10 + nums[i]; } for (int i = mid + 1; i &lt; len; i ++) { num2 = num2 * 10 + nums[i]; } if (abs(num1 - num2) &lt; ans) ans = abs(num1 - num2); } while (next_permutation(nums, nums + len)); printf(&quot;%d\\n&quot;, ans); }}int main() { scanf(&quot;%d&quot;, &amp;t); solve(); return 0;}","link":"/categories/algorithms/POJ-2718-全排列-暴力/"},{"title":"POJ-2393 简单贪心水题","text":"题目DescriptionThe cows have purchased a yogurt factory that makes world-famous Yucky Yogurt. Over the next N (1 &lt;= N &lt;= 10,000) weeks, the price of milk and labor will fluctuate weekly such that it will cost the company C_i (1 &lt;= C_i &lt;= 5,000) cents to produce one unit of yogurt in week i. Yucky’s factory, being well-designed, can produce arbitrarily many units of yogurt each week. Yucky Yogurt owns a warehouse that can store unused yogurt at a constant fee of S (1 &lt;= S &lt;= 100) cents per unit of yogurt per week. Fortuitously, yogurt does not spoil. Yucky Yogurt’s warehouse is enormous, so it can hold arbitrarily many units of yogurt. Yucky wants to find a way to make weekly deliveries of Y_i (0 &lt;= Y_i &lt;= 10,000) units of yogurt to its clientele (Y_i is the delivery quantity in week i). Help Yucky minimize its costs over the entire N-week period. Yogurt produced in week i, as well as any yogurt already in storage, can be used to meet Yucky’s demand for that week. Input Line 1: Two space-separated integers, N and S. Lines 2..N+1: Line i+1 contains two space-separated integers: C_i and Y_i. Output Line 1: Line 1 contains a single integer: the minimum total cost to satisfy the yogurt schedule. Note that the total might be too large for a 32-bit integer. Sample Input4 588 20089 40097 30091 500 Sample Output126900 HintOUTPUT DETAILS:In week 1, produce 200 units of yogurt and deliver all of it. In week 2, produce 700 units: deliver 400 units while storing 300 units. In week 3, deliver the 300 units that were stored. In week 4, produce and deliver 500 units. 思路&amp;心得 这题确实有点水，数据也很弱，比较的应该不只是相邻两周的数据，之后的应该也要比较，但是两种代码都能AC，这就有趣了。。。 我的代码比较麻烦点，还有更简单复杂度更小为O(N)的解法，扫描时维护一个min值就行了，每次进入循环时自加S，然后和当前周的Y进行比较取其中最小的即可。 代码解法一： 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;cstdio&gt;#include&lt;algorithm&gt;#define MAX_N 10005using namespace std;struct Week { int C; int Y; int visit;} W[MAX_N];int N, S;long long cost;void solve() { cost = 0; for (int i = 0; i &lt; N; i ++) { scanf(\"%d %d\", &amp;W[i].C, &amp;W[i].Y); } for (int i = 0; i &lt; N; i ++) { if (!W[i].visit) { cost += W[i].C * W[i].Y; W[i].visit = 1; for (int j = i + 1; j &lt; N; j ++) { if ((W[i].C + (j - i) * S) &lt;= W[j].C) { cost += (W[i].C + (j - i) * S) * W[j].Y; W[j].visit = 1; } else { break; } } } } printf(\"%lld\\n\", cost);}int main() { scanf(\"%d %d\", &amp;N, &amp;S); solve(); return 0;} 解法二： 1234567891011121314151617181920#include &lt;cstdio&gt;typedef long long ll;struct AC{ ll c,y;}r[11000];ll n,s;int main(){ scanf(\"%d%d\",&amp;n,&amp;s); for (int i=1;i&lt;=n;i++) scanf(\"%d%d\",&amp;r[i].c,&amp;r[i].y); ll ans=0,min=1&lt;&lt;30; for (int i=1;i&lt;=n;i++){ min+=s; if (min&gt;r[i].c) min=r[i].c; ans+=min*r[i].y; } printf(\"%lld\",ans); return 0;}","link":"/categories/algorithms/POJ-2393-简单贪心水题/"},{"title":"POJ-2718 贪心+枚举","text":"题目DescriptionGiven a number of distinct decimal digits, you can form one integer by choosing a non-empty subset of these digits and writing them in some order. The remaining digits can be written down in some order to form a second integer. Unless the resulting integer is 0, the integer may not start with the digit 0. Given a number of distinct decimal digits, you can form one integer by choosing a non-empty subset of these digits and writing them in some order. The remaining digits can be written down in some order to form a second integer. Unless the resulting integer is 0, the integer may not start with the digit 0.For example, if you are given the digits 0, 1, 2, 4, 6 and 7, you can write the pair of integers 10 and 2467. Of course, there are many ways to form such pairs of integers: 210 and 764, 204 and 176, etc. The absolute value of the difference between the integers in the last pair is 28, and it turns out that no other pair formed by the rules above can achieve a smaller difference. InputThe first line of input contains the number of cases to follow. For each case, there is one line of input containing at least two but no more than 10 decimal digits. (The decimal digits are 0, 1, …, 9.) No digit appears more than once in one line of the input. The digits will appear in increasing order, separated by exactly one blank space. OutputFor each test case, write on a single line the smallest absolute difference of two integers that can be written from the given digits as described by the rules above. Sample Input1210 1 2 4 6 7 Sample Output128 思路&amp;心得 贪心：根据题目特点，选择不同情况下的最优解 枚举：枚举多种局部最优解，然后求出符合题意的最值 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include&lt;cstdio&gt;#include&lt;algorithm&gt;#define MAX 99999using namespace std;int nums[11];char ch;int solve() { int len = 0; while (1) { scanf(\"%d%c\", &amp;nums[len ++], &amp;ch); if (ch == '\\n') break; } if (len == 2) { return abs(nums[0] - nums[1]); } int x = 0, y = 0; int mid; if (len % 2 != 0) { mid = len / 2; if (!nums[0]) swap(nums[0], nums[1]); for (int i = 0; i &lt; mid + 1; i ++) { x = x * 10 + nums[i]; } for (int i = len - 1; i &gt; mid; i --) { y = y * 10 + nums[i]; } return x - y; } else { int index, cnt = 0, ans = MAX; int min_XY = 11; for (int i = 1; i &lt; len; i ++) { if (min_XY &gt;= nums[i] - nums[i - 1] &amp;&amp; nums[i] &amp;&amp; nums[i - 1]) { min_XY = nums[i] - nums[i - 1]; x = nums[i], y = nums[i - 1]; for (cnt = 0, index = 0; index &lt; len, cnt &lt; (len - 2) / 2; index ++) { if (index != i &amp;&amp; index != i - 1) { x = x * 10 + nums[index]; cnt ++; } } mid = index; for (cnt = 0, index = len - 1; index &gt;= mid, cnt &lt; (len - 2) / 2; index --) { if (index != i &amp;&amp; index != i - 1) { y = y * 10 + nums[index]; cnt ++; } } if (ans &gt; (x - y)) ans = x - y; } } return ans; } }int main() { int t; scanf(\"%d\", &amp;t); while (t --) { printf(\"%d\\n\", solve()); } return 0;}","link":"/categories/algorithms/POJ-2718-贪心-枚举/"},{"title":"POJ-3009 DFS+回溯","text":"题目DescriptionOn Planet MM-21, after their Olympic games this year, curling is getting popular. But the rules are somewhat different from ours. The game is played on an ice game board on which a square mesh is marked. They use only a single stone. The purpose of the game is to lead the stone from the start to the goal with the minimum number of moves. Fig. 1 shows an example of a game board. Some squares may be occupied with blocks. There are two special squares namely the start and the goal, which are not occupied with blocks. (These two squares are distinct.) Once the stone begins to move, it will proceed until it hits a block. In order to bring the stone to the goal, you may have to stop the stone by hitting it against a block, and throw again. Fig. 1: Example of board (S: start, G: goal) The movement of the stone obeys the following rules: At the beginning, the stone stands still at the start square. The movements of the stone are restricted to x and y directions. Diagonal moves are prohibited. When the stone stands still, you can make it moving by throwing it. You may throw it to any direction unless it is blocked immediately(Fig. 2(a)). Once thrown, the stone keeps moving to the same direction until one of the following occurs: The stone hits a block (Fig. 2(b), (c)). The stone stops at the square next to the block it hit. The block disappears. The stone gets out of the board. The game ends in failure. The stone reaches the goal square. The stone stops there and the game ends in success. You cannot throw the stone more than 10 times in a game. If the stone does not reach the goal in 10 moves, the game ends in failure. Fig. 2: Stone movements Under the rules, we would like to know whether the stone at the start can reach the goal and, if yes, the minimum number of moves required. With the initial configuration shown in Fig. 1, 4 moves are required to bring the stone from the start to the goal. The route is shown in Fig. 3(a). Notice when the stone reaches the goal, the board configuration has changed as in Fig. 3(b). Fig. 3: The solution for Fig. D-1 and the final board configuration InputThe input is a sequence of datasets. The end of the input is indicated by a line containing two zeros separated by a space. The number of datasets never exceeds 100. Each dataset is formatted as follows. the width(=w) and the height(=h) of the board the width(=w) and the height(=h) of the boardFirst row of the board the width(=w) and the height(=h) of the boardFirst row of the board… the width(=w) and the height(=h) of the boardFirst row of the board…h-th row of the board The width and the height of the board satisfy: 2 &lt;= w &lt;= 20, 1 &lt;= h &lt;= 20. Each line consists of w decimal numbers delimited by a space. The number describes the status of the corresponding square. 0 vacant square 1 block 2 start position 3 goal position The dataset for Fig. D-1 is as follows: 6 6 6 61 0 0 2 1 0 6 61 0 0 2 1 01 1 0 0 0 0 6 61 0 0 2 1 01 1 0 0 0 00 0 0 0 0 3 6 61 0 0 2 1 01 1 0 0 0 00 0 0 0 0 30 0 0 0 0 0 6 61 0 0 2 1 01 1 0 0 0 00 0 0 0 0 30 0 0 0 0 01 0 0 0 0 1 6 61 0 0 2 1 01 1 0 0 0 00 0 0 0 0 30 0 0 0 0 01 0 0 0 0 10 1 1 1 1 1 OutputFor each dataset, print a line having a decimal integer indicating the minimum number of moves along a route from the start to the goal. If there are no such routes, print -1 instead. Each line should not have any character other than this number. Sample Input1234567891011121314151617182 13 26 61 0 0 2 1 01 1 0 0 0 00 0 0 0 0 30 0 0 0 0 01 0 0 0 0 10 1 1 1 1 16 11 1 2 1 1 36 11 0 2 1 1 312 12 0 1 1 1 1 1 1 1 1 1 313 12 0 1 1 1 1 1 1 1 1 1 1 30 0 Sample Output12345614-1410-1 思路&amp;心得 利用深度搜索、回溯以及剪枝算法进行搜索 算法编写时应注意：终止条件的判定、资源的还原（清空）以及相应的终止命令（return、break） 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include&lt;stdio.h&gt;#define VACANT 0#define BLOCK 1#define START 2#define GOAL 3#define MAX_TIMES 11int W, H;int sx, sy, gx, gy; int min, step;int map[25][25];int dirction[4][2] = {0, -1, -1, 0, 0, 1, 1, 0};void dfs(int x, int y) { if (step &gt; 10 || step &gt;= min) return; for (int i = 0; i &lt; 4; i ++) { int tx = x, ty = y; while (1) { int px = tx, py = ty; tx += dirction[i][0], ty+= dirction[i][1]; if (tx &lt; 0 || tx &gt;= H || ty &lt; 0 || ty &gt;= W) break; if (tx == gx &amp;&amp; ty == gy) { step ++; if (step &lt; min) min = step; step --; return; } else if (map[tx][ty] == BLOCK) { if (px != x || py != y) { map[tx][ty] = VACANT; step ++; dfs(px, py); map[tx][ty] = BLOCK; step --; } break; } } }}void solve() { min = MAX_TIMES; step = 0; for (int i = 0; i &lt; H; i ++) { for (int j = 0; j &lt; W; j ++) { scanf(\"%d\", &amp;map[i][j]); if (map[i][j] == START) { sx = i, sy = j; } else if (map[i][j] == GOAL) { gx = i, gy = j; } } } dfs(sx, sy); if (min == MAX_TIMES) printf(\"-1\\n\"); else printf(\"%d\\n\", min);}int main() { while(scanf(\"%d %d\", &amp;W, &amp;H)) { if (!W &amp;&amp; !H) break; solve(); } return 0;}","link":"/categories/algorithms/POJ-3009-DFS-回溯/"},{"title":"POJ-3050 基础DFS","text":"题目DescriptionThe cows play the child’s game of hopscotch in a non-traditional way. Instead of a linear set of numbered boxes into which to hop, the cows create a 5x5 rectilinear grid of digits parallel to the x and y axes.They then adroitly hop onto any digit in the grid and hop forward, backward, right, or left (never diagonally) to another digit in the grid. They hop again (same rules) to a digit (potentially a digit already visited).With a total of five intra-grid hops, their hops create a six-digit integer (which might have leading zeroes like 000201).Determine the count of the number of distinct integers that can be created in this manner. Input* Lines 1..5: The grid, five integers per line Output* Line 1: The number of distinct integers that can be constructed Sample Input123451 1 1 1 11 1 1 1 11 1 1 1 11 1 1 2 11 1 1 1 1 Sample Output115 HintOUTPUT DETAILS:111111, 111112, 111121, 111211, 111212, 112111, 112121, 121111, 121112, 121211, 121212, 211111, 211121, 212111, and 212121 can be constructed. No other values are possible. 思路&amp;心得 利用set集合保证数据不重复，最后直接输出set的大小即可 基础深搜，数据量较弱 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;cstdio&gt;#include&lt;set&gt;#define MAX_SIZE 5using namespace std;set&lt;int&gt; s;int N, M;int a[10];int map[MAX_SIZE][MAX_SIZE];int dirction[4][2] = {0, -1, -1, 0, 0, 1, 1, 0};bool isLegal(int x, int y) { if (x &gt;= 0 &amp;&amp; x &lt; MAX_SIZE &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; MAX_SIZE) return true; else return false;}void dfs(int x, int y, int k) { if (k == 6) { int num = a[0]; for (int j = 1; j &lt; 6; j ++) { num = num * 10 + a[j]; } s.insert(num); return; } for(int i = 0; i &lt; 4; i ++) { int tx = x + dirction[i][0], ty = y + dirction[i][1]; if (isLegal(tx, ty)) { a[k] = map[tx][ty]; dfs(tx, ty, k + 1); } }} int main() { for (int i = 0; i &lt; 5; i ++) { for (int j = 0; j &lt; 5; j ++) { scanf(\"%d\", &amp;map[i][j]); } } for (int i = 0; i &lt; 5; i ++) { for (int j = 0; j &lt; 5; j ++) { a[0] = map[i][j]; dfs(i, j, 1); } } printf(\"%d\", s.size()); return 0;}","link":"/categories/algorithms/POJ-3050-基础DFS/"},{"title":"POJ-3040 局部最优到全局最优","text":"题目DescriptionAs a reward for record milk production, Farmer John has decided to start paying Bessie the cow a small weekly allowance. FJ has a set of coins in N (1 &lt;= N &lt;= 20) different denominations, where each denomination of coin evenly divides the next-larger denomination (e.g., 1 cent coins, 5 cent coins, 10 cent coins, and 50 cent coins).Using the given set of coins, he would like to pay Bessie at least some given amount of money C (1 &lt;= C &lt;= 100,000,000) every week.Please help him ompute the maximum number of weeks he can pay Bessie. Input Line 1: Two space-separated integers: N and C Lines 2..N+1: Each line corresponds to a denomination of coin and contains two integers: the value V (1 &lt;= V &lt;= 100,000,000) of the denomination, and the number of coins B (1 &lt;= B &lt;= 1,000,000) of this denomation in Farmer John’s possession.Output Line 1: A single integer that is the number of weeks Farmer John can pay Bessie at least C allowance Sample Input3 610 11 1005 120 Sample Output111 HintINPUT DETAILS:FJ would like to pay Bessie 6 cents per week. He has 100 1-cent coins,120 5-cent coins, and 1 10-cent coin. OUTPUT DETAILS:FJ can overpay Bessie with the one 10-cent coin for 1 week, then pay Bessie two 5-cent coins for 10 weeks and then pay Bessie one 1-cent coin and one 5-cent coin for 100 weeks. 思路&amp;心得 贪心题目：从局部最优解得到全局最优解。 贪心策略如下：先对数据按照金额从小到大进行排序。对于金额大于C的纸币，直接全部取出；之后进行若干次循环，每次循环中先从大到小尽可能取到小于C的最大金额，之后再从小到大尽可能凑满C，允许超出一个当前最小金额值，一次处理结束后更新相应金额的数量。 这个贪心策略的数学化证明暂时没有想到，题目中还给出“金额之间还有确定的倍数关系”，也不清楚这个信息在算法中具体体现了什么作用。对于这个贪心策略，一个较为直观的解释如下：类比生活中买东西，当消费了一定金额后，我们肯定都是使用尽可能多的大面值的RMB，再使用小面值的，整个题目的思想应该跟这个差不多，我们生活中都下意识使用了很多的贪心策略。 PS：做这个题目时，实在被坑了好久，花了三四个小时，RE + WA无数次才过，主要的问题还是自己一开始上手时，所使用的贪心策略完全错误，尝试了多种，并且举反例证明之后才逐渐找到了正确的贪心策略。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#include&lt;cstdio&gt;#include&lt;climits&gt;#include&lt;algorithm&gt;using namespace std;const int MAX_N = 25;int N, C;int ans;int use[MAX_N];struct Money { int value; int number;} a[MAX_N];bool cmp(Money a, Money b) { return a.value &lt; b.value;}void solve() { int i, start = -1; int min_num, dist; //计算所有面额大于C的数据 for (i = N - 1; i &gt;= 0; i --) { if (a[i].value &gt;= C) { ans += a[i].number; } else { start = i; break; } } while (1) { fill(use, use + N, 0); dist = C; //从大到小取到小于C的最大数值 for (i = start; i &gt;= 0; i --) { if (a[i].number) { min_num = min(dist / a[i].value, a[i].number); use[i] = min_num; dist -= a[i].value * min_num; } } //从小到大凑满C if (dist &gt; 0) { for (int i = 0; i &lt;= start; i ++) { if (a[i].number) { min_num = min((dist + a[i].value - 1) / a[i].value, a[i].number - use[i]); use[i] += min_num; dist -= a[i].value * min_num; if (dist &lt;= 0) break; } } } if (dist &gt; 0) break; //数量更新 min_num = INT_MAX; for (i = 0; i &lt;= start; i ++) { if (use[i]) min_num = min(min_num, a[i].number / use[i]); } for (i = 0; i &lt;= start; i ++) { if (use[i]) { a[i].number -= min_num * use[i]; } } ans += min_num; } printf(\"%d\\n\", ans);}int main() { while (~scanf(\"%d %d\", &amp;N, &amp;C)) { ans = 0; for (int i = 0; i &lt; N; i ++) { scanf(\"%d %d\", &amp;a[i].value, &amp;a[i].number); } sort(a, a + N, cmp); solve(); } return 0;}","link":"/categories/algorithms/POJ-3040-局部最优到全局最优/"},{"title":"POJ-3169 差分约束系统","text":"题目DescriptionLike everyone else, cows like to stand close to their friends when queuing for feed. FJ has N (2 &lt;= N &lt;= 1,000) cows numbered 1..N standing along a straight line waiting for feed. The cows are standing in the same order as they are numbered, and since they can be rather pushy, it is possible that two or more cows can line up at exactly the same location (that is, if we think of each cow as being located at some coordinate on a number line, then it is possible for two or more cows to share the same coordinate). Some cows like each other and want to be within a certain distance of each other in line. Some really dislike each other and want to be separated by at least a certain distance. A list of ML (1 &lt;= ML &lt;= 10,000) constraints describes which cows like each other and the maximum distance by which they may be separated; a subsequent list of MD constraints (1 &lt;= MD &lt;= 10,000) tells which cows dislike each other and the minimum distance by which they must be separated.Your job is to compute, if possible, the maximum possible distance between cow 1 and cow N that satisfies the distance constraints. InputLine 1: Three space-separated integers: N, ML, and MD.Lines 2..ML+1: Each line contains three space-separated positive integers: A, B, and D, with 1 &lt;= A &lt; B &lt;= N. Cows A and B must be at most D (1 &lt;= D &lt;= 1,000,000) apart.Lines ML+2..ML+MD+1: Each line contains three space-separated positive integers: A, B, and D, with 1 &lt;= A &lt; B &lt;= N. Cows A and B must be at least D (1 &lt;= D &lt;= 1,000,000) apart. OutputLine 1: A single integer. If no line-up is possible, output -1. If cows 1 and N can be arbitrarily far apart, output -2. Otherwise output the greatest possible distance between cows 1 and N. Sample Input12344 2 11 3 102 4 202 3 3 Sample Output127 HintExplanation of the sample:There are 4 cows. Cows #1 and #3 must be no more than 10 units apart, cows #2 and #4 must be no more than 20 units apart, and cows #2 and #3 dislike each other and must be no fewer than 3 units apart.The best layout, in terms of coordinates on a number line, is to put cow #1 at 0, cow #2 at 7, cow #3 at 10, and cow #4 at 27. 思路&amp;心得 POJ-3169:查分约束系统，利用约束条件，将问题转化为最短路径问题，并利用Bellman-Ford或SPFA算法求解 本题需要考虑边的方向关系，虽然感觉是无向图，但是最后却还是初始化成有向图 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;cstdio&gt;#include&lt;climits&gt;#include&lt;algorithm&gt;#define MAX_N 10005#define MAX_M 30005#define MAX_D 2000005using namespace std;int N, ML, MD;int A, B, D;int dist[MAX_N];struct Edge { int from; int to; int cost;} E[MAX_M];int Bellman_Ford(int s) { for (int i = 1; i &lt;= N; i ++) { dist[i] = MAX_D; } dist[s] = 0; int edgeNum = ML + MD + N - 1; for (int i = 0; i &lt; N; i ++) { for (int j = 0; j &lt; edgeNum; j ++) { if (dist[E[j].from] + E[j].cost &lt; dist[E[j].to]) { if (i == N - 1) return -1; dist[E[j].to] = dist[E[j].from] + E[j].cost; } } } return dist[N] == MAX_D ? -2 : dist[N];}void solve() { /** * 图的初始化 */ for (int i = 0; i &lt; ML; i ++) { scanf(\"%d %d %d\", &amp;A, &amp;B, &amp;D); if (A &gt; B) swap(A, B); E[i].from = A, E[i].to = B, E[i].cost = D; } for (int i = 0; i &lt; MD; i ++) { scanf(\"%d %d %d\", &amp;A, &amp;B, &amp;D); if (A &gt; B) swap(A, B); E[ML + i].from = B, E[ML + i].to = A, E[ML + i].cost = -D; } for (int i = 0; i &lt; N - 1; i ++) { E[ML + MD + i].from = i + 2, E[ML + MD + i].to = i + 1, E[ML + MD + i].cost = 0; } printf(\"%d\\n\", Bellman_Ford(1));}int main() { while (~scanf(\"%d %d %d\", &amp;N, &amp;ML, &amp;MD)) { solve(); } return 0; } x","link":"/categories/algorithms/POJ-3169-差分约束系统/"},{"title":"POJ-3176","text":"题目DescriptionThe cows don’t use actual bowling balls when they go bowling. They each take a number (in the range 0..99), though, and line up in a standard bowling-pin-like triangle like this: 7 3 8 8 1 0 2 7 4 4 4 5 2 6 5 Then the other cows traverse the triangle starting from its tip and moving “down” to one of the two diagonally adjacent cows until the “bottom” row is reached. The cow’s score is the sum of the numbers of the cows visited along the way. The cow with the highest score wins that frame. Given a triangle with N (1 &lt;= N &lt;= 350) rows, determine the highest possible sum achievable. InputLine 1: A single integer, N Lines 2..N+1: Line i+1 contains i space-separated integers that represent row i of the triangle. OutputLine 1: The largest sum achievable using the traversal rules Sample Input573 88 1 02 7 4 44 5 2 6 5 Sample Output30Hint Explanation of the sample: 7 * 3 8 * 8 1 0 * 2 7 4 4 * 4 5 2 6 5 The highest score is achievable by traversing the cows as shown above. 思路&amp;心得简单的动态规划问题，从底向上依次扫描就行了，可以用直接用DP，也可以用记忆化搜索。 代码1.DP： 12345678910111213141516171819202122232425262728#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;const int MAX_N = 355;int N;int dp[MAX_N][MAX_N];void solve() { for (int i = 0; i &lt; N; i ++) { for (int j = 0; j &lt;= i; j ++) { scanf(\"%d\", &amp;dp[i][j]); } } for (int i = N - 2; i &gt;=0; i --) { for (int j = 0; j &lt;= i; j ++) { dp[i][j] += max(dp[i + 1][j], dp[i + 1][j + 1]); } } printf(\"%d\\n\", dp[0][0]);}int main() { while (~scanf(\"%d\", &amp;N)) { solve(); } return 0;} 2.记忆化搜索： 12345678910111213141516171819202122232425262728293031#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;const int MAX_N = 355;int N;int dp[MAX_N][MAX_N];int visit[MAX_N][MAX_N];int score(int x, int y) { if (visit[x][y] == 1) return dp[x][y]; visit[x][y] = 1; if (x == N - 1) return dp[x][y]; return dp[x][y] += max(score(x + 1, y), score(x + 1, y + 1));}void solve() { for (int i = 0; i &lt; N; i ++) { for (int j = 0; j &lt;= i; j ++) { scanf(\"%d\", &amp;dp[i][j]); } } printf(\"%d\\n\", score(0, 0));}int main() { while (~scanf(\"%d\", &amp;N)) { solve(); } return 0;}","link":"/categories/algorithms/POJ-3176/"},{"title":"POJ-3187 枚举全排列","text":"、题目DescriptionFJ and his cows enjoy playing a mental game. They write down the numbers from 1 to N (1 &lt;= N &lt;= 10) in a certain order and then sum adjacent numbers to produce a new list with one fewer number. They repeat this until only a single number is left. For example, one instance of the game (when N=4) might go like this:​ 3 1 2 4​ 4 3 6​ 7 9​ 16 Behind FJ’s back, the cows have started playing a more difficult game, in which they try to determine the starting sequence from only the final total and the number N. Unfortunately, the game is a bit above FJ’s mental arithmetic capabilities.Write a program to help FJ play the game and keep up with the cows. InputLine 1: Two space-separated integers: N and the final sum. OutputLine 1: An ordering of the integers 1..N that leads to the given sum. If there are multiple solutions, choose the one that is lexicographically least, i.e., that puts smaller numbers first. Sample Input14 16 Sample Output13 1 2 4 HintExplanation of the sample:There are other possible sequences, such as 3 2 1 4, but 3 1 2 4 is the lexicographically smallest. 思路&amp;心得 利用next_permutation()函数生成全排列，暴力搜索 代码12345678910111213141516171819202122232425262728293031323334353637#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int N, sum;int a[11], b[11];void solve() { for (int i = 0; i &lt; N; i ++) { a[i] = i + 1; } do { for (int i = 0; i &lt; N; i ++) { b[i] = a[i]; } for (int i = N - 1; i &gt; 0; i --) { for (int j = 0; j &lt; i; j ++) { b[j] += b[j + 1]; } } if (b[0] == sum) { for (int i = 0; i &lt; N; i ++) { printf(\"%d \", a[i]); } printf(\"\\n\"); break; } } while (next_permutation(a, a + N));}int main() { while (~scanf(\"%d %d\", &amp;N, &amp;sum)) { solve(); } return 0;}","link":"/categories/algorithms/POJ-3187-枚举全排列/"},{"title":"POJ-3190 区间问题","text":"题目DescriptionOh those picky N (1 &lt;= N &lt;= 50,000) cows! They are so picky that each one will only be milked over some precise time interval A..B (1 &lt;= A &lt;= B &lt;= 1,000,000), which includes both times A and B. Obviously, FJ must create a reservation system to determine which stall each cow can be assigned for her milking time. Of course, no cow will share such a private moment with other cows. Help FJ by determining:The minimum number of stalls required in the barn so that each cow can have her private milking periodAn assignment of cows to these stalls over timeMany answers are correct for each test dataset; a program will grade your answer. InputLine 1: A single integer, N Lines 2..N+1: Line i+1 describes cow i’s milking interval with two space-separated integers. OutputLine 1: The minimum number of stalls the barn must have. Lines 2..N+1: Line i+1 describes the stall to which cow i will be assigned for her milking period. Sample Input51 102 43 65 84 7 Sample Output412324 HintExplanation of the sample: Here’s a graphical schedule for this output: Time 1 2 3 4 5 6 7 8 9 10 Stall 1 c1&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Stall 2 .. c2&gt;&gt;&gt;&gt;&gt;&gt; c4&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; .. .. Stall 3 .. .. c3&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; .. .. .. .. Stall 4 .. .. .. c5&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; .. .. ..Other outputs using the same number of stalls are possible. 思路&amp;心得 贪心策略：先将所有数据按照开始时间start从小到大进行排序，然后以结束时间end为关键字维护一个最小优先队列。 开始时先将排序后的第一个数据加入到优先队列中，然后依次扫描数据，若start大于队首元素的end值，则弹出队首元素，并将此时的数据加入到优先队列中，同时更新每个元素对应的stall[i]值。 算法结束时，队列的大小即为所需要的stall个数。 在定义结构体时，加入pos位置元素，以保存每个数据对应的原始位置，因为输出要求按照原始数据的顺序进行输出的。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;cstdio&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;#define MAX_SIZE 50005using namespace std;struct P { int start; int end; int pos;} a[MAX_SIZE];int N;int stall[MAX_SIZE];bool cmp(P a, P b) { return a.start &lt; b.start;}bool operator &gt; (P a, P b) { return a.end &gt; b.end;}void solve() { priority_queue&lt;P, vector&lt;P&gt;, greater&lt;P&gt; &gt; que; for (int i = 0; i &lt; N; i ++) { scanf(\"%d %d\", &amp;a[i].start, &amp;a[i].end); a[i].pos = i; } sort(a, a + N, cmp); fill(stall, stall + N, 1); que.push(a[0]); for (int i = 1; i &lt; N; i ++) { P temp = que.top(); if (a[i].start &gt; temp.end) { stall[a[i].pos] = stall[temp.pos]; que.pop(); } else { stall[a[i].pos] = que.size() + 1; } que.push(a[i]); } printf(\"%d\\n\", que.size()); for (int i = 0; i &lt; N; i ++) { printf(\"%d\\n\", stall[i]); }}int main() { scanf(\"%d\", &amp;N); solve(); return 0;}","link":"/categories/algorithms/POJ-3190-区间问题/"},{"title":"POJ-3255 次短路径","text":"题目DescriptionBessie has moved to a small farm and sometimes enjoys returning to visit one of her best friends. She does not want to get to her old home too quickly, because she likes the scenery along the way. She has decided to take the second-shortest rather than the shortest path. She knows there must be some second-shortest path. The countryside consists of R (1 ≤ R ≤ 100,000) bidirectional roads, each linking two of the N (1 ≤ N ≤ 5000) intersections, conveniently numbered 1..N. Bessie starts at intersection 1, and her friend (the destination) is at intersection N. The second-shortest path may share roads with any of the shortest paths, and it may backtrack i.e., use the same road or intersection more than once. The second-shortest path is the shortest path whose length is longer than the shortest path(s) (i.e., if two or more shortest paths exist, the second-shortest path is the one whose length is longer than those but no longer than any other path). InputLine 1: Two space-separated integers: N and R Line 1: Two space-separated integers: N and RLines 2..R+1: Each line contains three space-separated integers: A, B, and D that describe a road that connects intersections A and B and has length D (1 ≤ D ≤ 5000) OutputLine 1: The length of the second shortest path between node 1 and node N Sample Input123454 41 2 1002 4 2002 3 2503 4 100 Sample Output1450 HintTwo routes: 1 -&gt; 2 -&gt; 4 (length 100+200=300) and 1 -&gt; 2 -&gt; 3 -&gt; 4 (length 100+250+100=450) 思路&amp;心得 定义权值的最大值时，尽可能大，综合题目的条件，比如这题的MAX_D就应该大于两倍的D最大值 注意题目条件：比如这题存在自环边、双向边，同一条道路可以经过 图论中，注意根据题目图的特点选择对应的算法，比如这题为稀疏图，应选择邻接表进行表示图 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#include&lt;cstdio&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#define MAX_SIZE 5005#define MAX_D 10005using namespace std;struct edge { int to, cost;};vector&lt;edge&gt; G[MAX_SIZE];int N, R;int dist[MAX_SIZE];int dist2[MAX_SIZE];int visit[MAX_SIZE];int visit2[MAX_SIZE];void Dijkstra(int s) { dist[s] = 0; while (true) { int min_D = MAX_D; int index; int d, flag = 0; for (int j = 1; j &lt;= N; j ++) { if (!visit[j] &amp;&amp; dist[j] &lt; min_D) { min_D = dist[j]; index = j; flag = 1; } else if (!visit2[j] &amp;&amp; dist2[j] &lt; min_D) { min_D = dist[j]; index = j; flag = 2; } } if (!flag) break; else if (flag == 1) { visit[index] = 1; d = dist[index]; } else if (flag == 2) { visit2[index] = 1; d = dist2[index]; } for (int k = 0; k &lt; G[index].size(); k ++) { edge e = G[index][k]; int temp = e.cost + d; if (!visit[e.to] &amp;&amp; temp &lt; dist[e.to]) { swap(dist[e.to], temp); } if (dist[e.to] &lt; temp &amp;&amp; temp &lt; dist2[e.to]) { dist2[e.to] = temp; visit2[e.to] = 0; } } } printf(\"%d\\n\", dist2[N]);}void solve() { for (int i = 1; i &lt;= N; i ++) { visit[i] = 0; visit2[i] = 1; dist[i] = MAX_D; dist2[i] = MAX_D; G[i].clear(); } int from; edge e; for (int i = 1; i &lt;= R; i ++) { scanf(\"%d %d %d\", &amp;from, &amp;e.to, &amp;e.cost); G[from].push_back(e); swap(from, e.to); G[from].push_back(e); } Dijkstra(1); }int main() { while (~scanf(\"%d %d\", &amp;N, &amp;R)) { solve(); } return 0; }","link":"/categories/algorithms/POJ-3255-次短路径/"},{"title":"POJ-3669 BFS","text":"题目DescriptionBessie hears that an extraordinary meteor shower is coming; reports say that these meteors will crash into earth and destroy anything they hit. Anxious for her safety, she vows to find her way to a safe location (one that is never destroyed by a meteor) . She is currently grazing at the origin in the coordinate plane and wants to move to a new, safer location while avoiding being destroyed by meteors along her way. The reports say that M meteors (1 ≤ M ≤ 50,000) will strike, with meteor i will striking point (Xi, Yi) (0 ≤ Xi ≤ 300; 0 ≤ Yi ≤ 300) at time Ti (0 ≤ Ti ≤ 1,000). Each meteor destroys the point that it strikes and also the four rectilinearly adjacent lattice points. Bessie leaves the origin at time 0 and can travel in the first quadrant and parallel to the axes at the rate of one distance unit per second to any of the (often 4) adjacent rectilinear points that are not yet destroyed by a meteor. She cannot be located on a point at any time greater than or equal to the time it is destroyed). Determine the minimum time it takes Bessie to get to a safe place. Input* Line 1: A single integer: M* Lines 2..M+1: Line i+1 contains three space-separated integers: Xi, Yi, and Ti Output* Line 1: The minimum time it takes Bessie to get to a safe place or -1 if it is impossible. Sample Input1234540 0 22 1 21 1 20 3 5 Sample Output15 思路 安全区可以到301 同一个点可以炸两次 必须在第一象限 t可以等于0 可以一开始就死，也可以一开始就是安全区 心得 要注意数据的范围 广搜常用来求解某个问题的最小值,求解过程中，利用dist数组更新最小值 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778int dist[MAX_SIZE][MAX_SIZE];int dirction[5][2] = {0, -1, -1, 0, 0, 1, 1, 0, 0, 0};typedef pair&lt;int, int&gt; P;struct Meteor { int X; int Y; int T;} meteors[MAX_M];bool cmp (Meteor a, Meteor b) { if (a.T &lt; b.T) return true; else return false;}bool isLegal(int x, int y) { if (x &gt;= 0 &amp;&amp; x &lt; MAX_SIZE &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; MAX_SIZE) return true; else return false;}int bfs() { queue&lt;P&gt; que; if (map[0][0] == 0) return -1; else if (map[0][0] == MAX_TIME) return 0; que.push(P(0, 0)); while (que.size()) { P p = que.front(); que.pop(); int px = p.first, py = p.second; if (px &gt;= MAX_SIZE || py &gt;= MAX_SIZE) continue; for(int i = 0; i &lt; 4; i ++) { int tx = px + dirction[i][0], ty = py + dirction[i][1]; if (isLegal(tx, ty)) { if (map[tx][ty] == MAX_TIME) { return ++ dist[px][py]; } if (dist[px][py] + 1 &lt; map[tx][ty] &amp;&amp; !dist[tx][ty]) { que.push(P(tx, ty)); dist[tx][ty] = dist[px][py] + 1; } } } } return -1;}void solve() { for (int i = 0; i &lt; MAX_SIZE; i ++) { for (int j = 0; j &lt; MAX_SIZE; j ++) { map[i][j] = MAX_TIME; } } for (int i = 0; i &lt; M; i ++) { scanf(\"%d %d %d\", &amp;meteors[i].X, &amp;meteors[i].Y, &amp;meteors[i].T); } sort(meteors, meteors + M, cmp); for (int i = 0; i &lt; M; i ++) { for (int j = 0; j &lt; 5; j ++) { int tx = meteors[i].X, ty = meteors[i].Y; tx += dirction[j][0]; ty += dirction[j][1]; if (isLegal(tx, ty) &amp;&amp; map[tx][ty] &gt; meteors[i].T) { map[tx][ty] = meteors[i].T; } } } printf(\"%d\\n\", bfs());}int main() { while (~scanf(\"%d\", &amp;M)) { solve(); } return 0;}","link":"/categories/algorithms/POJ-3669-BFS/"},{"title":"POJ-3616","text":"题目DescriptionBessie is such a hard-working cow. In fact, she is so focused on maximizing her productivity that she decides to schedule her next N (1 ≤ N ≤ 1,000,000) hours (conveniently labeled 0..N-1) so that she produces as much milk as possible. Farmer John has a list of M (1 ≤ M ≤ 1,000) possibly overlapping intervals in which he is available for milking. Each interval i has a starting hour (0 ≤ starting_houri ≤ N), an ending hour (starting_houri &lt; ending_houri ≤ N), and a corresponding efficiency (1 ≤ efficiencyi ≤ 1,000,000) which indicates how many gallons of milk that he can get out of Bessie in that interval. Farmer John starts and stops milking at the beginning of the starting hour and ending hour, respectively. When being milked, Bessie must be milked through an entire interval. Even Bessie has her limitations, though. After being milked during any interval, she must rest R (1 ≤ R ≤ N) hours before she can start milking again. Given Farmer Johns list of intervals, determine the maximum amount of milk that Bessie can produce in the N hours. Input Line 1: Three space-separated integers: N, M, and R Lines 2..M+1: Line i+1 describes FJ’s ith milking interval withthree space-separated integers: starting_houri , ending_houri , and efficiencyi Output Line 1: The maximum number of gallons of milk that Bessie can product in the N hours Sample Input12 4 21 2 810 12 193 6 247 10 31 Sample Output43 思路&amp;心得 对于”每头奶牛挤奶后需要休息R分钟“的条件限制，直接让每头奶牛的结束时间加上R分钟，来消除这个限制。 定义挤奶的结构体（开始时间、结束时间、获得收益），然后按照结束时间从小到大进行排序。 这题有点像贪心中的区间问题，排完序后,定义dp[i]为：，到dp[i].end这个时间点为止，可以获得的最大收益。显然，dp[i]满足如下递推式：dp[i] = dp[k] + T[i].gollon。（其中k为dp[0 to i - 1]的最大值下标，且T[k].end &lt;= T[i].start） 注意题目不不是在最后一个时间点取到最大值，因此需要记录最大的dp。 代码123456789101112131415161718192021222324252627282930313233343536373839#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;const int MAX_M = 1005;int N, M, R;int dp[MAX_M];struct times { int start; int end; int gollon;} T[MAX_M];bool cmp(times a, times b) { return a.end &lt; b.end;}int main() { int maxGollons = 0; scanf(\"%d %d %d\", &amp;N, &amp;M, &amp;R); for (int i = 0; i &lt;= M; i ++) { scanf(\"%d %d %d\", &amp;T[i].start, &amp;T[i].end, &amp;T[i].gollon); T[i].end += R; } sort(T, T + M, cmp); for (int i = 0; i &lt; M; i ++) { dp[i] = T[i].gollon; for (int j = 0; j &lt; i; j ++) { if (T[j].end &lt;= T[i].start) { dp[i] = max(dp[i], dp[j] + T[i].gollon); } } maxGollons = max(maxGollons, dp[i]); } printf(\"%d\\n\", maxGollons); return 0;}","link":"/categories/algorithms/POJ-3616/"},{"title":"POJ-3262","text":"题目DescriptionFarmer John went to cut some wood and left N (2 ≤ N ≤ 100,000) cows eating the grass, as usual. When he returned, he found to his horror that the cluster of cows was in his garden eating his beautiful flowers. Wanting to minimize the subsequent damage, FJ decided to take immediate action and transport each cow back to its own barn. Each cow i is at a location that is Ti minutes (1 ≤ Ti ≤ 2,000,000) away from its own barn. Furthermore, while waiting for transport, she destroys Di (1 ≤ Di ≤ 100) flowers per minute. No matter how hard he tries, FJ can only transport one cow at a time back to her barn. Moving cow i to its barn requires 2 × Ti minutes (Ti to get there and Ti to return). FJ starts at the flower patch, transports the cow to its barn, and then walks back to the flowers, taking no extra time to get to the next cow that needs transport. Write a program to determine the order in which FJ should pick up the cows so that the total number of flowers destroyed is minimized. InputLine 1: A single integer NLines 2..N+1: Each line contains two space-separated integers, Ti and Di, that describe a single cow’s characteristics OutputLine 1: A single integer that is the minimum number of destroyed flowers Sample Input63 12 52 33 24 11 6 Sample Output86 思路&amp;心得 贪心策略：用d/t的值进行选择，值大的数据先处理，以达到花费代价最小。 可以使用排序或则优先队列做，个人觉得排序比较简单直接。至于每一次数据处理时的代价计算，利用总代价反复减去对应数据的代价即可。 题目需注意是数据量较大，所有结果应采用long long或则__int64类型进行定义。 代码12345678910111213141516171819202122232425262728293031323334353637#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;const int MAX_N = 100005; typedef pair&lt;float, float&gt; P;int N, sum;long long cost;P a[MAX_N];bool cmp(P a, P b) { return a.second / a.first &gt; b.second / b.first;}void solve() { sum = 0, cost = 0; for (int i = 0; i &lt; N; i ++) { scanf(\"%f %f\", &amp;a[i].first, &amp;a[i].second); sum += a[i].second; } sort(a, a + N, cmp); for (int i = 0; i &lt; N; i ++) { sum -= a[i].second; cost += 2 * a[i].first * sum; } printf(\"%lld\\n\", cost);}int main() { while (~scanf(\"%d\", &amp;N)) { solve(); } return 0;}","link":"/categories/algorithms/POJ-3262/"},{"title":"POJ-3723 最大生成树","text":"题目DescriptionWindy has a country, and he wants to build an army to protect his country. He has picked up N girls and M boys and wants to collect them to be his soldiers. To collect a soldier without any privilege, he must pay 10000 RMB. There are some relationships between girls and boys and Windy can use these relationships to reduce his cost. If girl x and boy y have a relationship d and one of them has been collected, Windy can collect the other one with 10000-d RMB. Now given all the relationships between girls and boys, your assignment is to find the least amount of money Windy has to pay. Notice that only one relationship can be used when collecting one soldier.path). InputThe first line of input is the number of test case.The first line of each test case contains three integers, N, M and R.Then R lines followed, each contains three integers xi, yi and di.There is a blank line before each test case. 1 ≤ N, M ≤ 100000 ≤ R ≤ 50,0000 ≤ xi &lt; N0 ≤ yi &lt; M0 &lt; di &lt; 10000 OutputFor each test case output the answer in a single line. Sample Input123456789101112131415161718192021222325 5 84 3 68311 3 45830 0 65920 1 30633 3 49751 3 20494 2 21042 2 7815 5 102 4 98203 2 62363 1 88642 4 83262 0 51562 0 14634 1 24390 4 43733 4 88892 4 3133 Sample Output127107154223 思路&amp;心得代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include&lt;cstdio&gt;#include&lt;algorithm&gt;#define MAX_V 20005#define MAX_E 50005using namespace std;int fa[MAX_V];int N, M, R, t;struct Edge { int a; int b; int cost;} E[MAX_E];bool cmp(Edge x, Edge y) { return x.cost &lt; y.cost;}void Union_init(int n) { for(int i = 0; i &lt;= n; i ++) fa[i] = i;}int find(int x) { if (fa[x] == x) return x; return fa[x] = find(fa[x]);}void Union(int x, int y) { x = find(x); y = find(y); if(x != y) fa[x] = y;}int same(int x, int y) { return find(x) == find(y);}int Kruscal() { int x, y, sum = 0; for(int i = 0; i &lt; R; i ++) { x = E[i].a, y = E[i].b; x = find(x), y = find(y); if( x != y ) { sum += E[i].cost; fa[x] = y; } } return sum; }void solve() { scanf(\"%d %d %d\", &amp;N, &amp;M, &amp;R); Union_init(N + M + 1); for (int i = 0; i &lt; R; i ++) { scanf(\"%d %d %d\", &amp;E[i].a, &amp;E[i].b, &amp;E[i].cost); E[i].b += N, E[i].cost = -E[i].cost; } sort(E, E + R, cmp); printf(\"%d\\n\", 10000 * (N + M) + Kruscal());}int main() { scanf(\"%d\", &amp;t); while(t --) { getchar(); solve(); } return 0;}","link":"/categories/algorithms/POJ-3723-最大生成树/"},{"title":"Post Tuned Hashing，PTH","text":"[ACM 2018] Post Tuned Hashing_A New Approach to Indexing High-dimensional Data [paper] [code] Zhendong Mao, Quan Wang, Yongdong Zhang, Bin Wang. Overcome 大多数哈希方法都有二值化过程，二值化加速了检索过程，但同时难以避免得也破环了原始数据的相邻结构。 Contribute 提出了新的哈希方法——PTH，包含三个阶段：projection，binarization和post-tuning。其中post-tuning阶段可以在利用任意哈希方法得到哈希二值编码之后，再独立得进行post-tune处理以重建被二值阶段破坏的数据相邻结构，以改善算法表现。 为post-tuning算法提出了一个out-of-sample扩展，使得PTH算法可以处理训练数据集之外的数据，如测试集。 PTH在五个数据集的测试表现超过的所有的state-of-the-art算法。 AlgorithmPOST TUNED HASHING之前的哈希方法大都有projection和binarization两个阶段，这些two-stage的方法大都会造成neighborhood error。我们可以定义neighborhood error如下： L = ||S-V||_{F}^{2}其中， S, V分别是原始数据X和二值编码B的相似矩阵，其中$ij-th$个元素表明对应第i个数据和第j个数据是否相似。 Post Tuned Hashing（PTH）的post-tuning过程：$R：{-1，1}^m \\to {-1, 1}^m$，可以改善二值编码，使得neighborhood error最小化: PTH(X) = R(H(X))在post-tuning过程中，H(X)可以利用任何哈希方法产生。因此，PTH可以非常简单得应用于广泛的哈希方法中以改进其二值编码表现。 Overall Framework矩阵S表示原始数据X间的相似信息，其具体定义如下： V表示原始数据X对应的二值编码B间的相似信息，其具体定义如下： V_{ij} = （b_i · b_j )/ m此时，将neighbood error改写为： L = ||S-\\frac{1}{m} B^TB||_{F}^2定义U为post-tuning matrix，且Z=H(X)，此时，目标函数为： 矩阵U中的每一个元素代表Z中对应位置的元素是否需要更新以得到更小的neighborhood error。PTH方法最终得到的改善后的哈希编码为：B=U ○ Z。 Optimization AlgorithmObservation：目标函数中的所有二次项都是常数（取值只为1/-1），因此最小化目标函数等同于最小化所有线性项。 令$\\gamma=1/m$，则目标函数变为： 上述目标函数关于矩阵第p行的表示为： 令z_p为矩阵Z第p行的行向量，Q = Z*Z^T。则上述目标函数变为： 令矩阵$C=Q○(S - \\gamma O)$，则目标函数的线性项关于矩阵U第p行第q列的元素$u_{ij}$的结果为： 因此，对于元素$u{ij}$，最小化Q(U)即最小化上式，且其可以被认为是元素$u{ij}$的权重。当这个权重小于0时，我们将$u_{ij}$设为1，大于0时则设为-1。 Updating strategy：在每次更新时，当且仅当$u_{ij}$的权重绝对值大于一个阈值$\\eta$时对其进行更新，在实验中，阈值$\\eta$被设置为所有权重的均值。mean absolute value of projecttion results。为了增加计算效率，可以使用同一个矩阵C对U的每一行进行更新，所得到的表现和elementi-by-element的结果类似。 Pruning strategy：在算法中仅对projection results（未二值化处理）中值接近0或则小于一个阈值$\\delta$的元素进行更新，因为只有这些元素才有较大的概率而二值化到错误的编码。阈值$\\delta$被设置为mean absolute value of projection results。 在论文的代码中，并没有利用到$\\eta$。只要$(\\sumku_p^kC_q^k)u{ij}&lt;0$，就对$u_{ij}$取反。符合最小化目标函数的思想。 Out-of-Sample Post-TuningPTH在post-tuning阶段可以改善数据X的二值编码，使其更好得保留原有数据的相邻结构。但是我们还需要对不在数据集X中的数据（ 查询图片）进行测试。我们称X为skeleton points。完整的post-tuning阶段包含两个步骤： 对skeleton points进行post-tune处理； 对out-of-samples进行post-tune进行处理使得其二值编码能够和X保持一致。 假设q为out-of-sample，$z^q$为q的原始二值编码，则q的post-tuning过程为： 其中$S^q$为q和X的相邻信息矩阵，B为X的post-tuned编码。post-tuning过程和哈希函数的学习过程时独立的，因此skeleton points X可以和哈希函数所用的训练集不同，且后续实验表明，一小部分的数据集X就可以使得post-tuning过程达到很好的效果。","link":"/categories/computer-vision/Post-Tuned-Hashing，PTH/"},{"title":"ResNet","text":"问题先前的研究已经证明，拥有至少一个隐层的神经网络是一个通用的近似器，只要提高网络的深度，可以近似任何连续函数。因此，理想情况下，只要网络不过拟合，深度神经网络应该是越深越好。但是在实际情况中，在不断加神经网络的深度时，会出现一个 Degradation 的问题，即准确率会先上升然后达到饱和，再持续增加深度则会导致准确率下降。这并不是过拟合的问题，因为不光在测试集上误差增大，训练集本身误差也会增大。对此的解释为：当网络的层级很多时，随着前向传播的进行，输入数据的一些信息可能会被丢掉（激活函数、随机失活等），从而导致模型最后的表现能力很一般。 ResNet假设有一个比较浅的网络（Shallow Net）达到了饱和的准确率，那么后面再加上几个的全等映射层（Identity mapping），起码误差不会增加，即更深的网络不应该带来训练集上误差上升。而这里提到的使用全等映射直接将前一层输出传到后面的思想，就是 ResNet 的灵感来源。在ResNets中，作者通过shorcut connection操作，保证了网络的深度越深，模型的表现能力一定不会下降。 作者提出一个 Deep residual learning 框架来解决这种因为深度增加而导致性能下降问题。 假定某段神经网络的输入是 x，期望输出是 H(x)，即 H(x) 是期望的复杂潜在映射，但学习难度大；如果我们直接把输入 x 传到输出作为初始结果，通过下图“shortcut connections”，那么此时我们需要学习的目标就是 F(x)=H(x)-x，于是 ResNet 相当于将学习目标改变了，不再是学习一个完整的输出，而是最优解H(X) 和全等映射 x 的差值，即残差 Shortcut 原意指捷径，在这里就表示越层连接，在 Highway Network 在设置了一条从 x 直接到 y 的通路，以 T(x, Wt) 作为 gate 来把握两者之间的权重；而 ResNet shortcut 没有权值，传递 x 后每个模块只学习残差F(x)，且网络稳定易于学习，作者同时证明了随着网络深度的增加，性能将逐渐变好。可以推测，当网络层数够深时，优化 Residual Function：F(x)=H(x)−x，易于优化一个复杂的非线性映射 H(x)。 在 ResNet 的论文中，除了提出残差学习单元的两层残差学习单元，还有三层的残差学习单元。两层的残差学习单元中包含两个相同输出通道数（因为残差等于目标输出减去输入，即，因此输入、输出维度需保持一致）的3´3卷积；而3层的残差网络则使用了 Network In Network 和 Inception Net 中的1´1卷积，并且是在中间3´3的卷积前后都使用了1´1卷积，先降维再升维的操作，降低计算复杂度。另外，如果有输入、输出维度不同的情况，我们可以对 x 做一个线性映射变换，再连接到后面的层。 参考 Deep Learning-TensorFlow (14) CNN卷积神经网络_深度残差网络 ResNet","link":"/categories/deep-learning/ResNet/"},{"title":"RNN，循环神经网络","text":"问题分析传统神经网络 在不同的示例中，输入和输出可能具有不同的维度。 无法在不同的文本位置共享所学到的特征信息。 循环神经网络 RNN使用先前的信息以及现在的输入来得到输出，但是输出不仅仅只跟前面的信息有关，可能还会和后面的信息有关。因此没有利用到后面的信息，可能会导致预测出错。 因为梯度消失或梯度爆炸的原因，RNNs的神经网络无法很深，因此最开始的单词很难对句子后面的单词产生影响，例如英语中名词的单复数对was和were的影响。 梯度爆炸也是RNNS的一个问题，但是梯度爆炸更容易发现，当我们看到参数编程NAN时，便知道参数溢出了，此时可以采用梯度缩减等方法，将梯度进行缩放，控制在一定的量级之类，从而很好得解决梯度爆炸的问题。 RNN结构one-to-oneone-to-many 音乐生成 many-to-one 分类问题，例如电影分类 many-to-many 语言翻译（输入和输出具有不同的维度） 句子中的人名识别（输入和输出具有相同的维度） Deep RNNs深度循环神经网络的结构便是将序列模型增加几层，但是不同于先前几百层的神经网络结构，Deep RNNs的网络层次一般很少超过三层。由于时间维度的存在，即使层数很少然是网络的规模也会变得很大。 双向神经网络在RNN中，某个时刻的输出可能不仅与先前的信息有关，还可能与之后的信息有关。因此就需要利用到双向的信息，得到了双向身形网络。 例：在一个人名辨别的RNN中，有两个输入分别为： He said, “Teddy bears are on sale!” He said, “Teddy roosevelt was a great President!” 仅仅通过“He said Teddy”这三个词是无法判断出Teddy是否是人名，还需要利用到之后的信息。 LSTM公式 遗忘门在LSTM的忘记门里，如果没有加上偏置量，那么很可能导致一开始的梯度就消失。而使用了偏置，可以确保网络开始训练时忘记门是关闭的，然后在训练过程中学习到如何关闭忘记门。 ft控制上一时刻记忆单元ct-1的信息融入记忆单元ct。在理解一句话时，当前词xt可能继续延续上文的意思继续描述，也可能从当前词xt开始描述新的内容，与上文无关。和输入门it相反， ft不对当前词xt的重要性作判断， 而判断的是上一时刻的记忆单元ct-1对计算当前记忆单元ct的重要性。当ft开关打开的时候，网络将不考虑上一时刻的记忆单元ct-1。 输入门it控制当前词xt的信息融入记忆单元ct。在理解一句话时，当前词xt可能对整句话的意思很重要，也可能并不重要。输入门的目的就是判断当前词xt对全局的重要性。当it开关打开的时候，网络将不考虑当前输入xt。 输出门输出门ot的目的是从记忆单元ct产生隐层单元ht。并不是ct中的全部信息都和隐层单元ht有关，ct可能包含了很多对ht无用的信息，因此， ot的作用就是判断ct中哪些部分是对ht有用的，哪些部分是无用的。 记忆单元记忆单元ct综合了当前词xt和前一时刻记忆单元ct-1的信息。这和ResNet中的残差逼近思想十分相似，通过从ct-1到ct的”短路连接”， 梯度得已有效地反向传播。 当ft处于闭合状态时， ct的梯度可以直接沿着最下面这条短路线传递到ct-1，不受参数W的影响，这是LSTM能有效地缓解梯度消失现象的关键所在。 GRU公式 重置门重置门rt用于控制前一时刻隐层单元ht-1对当前词xt的影响。如果ht-1对xt不重要，即从当前词xt开始表述了新的意思，与上文无关， 那么rt开关可以打开， 使得ht-1对xt不产生影响。 更新门更新门zt用于决定是否忽略当前词xt。类似于LSTM中的输入门it， zt可以判断当前词xt对整体意思的表达是否重要。当zt开关接通下面的支路时，我们将忽略当前词xt，同时构成了从ht-1到ht的”短路连接”，这梯度得已有效地反向传播。和LSTM相同，这种短路机制有效地缓解了梯度消失现象， 这个机制于highwaynetworks十分相似。 其他为什么GRU和LSTM能够避免梯度消失？ GRU和LSTM的结构有点类似残差网络模块，通过维持细胞状态，将之前的网络信息传递到后面，避免了梯度消失/梯度爆炸等问题的出现。 细胞状态是否包含着多种混合信息（为什么需要输出门）？ 在LSTM中，输入门判断当前输入词对全局的重要性，而遗忘门判定上一时刻的细胞状态对当前的细胞状态的重要性。因此，当结合输入门和遗忘门更新完细胞状态后，当前的细胞状态不仅包含了与该时刻的输出相关的信息，可能也包含了和之后时刻相关的信息。此时，输出门的作用就是判断当前的细胞状态哪些对当前输出是有用的，哪些是无用的。","link":"/categories/deep-learning/RNN，循环神经网络/"},{"title":"Spherical Hashing，球哈希","text":"Introduction在传统的LSH、SSH、PCA-ITQ等哈希算法中，本质都是利用超平面对数据点进行划分，但是在D维空间中，至少需要D+1个超平面才能形成一个封闭、紧凑的区域。而球哈希方法利用超球面（hypersphere）对数据进行划分，在任何维度下，只需要1个超球面便可形成一个封闭的区域。利用球哈希方法，每个区域内样本的最大距离的平均值会更小，说明各个区域的样本是更紧凑的。这样更符合邻近的含义，更适合在进行相似搜索时使用。 Binary Code Embedding Function球哈希的函数族$H(x) = (h_1(x), h_2(x), h_3(x), … h_L(x))$。L为哈希编码的位数，其中每个哈希函数事实上就是一个超球面，每个超球面将空间划分为球内和球外两部分。哈希函数如下： 其中，$p_k$和$t_k$分别为球心和半径，$d(p_k, x)$表示点x与球心$p_k$之间的欧式距离。如果点到球心的距离大于半径，则编码为-1，否则编码为1。 为了比较基于超平面的区域和基于超球面的区域，不同紧凑性对结果的影响，论文作了如下两个实验： 左图中的Y轴表示在相同的编码下的哈希空间里，数据点中最大距离的平均值，X轴为不同的码长。实验结果曲线表明了基于超球面形成的区域更加紧凑，利用较少的码长便可对原始数据进行很好的编码；右图的Y轴为在相同的编码下，所对应的原始空间中数据点的最大距离，X轴为两个数据点的编码中，相同比特位均为+1的个数。实验结果除了超球面区域紧凑性的验证之外，还表明了如果两个数据点具有越多的相同特征，那么这两个数据点距离越近（越相似）。 对于右图的实验结果，个人直观的理解便是假设有类别A具有a、b、c三个特征，如果数据$x_1$和$x_2$均具有这三个特征（对应比特位均为+1），那么我们大致可以判定$x_1$和$x_2$均属于类别A；但是如果$x_1$和$x_2$均不具有这三个特征（对应比特位均为-1），那么我们只能确定$x_1$和$x_2$均不属于类别A，但是无法得出$x_1$和$x_2$是否属于同类的结论。 Distance between Binary Codes传统的哈希方法使用汉明距离作为衡量数据点中距离的方法，但是汉明距离无法对区域紧凑性进行很好的表征。因此，在球哈希方法中，使用了新的距离度量方法Shpherical Hamming distance，SHD： 其中，分子为两个编码中对应比特位不同的个数，分母则为对应比特位均为+1的个数。显然，当两个数据对应比特位均为+1的个数越多时，其对应的SHD距离越小，反之则越大，很好得体现了利用了基于超球面的区域紧凑型的特点。 Independence between Hashing Functions球哈希方法中同样也对哈希函数的平衡性和独立性进行了限制。 平衡性： 独立性： 具体的图示如下： Iterative Optimization在初始化时，从原始数据集中生成一个m大小的子集S，在子集S中随机选择C个数据点作为初始球心，初始球心的选择应该能大致反应数据集在空间中的分布情况，以减少后边的优化开销。在球心选择后，根据平衡性和独立性的限制便可得到半径。之后，球哈希函数训练可分为两个阶段。 第一阶段：根据平衡性的限制，调整球心，使得$O_{i,j}$的值尽可能接近$4/m$。在此过程中，定义了两个球心的作用力如下所示： 为了满足平衡性的条件，当两个球的重合过多时，应产生排斥力将其分开；当两个球距离过远时，应产生吸引力使得互相靠近。而上述公式的原理便是通过利用$(O_{i,j} - m/4)$和$(p_i - p_j)$的符号正负关系实现排斥力和吸引力。而分母中的$4/m$是为了保证力的大小不受数据集大小m的影响。 第二阶段：当球心通过作用力更新位置完毕后，我们通过独立性的限制来调整半径$t_k$的大小。 在第一阶段中，理想情况是$O_{i,j}$的均值和标准差分别为$m/4$和0，但是这样容易产生过拟合，因此我们对均值和标准差设定了两个阈值——10%和15%，在这两个阈值下，算法具有最优的表现。 综述，整个球哈希算法的过程如下所示：","link":"/categories/computer-vision/Spherical-Hashing，球哈希/"},{"title":"softmax分类器","text":"wiki百科：softmax函数的本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。 softmax公式意义在softmax函数，输入向量z的值有正有负，正数表示对应的特征对分类结果是积极的，负数则表示是消极的。因此，在softmax函数中，要 先计算$e^z$, 目的是为了把所有的输入先处理到大于0的空间内，比如负数经过计算后会得到很接近0的数，因此归一化后，对应的概率也接近于0，这就很好得体现了softmax函数的思想——值大的对应概率大，值小的对应概率小。 softmax回归与logistic回归 softmax回归，处理多分类问题；logisitc回归，处理二分类问题； softmax回归可以推导出和二元分类logistic回归一致的公式；多个logistic回归通过叠加也同样可以实现多分类的效果； softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类： 这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。） 多个logistic回归进行多分类，输出的类别并不是互斥的，即”苹果”这个词语既属于”水果”类也属于”3C”类别： 如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的logistic回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。 交叉熵损失从概率论的角度出发，最小化正确分类的负对数概率，等同于进行最大似然估计。 数值稳定问题编程实现softmax函数计算的时候，中间项和因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以要使用归一化技巧——在分式的分子和分母都乘以一个常数，并把它变换到求和之中，就能得到一个从数学上等价的公式： 的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将设为。该技巧简单地说，就是应该将向量中的数值进行平移，使得最大值为0。","link":"/categories/deep-learning/softmax分类器/"},{"title":"一群可爱的人","text":"请输入博文的阅读密码:： Incorrect Password! No content to display! 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","link":"/categories/essay/一群可爱的人/"},{"title":"九大排序算法汇总","text":"算法说明数据结构中经常需要用到各种排序算法，故参考网上代码，将九个排序算法整合在一起，以便日后使用。算法运行时，可以选择所要采用的排序算法，并会输出每一趟的排序过程，更利于对排序算法的理解。 运行截图 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297#include&lt;stdio.h&gt;#include&lt;math.h&gt;#include&lt;stdlib.h&gt;#define MAX_SIZE 100int n, order, step;int a[MAX_SIZE], temp[MAX_SIZE];//输出排序情况 void print(int a[]);//交换两个元素void swap(int a[], int i, int j); //插入排序void Insert_sort(int a[], int size);//冒泡排序void Bubble_sort(int a[], int size);//选择排序void Selection_sort(int a[], int size);//快速排序void Quick_sort(int a[], int l, int r);//归并排序void Merge_sort(int a[], int temp[], int l, int r);//希尔排序void Shell_sort(int a[], int size);//堆排序void Heap_sort(int a[], int size);//基数排序void Radix_sort(int a[], int size);//计数排序void Count_sort(int a[], int temp[], int k, int size); int main() { while (~scanf(\"%d\", &amp;n)) { step = 0; for (int i = 0; i &lt; n; i ++) { scanf(\"%d\", &amp;a[i]); } printf(\"1.插入排序\\n2.冒泡排序\\n3.选择排序\\n4.快速排序\\n5.归并排序\\n6.希尔排序\\n7.堆排序\\n8.基数排序\\n9.计数排序\\n\"); printf(\"请选择排序方法：\"); scanf(\"%d\", &amp;order); switch(order) { case 1: { Insert_sort(a, n); break; } case 2: { Bubble_sort(a, n); break; } case 3: { Selection_sort(a, n); break; } case 4: { Quick_sort(a, 0, n - 1); break; } case 5: { Merge_sort(a, temp, 0, n - 1); break; } case 6: { Shell_sort(a, n); break; } case 7: { Heap_sort(a, n); break; } case 8: { Radix_sort(a, n); break; } case 9: { Count_sort(a, temp, 9999, n); break; } default: { break; } } } return 0;}//输出排序情况 void print(int a[]) { printf(\"第%d趟排序：\", ++ step); for (int i = 0; i &lt; n; i ++) { printf(\"%d \", a[i]); } printf(\"\\n\");}//交换两个元素void swap(int a[], int i, int j) { int temp = a[i]; a[i] = a[j]; a[j] = temp;} //插入排序void Insert_sort(int a[], int size) { for (int i = 1; i &lt; size; i ++) { int temp = a[i]; int j = i - 1; while (j &gt;= 0 &amp;&amp; temp &lt; a[j]) { a[j + 1] = a[j]; j --; } a[j + 1] = temp; print(a); }}//冒泡排序void Bubble_sort(int a[], int size) { for (int j = 0; j &lt; size - 1; j++) { for (int i = 0; i &lt; size - 1 - j; i++) { if (a[i] &gt; a[i + 1]) { swap(a, i, i + 1); } } print(a); }} //选择排序void Selection_sort(int a[], int size) { for (int i = 0; i &lt; size - 1; i ++) { int min = i; for (int j = i + 1; j &lt; size; j ++) { if (a[j] &lt; a[min]) { min = j; } } if (min != i) { swap(a, min, i); } print(a); }}//快速排序void Quick_sort(int a[], int l, int r) { if (l &lt; r) { int i = l, j = r, x = a[l]; while (i &lt; j) { while(i &lt; j &amp;&amp; a[j] &gt;= x) j --; if(i &lt; j) a[i ++] = a[j]; while(i &lt; j &amp;&amp; a[i] &lt; x) i ++; if(i &lt; j) a[j --] = a[i]; } a[i] = x; print(a); Quick_sort(a, l, i - 1); Quick_sort(a, i + 1, r); } }//归并排序：合并操作 void Merge(int a[], int temp[], int l, int mid, int r) { int i = l, j = mid + 1, k = l; while(i != mid + 1 &amp;&amp; j != r+1) { if(a[i] &lt; a[j]) temp[k ++] = a[i ++]; else temp[k ++] = a[j ++]; } while(i != mid + 1) temp[k ++] = a[i ++]; while(j != r + 1) temp[k ++] = a[j ++]; for(i = l; i &lt;= r; i ++) a[i] = temp[i]; print(a);}//归并排序void Merge_sort(int a[], int temp[], int l, int r) { if(l &lt; r) { int mid = (l + r) / 2; Merge_sort(a, temp, l, mid); Merge_sort(a, temp, mid + 1, r); Merge(a, temp, l, mid, r); }} //希尔排序void Shell_sort(int a[], int size) { for (int gap = n / 2; gap &gt; 0; gap /= 2) { for (int i = 0; i &lt; gap; i++) { for (int j = i + gap; j &lt; n; j += gap) { if (a[j] &lt; a[j - gap]) { int temp = a[j]; int k = j - gap; while (k &gt;= 0 &amp;&amp; a[k] &gt; temp) { a[k + gap] = a[k]; k -= gap; } a[k + gap] = temp; } } } print(a); }}//堆排序：从i节点开始调整,n为节点总数 从0开始计算 i节点的子节点为 2*i+1, 2*i+2 void HeapAdjust(int a[], int i, int n) { int j, temp; temp = a[i]; j = 2 * i + 1; while (j &lt; n) { if (j + 1 &lt; n &amp;&amp; a[j + 1] &gt; a[j]) //在左右孩子中找最大的 j++; if (a[j] &lt;= temp) break; a[i] = a[j]; //把较大的子结点往上移动,替换它的父结点 i = j; j = 2 * i + 1; } a[i] = temp; } //堆排序：建立最大堆 void BuildHeap(int a[], int n) { for (int i = n / 2 - 1; i &gt;= 0; i--) HeapAdjust(a, i, n); } //堆排序void Heap_sort(int a[], int size) { BuildHeap(a, size); for (int i = n - 1; i &gt;= 1; i--) { swap(a, i, 0); HeapAdjust(a, 0, i); print(a); } }//基数排序void Radix_sort(int a[], int size) { int *radixArrays[10]; for (int i = 0; i &lt; 10; i++) { radixArrays[i] = (int *)malloc((size + 1) * sizeof(int)); radixArrays[i][0] = 0; } for (int pos = 0; pos &lt; 10; pos ++) { int ok = 0; //最大元素是否已经排序完毕 for (int i = 0; i &lt; size; i ++) { int num = (a[i] / (int) pow(10, pos)) % 10; //num为每个元素个位、十位、百位。。。等的数字 if (num == 0) ok ++; int index = ++ radixArrays[num][0]; //index为每个桶的元素个数 radixArrays[num][index] = a[i]; } if (ok == size) break; //收集过程 for (int i = 0, j = 0; i &lt; 10; i ++) { for (int k = 1; k &lt;= radixArrays[i][0]; k++) a[j++] = radixArrays[i][k]; radixArrays[i][0] = 0; } print(a); } }//计数排序void Count_sort(int a[], int temp[], int k, int size) { // a为输入数组，temp为输出数组，k表示有所输入数字都介于0到k之间 int c[k]; // 初始化 for (int i = 0; i &lt; k; i++) { c[i] = 0; } // 检查每个输入元素，如果一个输入元素的值为a[i],那么c[a[i]]的值加1，此操作完成后，c[i]中存放了值为i的元素的个数 for (int i = 0; i &lt; n; i++) { c[a[i]]++; } // 通过在c中记录计数和，c[i]中存放的是小于等于i元素的数字个数 for (int i = 1; i &lt; k; i++) { c[i] = c[i] + c[i - 1]; } // 把输入数组中的元素放在输出数组中对应的位置上 for (int i = n - 1; i &gt;= 0; i--) {// 从后往前遍历 temp[c[a[i]] - 1] = a[i]; c[a[i]]--;// 该操作使得下一个值为a[i]的元素直接进入输出数组中a[i]的前一个位置 } for (int i = 0; i &lt; n; i ++) { a[i] = temp[i]; } print(a);}","link":"/categories/algorithms/九大排序算法汇总/"},{"title":"卷积神经网络","text":"卷积计算方式im2col操作用矩阵乘法实现：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法： 输入图像的局部区域被im2col操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到im2col操作的输出矩阵X_col的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。 卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵W_row，尺寸为[96x363]。 现在卷积的结果和进行一个大矩阵乘np.dot(W_row, X_col)是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。 结果最后必须被重新变为合理的输出尺寸[55x55x96]。 这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在X_col中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的BLAS API）。还有，同样的im2col思路可以用在汇聚操作中。 归一化层（Batch normalization）定义批量归一化。让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。 优点 BN解决了反向传播过程中的梯度问题（梯度消失和爆炸），同时使得不同scale的 整体更新步调更一致。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。 减少坏初始化的影响； 加快模型的收敛速度； 可以用大些的学习率 能有效地防止过拟合。 前向传播过程公式 反向传播求导公式 代码1234567sample_mean = K.mean(X, axis=-1, keepdims=True)#计算均值 sample_var = K.std(X, axis=-1, keepdims=True)#计算标准差 X_normed = (X - sample_mean) / (sample_var + self.epsilon)#归一化 out = self.gamma * X_normed + self.beta#重构变换running_mean = momentum * running_mean + (1 - momentum) * sample_meanrunning_var = momentum * running_var + (1 - momentum) * sample_var out = self.gamma * X_normed + self.beta 这个操作为scale and shift操作。为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入（即当），从而保证整个network的capacity。（实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。） 当引入BN层，原始的数据分布可能会因此遭到破坏，从而导致网络的loss变大，则在反向传播中，可以使用梯度更新规则对参数gamma和beta进行更新，从而接用“scale and shift”操作，以求可能保持原输入的部分特征。 我们训练时使用一个minibatch的数据，因此可以计算均值和方差，但是预测时一次只有一个数据，所以均值方差都是0，那么BN层什么也不干，原封不动的输出。这肯定会用问题，因为模型训练时是进过处理的，但是测试时又没有，那么结果肯定不对。 解决的方法是使用训练的所有数据，也就是所谓的population上的统计。不过这需要训练完成之后在多出一个步骤。一种常见的办法就是基于momentum的指数衰减，这和低通滤波器类似。每次更新时把之前的值衰减一点点（乘以一个momentum，一般很大，如0.9,0.99），然后把当前的值加一点点进去(1-momentum)。 疑问assignments2 在代码文件FullyConnectedNets.ipynd 中，有代码如下： 1234567891011121314151617181920# Test the affine_forward functionnum_inputs = 2input_shape = (4, 5, 6)output_dim = 3input_size = num_inputs * np.prod(input_shape)weight_size = output_dim * np.prod(input_shape)x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)b = np.linspace(-0.3, 0.1, num=output_dim)out, _ = affine_forward(x, w, b)correct_out = np.array([[ 1.49834967, 1.70660132, 1.91485297], [ 3.25553199, 3.5141327, 3.77273342]])# Compare your output with ours. The error should be around 1e-9.print 'Testing affine_forward function:'print 'difference: ', rel_error(out, correct_out) Q：此处用np.prod和 np.linspace等一系列函数初始化权重w和x，与之前直接用np.random等函数想比略显复杂，如此初始化的好处是什么？ A： 在这个代码模块里，最主要是为了测试前向传播函数是否实现正确，所以需要固定的权重和数据来得出结果，以和函数的输出进行对比。而之前随机生成的数据输出结果也是随机的，无法用于判定实现的前向传播函数是否正确。当用多层FC网络过拟合50个样本时，如果网络层数越深，随机初始化权重时，所用的weight_scale应当越大点。 Q: 在神经网络训练开始前，都要对输入数据做一个归一化处理，那为什么需要归一化呢？ A: 神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。","link":"/categories/deep-learning/卷积神经网络/"},{"title":"因子分析模型","text":"协方差矩阵协方差矩阵为对称矩阵。 在高斯分布中，方差越大，数据分布越分散，方差越小发，数据分布越集中。 在协方差矩阵中，假设矩阵为二维，若第二维的方差大于第一维的方差，则在图像上的体现就是：高斯分布呈现一个椭圆形，且主轴对应的就是方差大的第二维度。简而言之，若对角线元素相等，则高斯分布的图形是圆形，反之则分布图形为椭圆形。 若协方差矩阵的非对角元素为0，则高斯分布图形平行于坐标轴，反之则不平行。 为什么当样本数量远小于特征向量的维数n时，协方差逆矩阵不存在（矩阵不满秩）？ 在多变量高斯分布中，协方差矩阵和均值刻画了每个维度的特征，n维可以理解为有n个未知量，每一个样本可以构造一个等式，如果样本数量小于未知量n,那么这个n元方程组将无法求解。 此外，在多变量高斯分布中，公式里包含了协方差矩阵的行列式和逆矩阵，如果不满秩，则公式无法表达。 为什么限制了协方差矩阵为对角矩阵，那么高斯分布的形状就会和坐标轴平行？ 限制协方差矩阵为对角矩阵，意味着不同维度之间的协方差为0，则会使得模型丢失了不同维度之间的相关性。 因子分析模型为什么因子分析模型可以解决样本数量少于特征维度n的问题？ 假设对于某个问题，有m个n维的样本数据，若m小于n,则协方差矩阵就不可逆，高斯分布的公式也无法得解，而在因子分析模型中，将n维的数据视为由d维（d &lt; n）的变量经过一定的变换得到的，从而降低了问题的维度，使得m &gt; n。（个人理解，不一定对） 假设可以解释为：每个点x都是由d维正态随机变量z生成。","link":"/categories/machine-learning/因子分析模型/"},{"title":"基于朴素贝叶斯的中文多分类器","text":"算法说明 为了便于计算类条件概率$P(x|c)$,朴素贝叶斯算法作了一个关键的假设：对已知类别，假设所有属性相互独立。 当使用训练完的特征向量对新样本进行测试时，由于概率是多个很小的相乘所得，可能会出现下溢出，故对乘积取自然对数解决这个问题。 在大多数朴素贝叶斯分类器中计算特征向量时采用的都是词集模型，即将每个词的出现与否作为一个特征。而在该分类器中采用的是词袋模型，即文档中每个词汇的出现次数作为一个特征。 当新样本中有某个词在原训练词中没有出现过，会使得概率为0，故使用拉普拉斯平滑处理技术解决这一问题。对应公式如下： \\phi_{j|y=2}=\\frac{\\sum_{i=1}^{m} 1\\left\\{z_{i}\\right\\}+1}{m+k}数据源在该模型中，所用到的训练数据和测试数据均来自于搜狗分类语料库，并选择了体育类、财经类和教育类这三种新闻的各40个样本，以作为该多分类器的输入数据。 中文分词为了对文本完成分词，对于英文文本而言，只需要简单得利用str.split(&quot; &quot;)，用空格对整个英文文本进行切割即可。而对于中文文本而言就相对复杂了点，因为在中文文本中，往往包含了中文、英文、数字、标点符号等多种字符，此外中文中常常是多个词组连接起来组成一个句子，所以也无法类似英文那样简单利用某个符号进行分割。为了完成中文文本的分词，使用了如下的文本过滤算法： 123456789stopWords = open(\"stop_words.txt\", encoding='UTF-8').read().split(\"\\n\")def textParse(inputData): import re global stopWords inputData = \"\".join(re.findall(u'[\\u4e00-\\u9fa5]+', inputData)) wordList = \"/\".join(jieba.cut(inputData)) listOfTokens = wordList.split(\"/\") return [tok for tok in listOfTokens if (tok not in stopWords and len(tok) &gt;= 2)] 利用正则表达式u'[\\u4e00-\\u9fa5]+'过滤掉输入数据中的所有非中文字符； 在Python下，有个中文分词组件叫做jieba，可以很好得完成对中文文本的分词。在这里便是利用jieba中的cut函数&quot;/&quot;.join(jieba.cut(inputData))完成对中文的分词，并且以“/”作为分隔符。 在中文文本中，存在在大量的停用词，这些停用词对于表示一个类别的特征没有多少贡献，因此必需过滤掉输入数据中的停用词。这里所用到的stop_words.txt ,包含了1598个停用词，利用tok not in stopWords过滤掉输入数据中的停用词。 在利用jieba完成分词后，往往会存在大量长度为1的词（不在停用词表里），这些词对特征表示同样贡献不大，利用len(tok) &gt;= 2将其过滤掉。 通过以上过程，便完成了中文文本的分词。分词结果如下： 分类结果为了对分类器的泛化误差进行评估，遂使用留存交叉验证法，即从输入的40 * 3共120个样本中，随机选中20个样本作为测试数据，其他100个样本作为训练数据，以此来测定泛化误差。 经过10次测试，得到分类器的泛化误差为：（0.1 + 0.0 + 0.1 + 0.1 = 0.0 + 0.05 + 0.1 + 0.15 + 0.1 + 0.25）/ 10 = 0.095，可见该中分多分类器在新样本上的表现还是很好的。其中部分分类结果如下所示：","link":"/categories/machine-learning/基于朴素贝叶斯的中文多分类器/"},{"title":"多类SVM","text":"梯度求导SVM的损失函数在某个数据点上的计算： 对函数进行微分，比如对进行微分得到： 在代码实现的时候，只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些行的梯度是： 数据集划分 训练集：训练模型用； 验证集：用以最优参数的调试选择； 测试集：测定训练好的模型的表现情况； 开发集：在实际开发中，训练集往往包含很多数据，为了节省计算时间，往往使用训练集的一小部分子集，用作模型的训练。 梯度检查数值梯度和解析梯度的值有时候在某个维度会相差较多。这是因为数值梯度的计算采用的是用前后2个很小的随机尺度（比如0.00001）进行计算，当loss不可导的时候，两者会出现差异。** 损失函数 SVM只关心正确分类的得分与错误分类的得分至少要高于边界值，若不满足，便计算相应的损失值。 折叶损失：max(0, -)$函数。 平方折叶损失：$max(0, -)^2$**。更强烈得惩罚过界的边界值。在某些数据集会工作得更好，可以通过交叉验证来决定使用哪个损失计算函数。 由于max操作，损失函数中存在一些不可导点（kinks），这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是次梯度依然存在且常常被使用。 Delta设置超参数delta和$\\lambda$一起控制损失函数中数据损失和正则化损失之间的权衡。但是对于通过缩小或扩大权重矩阵的值，改变不同分类值之间的差异，因此，在一定程序上改变delta是没有意义的。真正的权衡是通过正则化强度来控制权重能够变大到何种程度。 初始化损失函数的最优化的始终在非限制初始形式下进行。很多这些损失函数从技术上来说是不可微的（比如当时，函数就不可微分），但是在实际操作中并不存在问题，因为通常可以使用次梯度。","link":"/categories/deep-learning/多类SVM/"},{"title":"大学之路","text":"请输入博文的阅读密码:： Incorrect Password! No content to display! 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","link":"/categories/essay/大学之路/"},{"title":"如何成为一个更好的交谈者","text":"不要三心二意；专注当下。 不要好为人师；准备好在谈话中学习。 使用开放性的问题；who,where,when,hwo,what 顺其自然；不要固着在自己的计划和念头上 谨慎发言，dont be cheap 不要把自己的经历和别人比较,时尚没有一样的经历。 别重复表达； 少说废话；别人不关心你要说的细节，而是你是怎样一个人。 认真倾听；谈话的目的是理解而不是回应。 简明扼要。","link":"/categories/essay/如何成为一个更好的交谈者/"},{"title":"应用机器学习的建议","text":"偏差方差权衡 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过归一化手段来调整而更加适应数据。 通常选择较大的神经网络并采用归一化处理会比采用较小的神经网络效果要好。 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络 损失函数和收敛情况 在NG的BLR和SVM的例子中，通过a指标和目标函数的对比来判断问题出在收敛情况或则目标函数上。但是讲义里面的“&gt;”和“&lt;”符号好像写反了。 目标函数和损失函数的区别？ 误差分析和销蚀分析对顺序敏感，所以需要经过多次试验。 训练集、交叉验证集和测试集训练集是用来学习的样本集，通过匹配一些参数来建立一个分类器。 验证集是用来调整分类器的参数的样本集，比如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。 测试集纯粹是为了测试已经训练好的模型的分类能力的样本集。 划分方法 数据集：trainning set, cross validation set, test set. 数据集：training set和test set 对第二种分法来说，取得min(Err(test_set))的model作为最佳model，但是我们并不能评价选出来的这个model的性能，如果就将Err(test_set)的值当作这个model的评价的话，这是不公正的，因为这个model本来就是最满足test_set的model 相反，第一种方法取得min(Err(cv_set))的model作为最佳model，对其进行评价的时候，使用剩下的test_set对其进行评价 而不是使用Err(cv_set))的值","link":"/categories/machine-learning/应用机器学习的建议/"},{"title":"战争中的道德","text":"中国发明了火药，且用火药制造烟花用于庆礼；火药传入西方国家后，西方国家用火药制造弹药、炸弹，进行军事改革，并且用中国发明的火药炸开了中国的国门。面对这一史实，许多中国人无不痛恨当时政府的无能，嘲笑当时政府的迂腐可笑，抱怨他们为什么只懂得享受，没有看到世界之大，不会利用火药去强大国家，导致中国跟不上世界工业革命的脚步，以至于传承五千多年的文明古国在如今却要在仅仅建国一百多年的美国下唯唯诺诺！ 确实如此，但是现在我们来看看另外一些东西—— 英国作家乔纳森·斯威夫特的作品《格列佛游记》中，格列佛游历四方，当他来到大人国时，向国王提出他可以帮国王制造炸弹来维护其统治，却遭到国王的极力反对，并质疑格列佛的道德品质。大人国国王用理智、仁慈、公理来治理国家，厌恶格列佛所说的卑劣的政客，流血的战争。作者写这些，是为了揭发战争本质，突显英国残酷的黑暗社会，而当读者读到这里时，也与作者产生“共鸣”，“感慨”到一个和平的社会是如此难以建设，并也跟作者一样，对战争产生厌恶，为格列佛那充满“侵略性”的思想感到可耻，对大人国国王那高尚的品德感到钦服，中国人亦是如此的想法！ 所以现在，我们先抛开一些对中国当时政府的负面评价（因为这些评价可能是上一些的愤青所写出来的东西，并不代表事实的全部），我们是不是可以认为，当时的中国政府就如大人国国王一样，反对侵略战争，用其发明来使中国的百姓们安居乐业，喜喜庆庆的，并不用整天为参军的丈夫儿子的性命担忧？这样看的话，中国政府的品德之高尚，何人能及？ 历史告诉我们侵略战争是世界发展的必然性，战争从个别方面来说可促进的世界的进程；而语文政治却告诉我们要树立高尚的品德，反对战争，共建和平社会，说什么战争会破坏环境，浪费极大的物资人力，使许多人流离失所。作为新生代的我们，对世界、对历史、对未知的事物都充满着强烈的好奇，渴望了解更多的东西，然而对这种互相矛盾的教育事实，叫我们怎么如何去接受？ 或许是旧时中国政府的迂腐导致中国落后于西方国家，亦或他们那高尚的品德不允许他们作出侵略性的行为导致这种结果。总之，真正的史实就是一个笑话！胜者为王，败者为寇，假如当年解放战争的胜利者是蒋介石的话，那么今天的历史教科书所写的就是蒋介石为了中国人民，与反动派毛泽东等人周旋多年，终于胜利！并且歌颂蒋介石的高尚品德，写出他的伟大，并要求我们要谨记蒋介石等人的光辉事迹之类的东西，这些无非就是稳定中国的社会秩序罢了，因为如果中国的人们“知道”当年的开国主席是一个为了胜利不择手段的人的话，正义感会驱使他们反抗政府，这些当然就是每一个国家所不愿看到的了！ 其实，历史只是一本《故事会》！","link":"/categories/essay/战争中的道德/"},{"title":"毕设选导系统：测试","text":"测试用例1 测试内容 获取下拉框的输入测试 测试代码 12345678910$(\"#sub-confirm\").click(function() { gradeSelected = document.getElementById(\"gradeSelect\").value; departmentSelected = document.getElementById(\"departSelect\").value; QUnit.test(\"slecet test\", function(assert) { assert.equal(gradeSelect, \"2014\"); assert.equal(departmentSelected, \"计算机系\"); });}); 预期输入 在选择年级下拉框，选择“2014级”。 在选择系别下拉框，选择“计算机系”。 实际输入 在选择年级下拉框，选择“2014级”。 在选择系别下拉框，选择“信息安全与网络系”。 测试结果 未通过测试！ 预期输入 在选择年级下拉框，选择“2014级”。 在选择系别下拉框，选择“计算机系”。 实际输入 在选择年级下拉框，选择“2014级”。 在选择系别下拉框，选择“计算机系”。 测试结果 通过测试！ 测试用例2 测试内容 修改个人信息页面的确认密码测试，检查两次输入的密码是否一致 测试代码 1234567891011121314$('#newPasswordConfirm').change(function(){ newPW = $(\"#newPassword\").val(); newPWC = $(\"#newPasswordConfirm\").val(); QUnit.test(\"password test\", function(assert) { assert.equal(newPW, newPWC); }); // if (newPW == newPWC) { // $(\"#newPwConfirmWrong\").css(\"display\",\"none\"); // } // if (newPW != newPWC) { // $(\"#newPwConfirmWrong\").css(\"display\",\"block\"); // modify.newPasswordConfirm.focus(); // }}) 预期输入 在新密码一栏中输入，”fzu2016SE” 在确认密码一栏中输入，”fzu2016SE” 实际输入 在新密码一栏中输入，”fzu2016SE” 在确认密码一栏中输入，”fzu2016se” 测试结果 未通过测试！ 预期输入 在新密码一栏中输入，“woshuodedoudui” 在确认密码一栏中输入，“woshuodedoudui” 实际输入 在新密码一栏中输入，“woshuodedoudui” 在确认密码一栏中输入，“woshuodedoudui” 测试结果 通过测试！ 测试用例3 测试内容 测试系负责人搜索框输入的内容是否为空 测试代码 1234567891011121314function listenSearchEvent() { $(\".btn-search\").click(function () { var data = $(this).parent().parent().children(); var department = data[0].innerText; // searchteacher.departments=department; var selectinformation = $(this).parent().children(); var headname=selectinformation[0].value; QUnit.test(\"search test\", function(assert) { assert.notEqual(headname, \"\"); }); });} 预期输入 在系负责人列表搜索框中输入”张栋”老师，点击搜索按钮 实际输入 未在搜索框中作任何输入，直接点击搜索按钮 测试结果 未通过测试！ 预期输入 在系负责人列表搜索框中输入”张栋”老师，点击搜索按钮 实际输入 在系负责人列表搜索框中输入”张栋”老师，点击搜索按钮 测试结果 通过测试！ 测试用例4 测试内容 导入excel表格时，对上传类型的判断测试 测试代码 12345678910111213141516171819function initUpload() { var response = \"\"; uploadObj = $(\"#fileuploader\").uploadFile({ url: api_teacher_excel_upload, fileName: \"excel_file\", uploadStr: \"上传文件\", abortStr: \"停止\", cancelStr: \"取消\", deletelStr: \"删除\", doneStr: \"完成\", onSuccess: function (files, data, xhr, pd) {}, onError: function (files, status, message, pd) {}, onSelect: function (files) { // 测试文件类型是否为xls QUnit.test(\"upload test\", function(assert) { assert.equal(files[0].name.split(\".\")[1], \"xls\"); }); } }); 预期输入 点击“从文件导入”按钮，在弹出框内选择”上传文件”，然后选择 test.xls文件，点击”确认导入”。 实际输入 点击“从文件导入”按钮，在弹出框内选择”上传文件”，然后选择 test.txt文件，点击”确认导入”。 测试结果 未通过测试！ 预期输入 点击“从文件导入”按钮，在弹出框内选择”上传文件”，然后选择 test.xls文件，点击”确认导入”。 实际输入 点击“从文件导入”按钮，在弹出框内选择”上传文件”，然后选择 test.xls文件，点击”确认导入”。 测试结果 通过测试！ 测试用例5 测试内容 在匹配设置中，测试学生人数设置是否有出现负数 测试代码 123456789101112131415161718192021function validate() { var formNum = $(\"input[type='number']\"); var isNegative = numIsNegative(formNum); QUnit.test(\"isNegative test\", function(assert) { assert.equal(isNegative, false); });}function numIsNegative(formNum) { var idSet = new Array(); var isNegative = false; for (var i = 0; i &lt; formNum.length; ++i) { if (formNum[i].value !== \"\" &amp;&amp; formNum[i].value &lt; 0) { idSet.push(formNum[i].id); isNegative = true; } } displayWarnStyleNum(idSet); return isNegative;} 预期输入 在学生人数设置栏目的”最多人数”中，填入人数”10”，其他都填入非负数。 实际输入 在学生人数设置栏目的”最多人数”中，填入人数”-100”，其他都填入非负数。 测试结果 未通过测试！ 预期输入 在学生人数设置栏目的”最多人数”中，填入人数”9”，其他都填入非负数。 实际输入 在学生人数设置栏目的”最多人数”中，填入人数”9”，其他都填入非负数。 测试结果 通过测试！","link":"/categories/software-engineering/毕设选导系统：测试/"},{"title":"浅谈教育","text":"十几天的寒窗苦读难道仅仅只是为了高考吗？ 我们从一出生就被各式各样的“考”所缠身。小考、中考、高考，这其间还夹杂着太多太多的测试！老师们不停地强调着各类考试的重要性，将其与人生航向联系起来，还自创什么“此考若失利，人生即悲剧”来激励我们。背负了太多压力的我们不得不整日伏案提笔，做着那永远也做不完的考卷。 仅此而已吗？不！我们众位聪明绝顶的老师们将其应考十几年的“宝贵经验”传授与我们，坚持“易（一）分不失”的原则，至于那些与考试内容无关的只是就被忽略。确实如此，这些方法能让我们取得更高的分数，但有多少人有想过其中的弊端呢？知识的漏洞已经在“不考则不学”的思想下逐渐扩大。我们还无法准确得定位以后的人生航向，被忽略的知识或许能让临考前的你放下一些包袱，变得轻松一些，但若这些只是对你以后的人生至关重要呢？那是你才会深感“书到用时方恨少”把？ 老师在讲某一个重要的知识点时，总会说这个知识点高考一定会出现，然后经验老道得分析了这个知识点会以如何形式出现在试卷上，然后底下一大片稚嫩的同学们带着一脸佩服，听说高考会出现，都赶紧打起精神，在YY的赶紧回过神来，小两口在眉来眼去的立刻将那所谓忠贞的爱情丢在一边，玩手机的马上拿起笔神情专注了起来……呵！这种场景不是甚为讽刺吗？我们的学习只是为了高考吗？亲爱的老师们，为什么你们不教我们如何现学现用，将知识化用到实践当中，而是整天讲授那些考试技巧呢？ 我们还会经常看到这一幕——期待已久的考卷发了下来，看到各自成绩的同学们神情各异。差点的呢就立即狠下决心，要痛改前非，重新做人；好些的呢就洋洋得意，不停地询问周围同学的成绩，看到比自己差的呢，表面在安慰他人，其实内心则在暗暗自喜；而看到比自己高分的，甚至有时只是一分之差，内心便会立马失落许多，因为此时第一名的宝座已经被他人捷足先登了。 中国的教育的表面工作做得很好，说什么现在的课程有着语数英政史地生物化音乐体育信息等等，能促进学生全面发展之类的，看上去貌似如此，可是每一次临考前时课程的调换都深深的揭露了“全面发展”的本质，中国哪所学校在临考前不是将那些音乐体育等与考试无关的科目换成自习或则其他重要的科目呢？我们总会发现那些与考试无关的科任的老师上课都不大在意，其实大多数内心都认为自己所任教的科目反正又不用考试，同学知识是否能接受又影响不到自己的工作，而学生们却也自甘堕落，为什么呢？呵呵，反正也不用考试，对不对？！可悲！ 亲爱的同学们，你们有想过自己坐在教室里是为了什么吗？为了父母？为了自己以后的人生？如果你问学数学是为了什么，老师会告诉你数学可以拉分；如果你问整天背诵那些语政史是为了什么，老师会告诉你那些东西纯记忆性，该拿下的分要拿下；如果你问老师学习是为了什么，老师会告诉你高考需要…..呵！弄到最后我们都不知道自己学习是为了什么，只是整天程序性得跟着来教室而已!总是在想，如果一个人从小就立志长大后要当个歌手，而自身又符合当歌手的一切条件，那他整天去学校是干嘛呢？音乐课老师又不重视，如果他把用十几年去应付高考的时间去用来练习自己的歌声，追寻自己的梦想，那么谁敢断定他不会成为下一个迈克尔杰克逊呢？ 考考考，老师的法宝，分分分，学生的命根。我们在不停地对比之中扭曲了学习的本质，遗忘了学习的乐趣。我们的学习变得目的性十足，仅仅只是未来能在各类考试中取得比他人更高的分数；我们一心想赶往目的地，却忽略了沿途太多美丽的风景；我们不是一群只会做作业的机器，我们是一群有个性，有主见的热血青年，可我们的自由发展却被老师们冠以“不务正业”、“另类”、“幼稚”，要求我们必须把没一分每一秒都用在学习上；我们被限制了太多太多；我们成为”新式科举制度下的牺牲品。中国不是强调要人才强国吗？可是这样的教育培养出来的真的是人才吗？","link":"/categories/essay/浅谈教育/"},{"title":"神经网络：激活函数","text":"激活函数Sigmoidsigmoid将输入实数值“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。它对于神经元的激活频率有良好的解释：从完全不激活到在求和后的最大频率处的完全饱和（saturated）的激活。然而现在sigmoid函数实际很少使用了，这是因为它有两个主要缺点： Sigmoid函数饱和使梯度消失。sigmoid神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。 Sigmoid函数的输出不是零中心的。在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在中每个元素都），那么关于的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。 Tanh将实数值压缩到[-1,1]之间。和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：。 ReLU函数公式是。，这个激活函数就是一个关于0的阈值（如上图左侧）。使用ReLU有以下一些优缺点： 优点： 相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（6倍之多）。据称这是由它的线性，非饱和的公式导致的。 sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。 缺点： 在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。 神经网络结构命名规则当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。 输出层和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的（或者也可以认为它们有一个线性相等的激活函数）。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）。 表达能力 拥有至少一个隐层的神经网络是一个通用的近似器。给出任意连续函数和任意，均存在一个至少含1个隐层的神经网络（并且网络中有合理选择的非线性激活函数，比如sigmoid），对于，使得。换句话说，神经网络可以近似任何连续函数。 既然一个隐层就能近似任何函数，那为什么还要构建更多层来将网络做得更深？ 答案是：虽然一个2层网络在数学理论上能完美地近似所有连续函数，但在实际操作中效果相对较差。神经网络在实践中非常好用，是因为它们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法（例如梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。 层的尺寸尽可能使用大网络，然后用正则化技巧来控制过拟合。","link":"/categories/deep-learning/神经网络：激活函数/"},{"title":"神经网络：静态部分","text":"白化白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下： 123# 对数据进行白化操作:# 除以特征值 Xwhite = Xrot / np.sqrt(S + 1e-5) 警告：夸大的噪声。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。 注意任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。（避免过拟合等） 权重初始化 错误：全零初始化。 权重全零，会导致每个神经元都计算出同样的输出，在BP时也会计算出同样的梯度，从而进行同样的参数更新。** 小随机数初始化： W = 0.01 * np.random.randn(D,H)。并不是数值越小结果越好，要控制在一定的量级内才不会导致BP时梯度信号过小。 使用$1/sqrt(n)$校准方差。随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。公式如下: w = np.random.randn(n) / sqrt(n). 此外，基于BP时梯度的分析，神经网络算法使用ReLU神经元时的当前最佳推荐形式为： w = np.random.randn(n) / sqrt(2.0/n)。 批量归一化。让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。 正则化L2正则化L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。 L1正则化L1正则化让权重向量在最优化的过程中变得稀疏（即非常接近0）。即使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。 随机失活在训练的时候，随机失活的实现方法是让神经元以超参数$p$的概率被激活或者被设置为0。随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数。注意：在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以，调整其数值范围。 123456789101112131415161718192021222324\"\"\" 普通版随机失活: 不推荐实现 (看下面笔记) \"\"\"p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): \"\"\" X中是输入数据 \"\"\" # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &lt; p # 第一个随机失活遮罩 H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &lt; p # 第二个随机失活遮罩 H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略) def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p out = np.dot(W3, H2) + b3 反向随机失活。在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。 12345678910111213141516171819202122232425\"\"\" 反向随机失活: 推荐实现方式.在训练的时候drop和调整数值范围，测试时不做任何事.\"\"\"p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # 第一个随机失活遮罩. 注意/p! H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = (np.random.rand(*H2.shape) &lt; p) / p # 第二个随机失活遮罩. 注意/p! H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略)def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了 H2 = np.maximum(0, np.dot(W2, H1) + b2) out = np.dot(W3, H2) + b3 随机失活的解释。 1、使用许多小的模型集成的一个大模型。假设某些神经元被随机失活了，那么在BP中，与这些神经元相连的上一层的权重也不会更新，那么就相当于只对整个大模型的子网络进行了训练。 2、假设我们使用神经网络对猫这个类别进行检测，在神经网络中我们用到的特征可能有：耳朵、尾巴、眼睛等等，在标准的神经网络中，我们需要考虑每一个特征因素才能对猫进行得分计算。但是在测试集中，猫的图片是多种多样的，可能有时看不到尾巴或则耳朵，这就会影响了模型的泛化能力。而利用了随机失活，即在训练时我们随机得不考虑一些特征（例如耳朵）来训练模型，这样模型在测试集上一般能得到更好的泛化能力。 分类问题当面对一个回归任务，首先考虑是不是必须使用回归模型。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。","link":"/categories/deep-learning/神经网络：静态部分/"},{"title":"经验风险最小化","text":"经验风险最小化有限假设类情形对于Chernoff bound 不等式，最直观的解释就是利用高斯分布的图象。而且这个结论和中心极限定律没有关系，当m为任意值时Chernoff bound均成立，但是中心极限定律不一定成立。 随着模型复杂度（如多项式的次数、假设类的大小等）的增长，训练误差逐渐降低，而一般误差先降低到最低点再重新增长。训练误差降低，是因为模型越复杂，对于训练集合的拟合就越好。对于一般误差，最左边的端点表示欠拟合（高偏差），最右边的端点表示过拟合（高方差），最小化一般误差时，一般倾向于选取中间的模型复杂度，最小一般误差的区域。 经验风险最小化中，这个得到的函数具有最小的训练误差，但是如上图所示，并不具有最小的一般误差。 无限假设类情形VC维：只要存在大小为d的集合可以被某个假设空间分散，那么这个假设的VC维就是d。 在经验风险最小化中，最终的目的就是确定模型所需样本数的界限，这个界限是宽松的，这也是为什么在界限的表达时通常使用O这个符号来表示的原因。此外，这个界限对于符合任何分布的数据均成立，即使在最坏的情形下也是成立。但是在实际应用中，无法直接通过这个界限来确定我们所需的样本数量，因为在实际问题中，我们所研究的某个问题往往服从特定的分布，并不像最坏的情形那样糟糕，若直接将参数代入求解m的界，往往会得到非常大的m的值。 模型选择保留交叉验证法 通常只利用了70％左右的数据，造成了浪费 K折交叉验证法 每个模型都需要训练K次，需要大量的计算 留一交叉验证法 m = k,即每次只留下一个样本作为测试数据 能够更充分得利用数据，但是计算量更大 当数据非常少时才适用 特征选择前向查找和反向查找 这两种算法是一种启发式搜索算法，并不保证一定能找到最优的特征集。 在文本分类问题中，特征向量往往非常大，一般是几万的量级，此时选用这两种算法不大合适，因为所需要的计算量太大了。 过滤特征选择 通过计算为每个特征向量$x_{i}$计算其对结果y的贡献值，然后选择贡献值最大的k个特征。 如何决定k取多少？一个方法是通过交叉验证，不停选择前一个特征、前两个特征、前三个特征等等，以此来决定要选择几个特征值。 贝叶斯统计和规则化 频率派：将参数$\\theta$视为未知的常量，并采用最大似然估计法去求解。 贝叶斯学派：将参数$\\theta$视为未知的随机变量。 贝叶斯统计和规则化，就是找出新的估计方法来代替原有的最大似然估计法，来减少过拟合的发生。","link":"/categories/machine-learning/经验风险最小化/"},{"title":"线性回归和逻辑回归","text":"线性回归批量梯度下降法 每次对参数进行一次迭代时，都要扫描一遍输入全集 算法可以收敛到局部最优值 当迭代多次之后，每次迭代参数的改变越小 随机梯度下降法 对于一个输入样本，对参数进行一次更新 算法通常不会收敛到局部最优值，整个过程类似在上山迂回下山，有时可能上山，有时可能下山，但算法的最后都会得到局部最优值附近的一个值 若输入数据非常多的时候，随机梯度下降比批量梯度下降更加合适 概率解释 在原式子里加入一个”error term“，之后得到这个“error tem”的正态分布，从而到处输出y和x、θ之间的概率关系。然后通过最大似然估计法，得到若要使得y的概率最大，等价于利用最小二乘法最小化θ。 局部加权线性回归 参数θ的数量随着训练数据的增大而变多，但是当训练数据量巨大的时候，每次预测所需要的代价都很高。 原训练数据需要保留，因为每当对一个新的数据X进行预测时，需要用到X周围的测试数据，从而得出θ的值。对于和测试点靠近的训练点数据，所得权值较高，而对于距离测试点很远的测试数据，所得权值就很小，这就是为什么叫做局部线性回归的原因。 对于线性回归，，利用训练数据求出θ之后，在对一个新的数据进行预测时，将不会再使用到原训练数据 局部加权线性回归是一种非参数学习算法，而线性回归是一种参数学习算法。 逻辑回归 核心在于使用了sigmod函数，使得函数输出的值分布在[0, 1]区间内。 在某些特定条件下，为了使得sigmod函数g(z)的输出为两个离散值：0和1.可以使用感知器学习算法。 逻辑回归推导到最后的公式形式和线性回归中的最小二乘形式几乎相同，但是它们属于不同的算法，因为h(θ)函数不同，导致了根本的差异。 牛顿法 在对参数θ进行极大似然估计时，可以采用Newton’s method。这个算法收敛的速度非常快（二次收敛），迭代次数也少，但是在每次迭代时，都需要计算一次Hessian矩阵，计算量和n有关。因此当量级偏少少，牛顿迭代法也是一个相当好的算法。 参数的数值可以任取，但是一般取为零向量。 海森价值函数: $J(\\Theta )=\\frac{1}{2}\\sum_{i=1}^{m}(\\Theta ^{T}x^{(a)} - y^{i})^{2}$ $H=X^{T}X$ 无论θ的初始值为什么，牛顿法迭代一次后即可得到：$\\Theta^{*}=(X^{T}X)^{-1}X^{T}\\vec{y}$, 即最小二乘法的解。 广义线性模型 线性回归和逻辑回归中的伯努利分布和高斯分布都可以转换为指数分布的形式。 在将伯努利分布转换为指数分布的过程中，可以得到sigmod函数，这就是之前为什么Logisitic regression刚好是sigmod函数的原因。（当然，还有更深层次的原因） 回归问题实战线性回归 局部加权回归（参数0.01） 局部加权回归中，参数设定非常重要，可能存在欠拟合和过拟合的情况。 逻辑回归 参数θ的有多种更新方法——梯度下降法和牛顿法等，务必掌握其优缺点，合理选用。","link":"/categories/machine-learning/线性回归和逻辑回归/"},{"title":"计算机视觉顶尖期刊和会议","text":"计算机视觉会议A类 CVPR: International Conference on Computer Vision and Pattern Recognition ICCV: International Conference on Computer Vision AAAI: AAAI Conference on Artificial Intelligence ICML: International Conference on Machine Learning NIPS: Annual Conference on Neural Information Processing Systems ACM MM: ACM International Conference on Multimedia IJCAI: International Joint Conference on Artificial Intelligence B类 ECCV: European Conference on Computer Vision C类 ACCV: Asian Conference on Computer Vision ICPR: International Conference on Pattern RecognitionBMVC: British Machine Vision Conference 计算机视觉刊物A类 TPAMI: IEEE Trans on Pattern Analysis and Machine Intelligence IJCV: International Journal of Computer Vision TIP: IEEE Transactions on Image Processing B类 CVIU: Computer Vision and Image Understanding Pattern Recognition C类 IET-CVI: IET Computer VisionIVC: Image and Vision Computing IJPRAI: International Journal of Pattern Recognition and Artificial Intelligence Machine Vision and Applications PRL: Pattern Recognition Letters 在做计算机视觉的相关企业里，目前比较认可的顶会主要是CVPR、ICCV（第一档），AAAI、NIPS、ECCV、IJCAI（第二档）。关于顶刊，IJCV和TPAMI不相上下，TIP次之。国内的硕士一般很难接触到这个层级。","link":"/categories/computer-vision/计算机视觉顶尖期刊和会议/"},{"title":"脑部开发奇想","text":"请输入博文的阅读密码:： Incorrect Password! No content to display! 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","link":"/categories/essay/脑部开发奇想/"},{"title":"领骑衫的朝圣之旅","text":"前言因为变态的考试月加上放假后嗨过头，这篇领骑衫的博客姗姗来迟，看到小明和林锦辛苦整理的博客书里面，领骑衫的博客独缺我一个，真是蜜汁尴尬，为自己死活改不掉的拖延症说一声抱歉。刚看到大史助教里面提到的拖延症患者，我可能就是其实一位吧。完美主义者加拖延癌晚期，活该自己天天熬夜哈哈哈~ 时光倒流到半年前，大二下的我们看着大三上数不尽的课程无从下手，渴望学到知识，又担心精力不足难以兼顾，于是各种招式层出不穷，问老师，问学长学姐，各种百度。在各种“小道消息”中，“软工”二字出现的次数却是最多的，伴随出现的字眼还有“魔鬼强度”、“永远做不完的作业”、“熬夜”、“值得”、“学到了很多”等。魔鬼吓退了一部分人，也迎来了一群知难而上的人。软工实践新颖的教学模式让2013级的学长学姐尝到了甜头，也让我对其充满了向往。毅然决然得把最多的分数都投给了软工实践。 从开学初的课程展望到最后的学期总结，软工实践伴随了我整个大三上学期。这一路下来喜忧参半，为理不完的需求碾转难眠，为团队的磨合沟通苦思冥想，更为团队每一次的进步欢欣雀跃。有人说，软工实践，付出了其他课程好几十倍的精力，最后也才是一门1.5学分的课程，不值得，然而只有真正从始至终经历下来的人，才会懂得这1.5学分究竟值不值得。软工实践的价值，并不是一个冷冰冰的学分可以体现得出来的，教与学生的更多是以前从来接触不到的项目经验以及软件开放模式。选题报告、需求分析、体系结构设计、编码冲刺，从一开始对项目一窍不通，再到最后团队合力开发出一款可能将真正投入使用的系统，其中的成就感真的是难以用言语形容出来。闲暇之余总会打开项目，不停地在各个页面间来回切换，回想着界面背后复杂的程序逻辑，彼时对“代码改变世界”这几字箴言的理解越发透彻。这句话，应当被奉为IT界的至理箴言，整个世界，都是在代码的支撑下发展起来的，“互联网+” 的兴行，让越来越多的人投奔了IT行业，而我也是其中的一员，带着自己的爱好，带着自己的梦想，以求用双手创造代码，用代码创造世界。 黄色领骑衫——象征着软工实践的最高荣誉，在软工的最后一节课上，我“意外”得收获了一件领骑衫，其实，这件领骑衫并不属于我，它属于“我说的都队”整个团队，属于在整个编码过程冲锋陷阵的技术大牛们，相比于他们连日连夜的付出，我更想把这件领骑衫给予他们。人人都想抱大腿，而我就比较厉害了，不但抱到了大腿，而且一次性抱到了六个大腿！队员们的高效编码才是这件领骑衫的真正来源，可惜衣服没有提供印字服务，不然我挺想在这件衣服上引上队伍的名字的，然后作为一件纪念品保存起来。 在其他人看来，我们的团队大牛云集，一路下来都是一帆风顺，其实不然，在期初的选题报告时，我们因为彼此的意见不合，连夜召开会议讨论到凌晨2点都没有结果。选题一变再变，前后改变了4次之多，而之后更是因为毕设系统的原因，从队员重组到投票否决再到最后抛弃我们擅长的Android转型为PHP组，团队之间大大小小出现了多次的矛盾摩擦，团队能否顺利走下去能成为了一个问题。不过，庆幸的是，通过之后不断的沟通，我们不仅顺利得走了下去，更是走得一片阳光大道！有言道，“我有一种思想，你有一种思想，彼此交换之后，我们都有了两种思想”，集思广益，意见的冲突才是一个团队真正的前进动力！ 实在一开始写这篇黄色领骑衫博客时，预想的是一种轻松诙谐的笔调，可是写着写着发现字里行间好像严肃了许多。以往在写团队的博客时候，都是想到什么写什么，洋洋洒洒几千字毫不拖泥带水得就完成了，然而这篇博客写了又改改了又写，短短不到两千字的内容花了好几天的时间才完成，最后还是感觉不是很满意。这种感觉，也只有在高中时期写语文作文时才感受得到。想说的话还有很多，不仅仅只是软工的，关于大学、关于人生、关于IT界的看法等等，写了好几段长长的心路之旅，然而斟酌了许久，最后还是删掉了。 正如栋哥所言，相见容易再聚难。在软工实践的最后，附上自己的领骑衫照片。 谨记生而为龙的模样，谨记我的姓名是炎黄！","link":"/categories/software-engineering/领骑衫的朝圣之旅/"},{"title":"高斯混合模型","text":"高斯混合模型 软分类算法，即对每一个样本，计算其属于各个分布的概率，概率值最大的就是这个样本所属的分类。 对于训练样本的分布，看成为多个高斯分布加权得到的。其中每个高斯分布即为某一特定的类。 高斯混合模型和高斯判别分析非常像，唯一的区别就是在高斯混合模型中，每个样本所属的类别标签是未知的。 为了计算每个样本属于各个分布的概率Z，对每个高斯分布的参数进行初始化，然后以此计算概率Z，再根据Z来对所有参数进行优化，直到收敛。 EM算法Jensen不等式 若二阶导数的不等号方向逆转（f(x)为凹函数），则不等式的不等号方向逆转。 严格凸函数：非正式得定义，曲线中不能包含直线部分，","link":"/categories/machine-learning/高斯混合模型/"},{"title":"Supervised Hashing with Kernels, KSH","text":"Notation该论文中应用到较多符号，为避免混淆，在此进行解释： n：原始数据集的大小 l：实验中用于监督学习的数据集大小（矩阵S行/列的大小） m：辅助数据集，用于得到基于核的哈希函数 r：比特位数量/哈希函数的个数 Introduction先前的哈希检索方法，要么精度低，要么目标函数过于复杂导致导致训练慢。在大规模的图像数据检索中，这些方法就不太适用。先前的哈希方法都是对汉明距离进行直接优化，但是因为汉明距离是nonconvex和nonsmooth，难以优化。在本文中，作者利用汉明距离和编码内积等价的特点，得到了一个非常高效易于优化的目标函数。同时，作者利用的内积的可分性，设计了一个贪婪算法逐比特位得对哈希函数进行求解。此外，为了适应线性不可分的数据，作者还应用了基于核的公式。得到了Kernel-Based Supervised Hashing(KSH)。并在CIFAR-10和TINY数据集上与无监督、半监督、监督等多种哈希方法进行比较，证明了KSH具有最好的表现。 Kernel-Based Supervised HashingHash Functions with Kernels论文利用基于核的哈希函数，解决了原始数据线性不可分的问题。对应的哈希函数如下： 该哈希函数由Kernelized Locality-Sensitive Hashing（KLSH）推导而来。查阅了下这篇文献，整个核哈希函数的推导过程大致如下：两个数据点间的相似性可由向量内积$sim(x_i, x_j) = (x_i^{T}x_j) / (||x_i||_2||x_j||_2)$来表示，为了解决原始数据的线性不可分问题，KLSH引入了核函数，于是得到了： 之后，经由均值、方差、特征值分解等等一系列的代数变换，得到了上述基于核的哈希函数。 In order to maintain efficiency and to maintain sublinear time searches, we want p to be much smaller than n. For example, $p = O(\\sqrt n)$would guarantee that the algorithm maintains sublinear search times. 为了确保线性的搜索速度，KLSH中建议m（p）的的取值为$O(\\sqrt n)$。 Manipulating Code Inner Products在监督学习中，通过类别标签信息，原始数据集$X$可以划分为$M$和$C$两个集合，其中$M$中的数据点都是相似的，$C$中的数据点都是不相似的。通过对$X$进行部分采样，取$l$个样本构造相似矩阵$S$如下： 我们的目标是通过相似矩阵优化得到具有局部敏感特征的哈希函数，但是直接对汉明距离$D_h(x_i,x_j)=|{k|h_k(x_i)≠h_k(x_j),1≤k≤r}|$进行优化在数学上是难以求解的。因此，论文采用了code inner products来进行优化。code inner products可以如下表示： 由于r为码长是个常熟，因此从上式可以看到code inner products等价于汉明距离，存在一一映射的关系，我们可以通过优化code inner products的方法进而优化汉明距离。得到目标函数如下： 其中矩阵$H$表示向量$x$的哈希编码。接着再通过引入2.1节中的核哈希函数，得到最终目标函数如下： Greedy Optimization 通过简单的代数变换，可以得到目标函数如下： 其中$ak$为第k个哈希函数。在优化时，逐比特位得进行优化，即先优化$a_1、a_2 … a{k-1}$，然后再之前的基础上再优化$a_k$。在此定义一个残差矩阵为： 显然，有$R_0 = rS$。因此，哈希函数可以通过最小化如下损失函数进行求解： 通过去掉常数项，可以得到哈希函数$a_k$的目标函数为： Spectral Relaxation：因为此时的目标函数时非凸的，难以进行优化，因此论文应用了Spectral relaxation trick的方法，去掉了sgn函数，得到目标函数如下： 显然，通过变换，我们可以得到一个广义特征值问题$Kl^TR{k-1}K_la = \\lambda K_l^TK_la$。通过求解最大特征值对应的特征向量，并进行适当的尺度缩放以满足限制条件，我们可以得到$a$的解。但是，这个方法在l非常大时，优化结果不理想，因此KSH仅将这部作为$a_k$的初始化选择。 Sigmoid Smoothing：通过用sigmoid函数替换sgn函数，我们可以对目标函数进行优化。之后通过牛顿梯度下降方法，可以得到$a_k$的最优解（局部）。 算法流程如下： 在算法运行时，先初始化$R_0 = rS$，在这个基础上逐步对$a_1、a_2 … a_k$等进行优化。因为有着Spectral Relaxing中限制条件的约束，因此在一开始对$a_1$进行优化时，经由广义特征值求解、梯度下降之后得到的哈希函数$a_1$不会使得$R_0 = rS$，优化结果只与$a_1$本身相关，和其他哈希函数无关。 从本质上看，$rS$ 的信息是由r个哈希函数组合而成的，且哈希函数之间互相独立不相关。因此，在对$a_1$进行优化之后，我们可以从$rS$中去除掉哈希函数$a_1$相关的信息，继而对剩下的哈希函数进行优化。整个算法的思想不是直接对全局的目标函数进行优化，而是尽可能贪心地、逐比特位进行优化，这也是为什么算法称为Greedy Optimization的原因（个人理解）。 Experiments在对查询点进行查询时，将汉明距离小于一定距离的图像作为查询结果返回。 metric distance neighbors 当距离小于一定阈值时，即认为两个数据是相似的。 semantically similar neighbors 只有当两个属于的标签一致时，才认为两个数据是相似的。具有更强的置信度。 Hashing table的构造 个人理解：m个数据，经过c个哈希函数映射后，得到矩阵$H∈R^{m*c}$，矩阵H的第i行代表第i个数据对应的哈希编码，矩阵的第j列代表哈希表的第j个bucket。 Evaluation precision 对于一个查询，返回了一系列的文档，正确率指的是返回的结果中相关的文档占的比例，定义为：precision=返回结果中相关文档的数目/返回结果的数目。 recall 召回率则是返回结果中相关文档占所有相关文档的比例，定义为：Recall=返回结果中相关文档的数目/所有相关文档的数目。 mAP 正确率只是考虑了返回结果中相关文档的个数，没有考虑文档之间的序。对一个搜索引擎或推荐系统而言返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了AP的概念。对一个有序的列表，计算AP的时候要先求出每个位置上的precision，然后对所有的位置的precision再做个average。如果该位置的文档是不相关的则该位置 precision=0。 AP = \\frac { 1 } { R } \\times \\sum _ { r = 1 } ^ { R } \\frac { r } { p o s i t i o n ( r ) }其中R表示相关文档的总个数，position(r)表示结果列表从前往后看，第r个相关文档在列表中的位置。 MAP = \\frac { \\sum _ { q - 1 } ^ { Q } AP ( q ) } { Q }其中Q表示查询数据的个数。 the success rate of hash lookup ConclusionsKSH算法具有以下三个优点： 利用核函数解决了原始数据不可分的问题； 设计了一个基于coder inner products的目标函数，易于优化； 利用贪心算法逐比特位得求解方法。","link":"/categories/computer-vision/Supervised-Hashing-with-Kernels-KSH/"},{"title":"隐马尔可夫模型","text":"隐马尔可夫模型（Hidden Markov Model，HMM）是可用于标注问题的统计学习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型。隐马尔可夫模型在语音识别、自然语言处理、生物信息、模式识别等领域有着广泛的应用。 原理定义：隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链生成不可观测的状态随机序列，再由各个状态生成的一个观测产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列成为状态序列（stae sequence）；每个状态生成一个观测，而由此产生的观测的随机序列成为观测序列（observation sequence）。序列的每一个位置又可以看作是一个时刻。 隐马尔可夫模型由初始概率分布$\\Pi$、状态转移概率分布A以及观测概率分布B确定。隐马尔可夫的形式定义如下： \\begin{array}{l}{A=\\left[a_{i j}\\right]_{N \\times N}, \\quad a_{i j}=P\\left(Z_{t+1}=q_{j} | Z_{t}=q_{i}\\right)} \\\\ {B=\\left[b_{j}(k)\\right]_{N \\times M}, \\quad b_{j}(k)=P\\left(X_{t}=v_{k} | Z_{t}=q_{j}\\right)} \\\\ {\\Pi=\\left(\\pi_{i}\\right)_{\\mathrm{kN}}, \\quad \\pi_{i}=P\\left(Z_{1}=q_{i}\\right)}\\end{array}因此，隐马尔可夫模型$\\lambda$可以用三元符号表示，即： \\lambda=(A,B,\\Pi)状态概率矩阵A与初始状态概率矩阵$\\Pi$确定了隐马可夫链，生成不可观测的状态序列，观测概率矩阵B确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。 隐马尔可夫模型有三个基本问题： （1）概率计算问题：给定模型$\\lambda=(A,B,\\Pi)$和观测序列$O=(o_1,o_2,\\ldots,o_T)$，计算在模型$\\lambda$下观测序列O出现的概率$P(O|\\lambda)$。 （2）学习问题：已知观测序列$O=(o_1,o_2,\\ldots,o_T)$，估计模型$\\lambda=(A,B,\\Pi)$参数，使得在该模型下观测序列概率$P(O|\\lambda)$最大。即用极大似然估计的方法估计参数。 （3）预测问题（解码问题）：已知模型$\\lambda=(A,B,\\Pi)$和观测序列$O=(o_1,o_2,\\ldots,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,\\ldots,i_T)$。即给定观测序列，求最优可能的对应的状态序列。 第一个问题可用前向-后向算法（Forward-Backword Algorithm）解决，第二个问题是模型训练问题，第三个问题可用维特比算法（Viterbi Algorithm）解决。接下来，我们将对这些算法逐一进行介绍。 概率计算问题已知：模型$\\lambda=(A,B,\\Pi)$和观测序列$O=(o_1,o_2,\\ldots,o_T)$ 求：$P(O|\\lambda)$ 直接计算法通过列举所有可能的长度为T的状态序列$I=(i_1,i_2,\\ldots,i_T)$，求各个状态序列$I$与观测序列$O=(o_1,o_2,\\ldots,o_T)$的联合概率$P(O,I|\\lambda)$，然后对所有可能的状态序列求和，得到$P(O|\\lambda)$。 \\begin{array}{l}{P(O)=P\\left(O_{1}, O_{2}, \\ldots, O_{T}\\right)} \\\\ {=\\sum_{I} P(O, I)=\\sum_{I} P(O | I) P(I)=\\sum_{I} P\\left(O_{1}, O_{2}, \\ldots, O_{T} | I_{1}, I_{2}, \\ldots, I_{T}\\right) P\\left(I_{1}, I_{2}, \\ldots, I\\right)} \\\\ {=\\sum_{I} P\\left(O_{1} | I_{1}\\right) \\times P\\left(O_{2} | I_{2}\\right) \\times \\ldots \\times P\\left(O_{T} | I_{T}\\right) \\times P\\left(I_{1}\\right) \\times P\\left(I_{2} | I_{1}\\right) \\times \\ldots \\times P\\left(I_{T} | I_{T-1}\\right)} \\\\ =\\sum_{I_{1}, \\mathbf{L}, I_{T}}\\left(\\prod_{t=1}^{T} P\\left(O_{t} | I_{t}\\right)\\right) \\times\\left(\\prod_{t=1}^{T-1} P\\left(I_{t-1} | I_{t}\\right)\\right) \\times P\\left(I_{1}\\right) \\\\ =\\sum_{I_{1}} \\sum_{I_{2}} \\sum_{I_{3}} \\ldots \\ldots \\sum_{I_{T}}\\left(\\prod_{t=1}^{T} P\\left(O_{t} | I_{t}\\right)\\right) \\times\\left(\\prod_{t=1}^{T-1} P\\left(I_{t-1} | I_{t}\\right)\\right) \\times P\\left(I_{1}\\right)\\end{array}但是，这个公式的计算量很大，是$O(TN^T)$阶的，显然不可行。 下面将介绍前向-后向算法，通过递推地计算前向-后向概率，可以高效地进行隐马尔可夫模型的概率计算。 前向算法前向概率：给定隐马尔可夫模型$\\lambda$，定义到时刻t部分观测序列为$o_1,o_2,\\ldots,o_t$，且状态为$q_i$的概率为前向概率，记作： \\alpha_i=P(o_1,o_2,\\dots,o_t,i_t=q_i|\\lambda)可以递推得求得前向概率$\\alpha_t(i)$及观测序列概率$P(O|\\lambda)$。 一个观测： 两个观测： 三个观测： 通过上面的推导，可以发现一些规律： \\begin{array}{l}{\\alpha_{1}(j)=P\\left(O_{1}=o_{1} | I_{1}=j\\right) P\\left(I_{1}=j\\right)} \\\\ {\\alpha_{2}(j)=P\\left(O_{2}=o_{2} | I_{2}=j\\right) \\sum_{i=1}^{N} P\\left(I_{2}=j | I_{1}=i\\right) \\alpha_{1}(i)} \\\\ {\\alpha_{3}(j)=P\\left(O_{3}=o_{3} | I_{3}=j\\right) \\sum_{i=1}^{N} P\\left(I_{3}=j | I_{2}=i\\right) \\alpha_{2}(i)} \\\\ \\alpha_{t+1}(j) =P\\left(O_{t+1}=o_{t+1} | I_{t+1}=j\\right) \\sum_{i=1}^{N} P\\left(I_{t+1}=j | I_{t}=i\\right) \\alpha_{t}(i) \\\\ \\alpha_{t+1}(j) =\\left(\\sum_{i=1}^{N} \\alpha_{t}(i) a_{i j}\\right) b_{j}\\left(o_{t+1}\\right) \\end{array} 算法：观测序列概率的前向算法 输入：模型$\\lambda$，观测序列$O$ 输出：$P(O|\\lambda)$ （1）初始值 \\alpha_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), i=1,2, \\dots, N（2）递推，$t=1,2,\\ldots,T-1$ \\alpha_{t+1}(j)=\\left[\\sum_{i} \\alpha_{t}(i) a_{i j}\\right] \\times b_{j}\\left(o_{t+1}\\right), j=1,2, \\ldots, N（3）终止 P(O | \\lambda)=\\sum_{i=1}^{N} \\alpha_{T}(i) 后向算法后向概率：给定隐马尔可夫模型$\\lambda$，定义在时刻t状态为$qi$的条件下，从t+1到T的部分观测序列为$o{t+1},o{t+2},\\ldots,o{T}$的概率为后向概率，记作： \\beta_t(i)=P(o_{t+1},o_{t+2},\\ldots,o_{T}|i_t=q_i,\\lambda)可以用递推的方法求得后向概率$\\beta_t(i)$及观测序列概率$P(O|\\lambda)$。 将概率公式$P(O)=P(O_1,O_2,\\ldots,O_T)$展开，得到: \\begin{array}{l}{P(O)=P\\left(O_{1}, O_{2}, \\ldots, O_{T}\\right)} \\\\ {=\\sum_{I} P\\left(O_{1}, O_{2}, \\ldots, O_{T}, I_{1}, I_{2}, \\ldots, I_{T}\\right)} \\\\ {=\\sum_{I_{1}-I_{T}} P\\left(I_{1}\\right) P\\left(O_{1} | I_{1}\\right) P\\left(I_{2} | I_{1}\\right) P\\left(O_{2} | I_{2}\\right) \\ldots P\\left(I_{T} | I_{T-1}\\right) P\\left(O_{T} | I_{T}\\right) \\cdot 1} \\\\ {=\\sum_{I_{1}} P\\left(I_{1}\\right) P\\left(O_{1} | I_{1}\\right) \\sum_{I_{2}} P\\left(I_{2} | I_{1}\\right) P\\left(O_{2} | I_{2}\\right) \\mathrm{L} \\sum_{I_{T}} P\\left(I_{T} | I_{T-1}\\right) P\\left(O_{T} | I_{T}\\right) \\cdot 1} \\\\ {=\\sum_{I_{1}} P\\left(I_{1}\\right) P\\left(O_{1} | I_{1}\\right) \\beta_{1}\\left(I_{1}\\right)=\\sum_{i=1}^{N} \\pi_{i} b_{i}\\left(o_{1}\\right) \\beta_{1}(i)}\\end{array}通过上面的推导，可以发现一些规律： \\begin{array}{l}{\\beta_{T-1}(i)=\\sum_{I_{T}} P\\left(I_{T} | I_{T-1}=i\\right) P\\left(O_{T} | I_{T}\\right) \\cdot \\beta_{T}\\left(I_{T}\\right)} \\\\ \\ldots\\ldots \\\\{\\beta_{1}(i)=\\sum_{I_{2}} P\\left(I_{2} | I_{1}=i\\right) P\\left(O_{2} | I_{2}\\right) \\beta_{2}\\left(I_{2}\\right)} \\\\ \\beta_{t}(i) =\\sum_{j=1}^{N} P\\left(I_{t+1}=j | I_{t}=i\\right) P\\left(O_{t+1}=o_{t+1} | I_{t+1}=j\\right) \\beta_{t+1}\\left(I_{t+1}=j\\right) \\\\ \\quad \\quad =\\sum_{j=1}^{N} a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j) \\end{array}其中，$i=1,2,\\ldots,N,t=T-1,T-2,\\ldots,1$。 算法：观测序列概率的后向算法 输入：模型$\\lambda$，观测序列$O$ 输出：$P(O|\\lambda)$ （1）初始值 \\beta_{T}(i)=1, \\quad i=1,2, \\dots, N（2）递推，$t=T-1,T-2,\\dots,1$ \\beta_{t}(i)=\\sum_{j=1}^{N} \\alpha_{ij} b_{j}(o_{t+1})\\beta_{j+1}(j), \\quad i=1,2, \\ldots, N（3）终止 P(O | \\lambda)=\\sum_{i=1}^{N} \\pi_ib_{i}(o_1)\\beta_1(i) 预测问题（解码问题）已知：模型$\\lambda=(A,B,\\Pi)$和观测序列$O=(o_1,o_2,\\ldots,o_T)$ 求：$I^*=argmax_IP(I|O,\\lambda)$ 近似算法给定隐马尔可夫模型$\\lambda$和观测序列O，在时刻t处于状态$q_i$的概率$\\gamma_t(i)$为： \\gamma_{t}(i)=P\\left(i_{t}=i | O, \\lambda\\right)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\times \\beta_{t}(j)}在每一时刻t，最优可能的状态$i_t^*$为： i_{t}^{*}=\\underset{1 \\leq i \\leq N}{\\arg \\max }\\left[\\gamma_{t}(i)\\right], t=1,2, \\ldots, T近似算法的优点是计算简单，但是存在以下两个问题： 不能保证预测的状态序列是整体最优的； 状态序列中可能存在转移概率为0的相邻状态。 尽管如此，近似算法仍然是有用的。下面将介绍更为精确的维特比算法。 维特比算法维特比算法实际是用动态桂花解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）。这时一条路径对应着一个状态序列。 I^{*}=\\arg \\max _{I} P(I | O, \\lambda)=\\arg \\max _{I} \\frac{P(I | \\lambda) P(O | I, \\lambda)}{P(O | \\lambda)}=\\arg \\max _{I} P(I | \\lambda) P(O | I, \\lambda) \\begin{array}{l}{P(O)=P\\left(O_{1}, O_{2}, \\ldots, O_{T}\\right)} \\\\ {=\\sum_{I} P(O, I)=\\sum_{I} P(O | I) P(I)=\\sum_{I} P\\left(O_{1}, O_{2}, \\ldots, O_{T} | I_{1}, I_{2}, \\ldots, I_{T}\\right) P\\left(I_{1}, I_{2}, \\ldots, I_{T}\\right)} \\\\ {=\\sum_{I} P\\left(O_{1} | I_{1}\\right) \\times P\\left(O_{2} | I_{2}\\right) \\times \\ldots \\times P\\left(O_{T} | I_{T}\\right) \\times P\\left(I_{1}\\right) \\times P\\left(I_{2} | I_{1}\\right) \\times \\ldots \\times P\\left(I_{T} | I_{T-1}\\right)}\\end{array}一个观测： 两个观测： 三个观测： 算法：维特比算法 输入：模型$\\lambda$，观测序列$O$ 输出：最优路径$P(I|O,\\lambda)$ （1）初始值 \\begin{array}{ll}{\\delta_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right),} & {i=1,2, \\ldots, N} \\\\ {\\psi(i)=0,} & {i=1,2, \\ldots, N}\\end{array}（2）递推，对$t=2,\\ldots,T$ \\begin{array}{l}{\\delta_{t}(j)=\\max _{1 \\leq i \\leq N}\\left[\\sum_{i} \\delta_{t-1}(i) a_{i j}\\right] b_{j}\\left(o_{t+1}\\right), j=1,2, \\ldots, N} \\\\ {\\psi(j)=\\arg \\max _{1 \\leq i \\leq N}\\left[\\sum_{i} \\delta_{t-1}(i) a_{i j}\\right], \\quad j=1,2, \\ldots, N}\\end{array}（3）终止 \\begin{aligned} P^{*} &=\\max _{1 \\leq i \\leq N} \\delta_{T}(i) \\\\ i_{T}^{*} &=\\arg \\max _{1 \\leq i \\leq N} \\delta_{T}(i) \\end{aligned}（4）最优路径回溯，对$t=T-1,T-2,\\ldots,1$ i_{t}^{*}=\\psi_{t+1}\\left(i_{t+1}^{*}\\right)求得最优路径$I^=(i_1^,i_2^,\\ldots,i_r^)$。","link":"/categories/machine-learning/hidden-markov-model/"},{"title":"朴素贝叶斯","text":"朴素贝叶斯（Naive Beyes）是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。 原理假设有一些数据D，其类别取值为0和1： D=\\left\\{\\left\\langle x_{i}, y_{i}\\right\\rangle\\right\\}_{i=1}^{n}, x_{i} \\in R^{d}, y_{i} \\in\\{0,1\\} \\quad\\langle X, Y\\rangle \\sim P对于一个新样本x，我们希望模型能够预测出其对应的类别。其形式化表达为： y^{*}=\\arg \\max _{y} p(y | x)逻辑斯蒂回归等判别式模型直接对条件概率$p(y|x)$进行建模，找到一条决策边界将数据集中不同类别的数据点区分开。而朴素贝叶斯等产生式模型则是对联合概率$p(x, y)$进行建模，通过数据学习到类别$y=0$下的数据分布$p(x|y=0)$和类别$y=1$下的数据分布$p(x|y=1)$。对于新样本x，朴素贝叶斯计算出x在每个类别y下条件概率$p(x|y)$，并取概率值最大的类别作为最终的预测输出。 根据贝叶斯公式： \\begin{array}{l}{y^{*}=\\arg \\max _{y} p(y | x)} \\\\ {=\\arg \\max _{y} \\frac{p(x, y)}{p(x)}=\\arg \\max _{y} p(y) p(x | y)}\\end{array}其中$p(y)$为类先验概率，$p(x|y)$为类条件概率。 假设朴素贝叶斯需要求解的参数为： y^{*}=\\arg \\max _{y} p(y) p(x | y)假设x有d维，每个维度有$m_i$种取值可能。那么在K分类问题下，所需的参数个数为： 1+K \\times\\left(\\prod_{j=1}^{d} 2-1\\right)=K^{d+1}+K-1当维数d非常大时，显然所需求解的参数个数会呈指数爆炸，使得模型的求解变得异常困难。因此，为了解决参数个数指数爆炸的问题，朴素贝叶斯对条件概率分布作了条件独立性的假设，即当给定类别y时数据x中的各个维度之间是相互独立的。 P\\left(X_{1}, \\ldots, X_{d} | Y\\right)=\\prod_{j=1}^{d} P\\left(X_{j} | Y\\right)这是一个很强的假设，与实际情况不太符合，因为现实中各个维度之间往往存在关联，这也是朴素贝叶斯的命名由来。但是通过这个朴素假设，朴素贝叶斯将参数个数减少到一个非常小的量级，有效提高了计算效率，并在实际应用中也取得了非常好的表现。 基于这个假设，朴素贝叶斯的模型可以表达为： \\begin{array}{l}{P\\left(Y=y_{k} | X_{1}, \\ldots, X_{d}\\right)} \\\\ {=\\frac{p\\left(Y=y_{k}\\right) P\\left(X_{1}, \\ldots, X_{d} | Y=y_{k}\\right)}{\\sum_{j} p\\left(Y=y_{j}\\right) P\\left(X_{1}, \\ldots, X_{d} | Y=y_{j}\\right)}} \\\\ {=\\frac{p\\left(Y=y_{k}\\right) \\prod_{i} P\\left(X_{i} | Y=y_{k}\\right)}{\\sum_{j} p\\left(Y=y_{j}\\right) \\prod_{i} P\\left(X_{i} | Y=y_{j}\\right)}}\\end{array} Y \\leftarrow \\underset{y_{k}}{\\arg \\max } p\\left(Y=y_{k}\\right) \\prod_{i} P\\left(X_{i} | Y=y_{k}\\right)对于朴素贝叶斯的参数求解，在第六讲 Logistic回归+产生式模型中作了非常全面的讲解，这里就不再展开，想要深入了解的读者可以自行移步。 例一 《统计学习方法》page50，例4.1 试由下表的训练数据学习一个朴素贝叶斯分类器，并确定$x=(2,S)^T$的类标记y。表中$X^{(1)},X^{(2)}$为特征，取值的集合分别为$A_1={1,2,3}$，$A_2={S,M,L}$，y为类标记，$Y\\in C={1,-1}$。 解： 通过表中数据计算类先验概率和类条件概率： P(Y=1)=\\frac{9}{15}, \\quad P(Y=-1)=\\frac{6}{15} P(X_1=2|Y=1)=\\frac{3}{9}, \\quad P(X_2=S|Y=1)=\\frac{1}{9} P(X_1=2|Y=-1)=\\frac{2}{6}, \\quad P(X_2=S|Y=-1)=\\frac{3}{6}对于$x=(2,S)^T$，通过朴素贝叶斯公式，有： P(Y=1) P\\left(X_{1}=2 | Y=1\\right) P\\left(X_{2}=S | Y=1\\right)=\\frac{9}{15} \\times \\frac{3}{9} \\times \\frac{1}{9}=\\frac{1}{45} P(Y=-1) P\\left(X_{1}=2 | Y=-1\\right) P\\left(X_{2}=S | Y=-1\\right)=\\frac{6}{15} \\times \\frac{2}{6} \\times \\frac{3}{6}=\\frac{1}{15}因为$\\frac{1}{45}&lt;\\frac{1}{15} \\Rightarrow y=-1$，所以$x=(2,S)^T$的类标记为-1。 例二试用下表的训练数据学习一个垃圾邮件分类器，并对新邮件“review us now”进行分类。 解： 左边的表格给出了所有样本的类别标签，可以看到垃圾邮件的概率$P(spam)=4/6$，正常邮件的概率为$P(ham=2/6)$。 右边的表格则分别给出了在垃圾邮件和正常邮件下各个单词出现的概率。 通过上述信息，我们可以利用朴素贝叶斯对新邮件“review us now”进行判定。其中单词“now”在词典里并没有出现，在这里我们简单得忽略掉这个单词。 在垃圾邮件下，新邮件“review us”出现的概率为： P(\\text { review us } | spam)=\\left(1-\\frac{2}{4}\\right)\\left(\\frac{1}{4}\\right)\\left(1-\\frac{3}{4}\\right)\\left(\\frac{3}{4}\\right)\\left(1-\\frac{3}{4}\\right)\\left(1-\\frac{1}{4}\\right)=0.0044在正常邮件下，新邮件“review us”出现的概率为： P(\\text { review us } | h a m)=\\left(1-\\frac{1}{2}\\right)\\left(\\frac{2}{2}\\right)\\left(1-\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)\\left(1-\\frac{1}{2}\\right)\\left(1-\\frac{0}{2}\\right)=0.0625通过贝叶斯定义，可以得到新邮件”review us“属于正常邮件的概率为： P(h a m | \\text {review us})=\\frac{0.0625 \\times 2 / 6}{0.0625 \\times 2 / 6+0.0044 \\times 4 / 6}=0.87从计算结果可以看到，”review us now“属于正常邮件的概率更高点，但是从表格中显然可以看出邮件D4：”review us“是属于垃圾邮件。这个不一致的问题在于单词”now“在词典中的缺失以及样本数量过少的原因。比如样本中正常邮件中并没有出现单词”account“，这使得任何包含”account“的新邮件都会被分类为垃圾邮件。 上述情况便涉及到一个零概率问题（Zero-Frequency Problem），即在计算新实例的概率时，如果某个分量在训练集中没有出现过，会导致整个示例的概率为0，这显然是合理的。为了解决这个问题，引入了拉普拉斯平滑（Laplace Smoothing）： P(w | c)=\\frac{\\operatorname{num}(w, c)+\\varepsilon}{\\operatorname{num}(c)+2 \\varepsilon}朴素贝叶斯针对属性缺失问题，即测试样本中某些属性的值丢失，可以直接忽略这些属性，对计算结果并没有影响。 策略朴素贝叶斯将示例分到后验概率最大的类中，这等价于期望风险最小化。 假设选择0-1损失函数： L(Y, f(X))=\\left\\{\\begin{array}{ll}{1} & {, \\quad Y \\neq f(X)} \\\\ {0} & {, \\quad Y=f(X)}\\end{array}\\right.式中$f(X)$是分类决策函数。这时，期望风险函数为： R_{\\mathrm{cxp}}(f)=E[L(Y, f(X))]期望是对联合分布$P(X,Y)$取的。由此取条件期望： \\begin{array}{l}{R_{\\mathrm{exp}}(f)=E[L(Y, f(X))]} \\\\ {=\\iint L(Y=y, f(X=x)) P(X=x, Y=y) d x d y} \\\\ {=\\iint L(Y=y, f(X=x)) P(Y=y | X=x) p(X=x) d x d y}\\\\ {=\\int\\left\\{\\int L(Y=y, f(X=x)) P(Y=y | X=x)\\right] d y \\} p(X=x) d x} \\\\ {=\\int\\left\\{\\sum_{k=1}^{K} L\\left(c_{k}, f(X=x)\\right) P\\left(c_{k} | X=x\\right)\\right\\} p(X=x) d x} \\\\ {=E_{X}\\left[\\sum_{k=1}^{K} L\\left(c_{k}, f(X)\\right) P\\left(c_{k} | X\\right)\\right]}\\end{array}为了使期望风险最小化，只需对$X=x$逐个极小化： \\begin{array}{l}{f(x)=\\underset{y \\in Y}{\\arg \\min } \\sum_{k=1}^{K} L\\left(c_{k}, y\\right) P\\left(c_{k} | X=x\\right)} \\\\ \\quad \\quad {=\\arg \\min _{y \\in Y} \\sum_{k=1}^{K} P\\left(y \\neq c_{k} | X=x\\right)} \\\\ \\quad \\quad {=\\arg \\min _{y \\in \\mathrm{Y}}\\left(1-P\\left(y=c_{k} | X=x\\right)\\right)} \\\\ \\quad \\quad {=\\underset{y \\in \\mathrm{Y}}{\\arg \\max } P\\left(y=c_{k} | X=x\\right)}\\end{array}这样一来，根据期望风险最小化准则就得到了后验概率最大化准则： f(x)=\\underset{c_{k}}{\\arg \\max } P\\left(c_{k} | X=x\\right)即朴素贝叶斯法所采用的原理。 高斯判别模型在朴素贝叶斯中，特征向量$x$是离散的，服从多项分布。而在高斯判别分析（Gaussian Discriminant Analysis，GDA）中，我们假设$x$是连续的，服从高斯分布。高斯判别模型基于两点假设：y服从伯努利分布；x服从多元高斯分布。其分布概率密度为： \\begin{array}{l}{p(y=1)=\\phi, \\quad p(y=0)=1-\\phi} \\\\ {p(x | y=1) \\sim N\\left(\\mu_{1}, \\Sigma\\right), p(x | y=1)=(2 \\pi)^{-\\frac{d}{2}}|\\Sigma|^{\\frac{1}{2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu_{1}\\right)^{T} \\Sigma\\left(x-\\mu_{1}\\right)\\right\\}} \\\\ {p(x | y=0) \\sim N\\left(\\mu_{0}, \\Sigma\\right), p(x | y=0)=(2 \\pi)^{-\\frac{d}{2}}|\\Sigma|^{\\frac{1}{2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu_{0}\\right)^{T} \\Sigma\\left(x-\\mu_{0}\\right)\\right\\}}\\end{array}我们通过比值关系比较$p(x|y=1)$和$p(x|y=0)$的大小，并对其取对数： \\begin{array}{l}{\\ln \\left[\\frac{p(y=1) p(x | y=1)}{p(y=0) p(x | y=0)}\\right]} \\\\ {=\\ln \\frac{\\phi \\times(2 \\pi)^{-\\frac{d}{2}}|\\Sigma|^{-\\frac{1}{2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu_{1}\\right)^{T} \\Sigma\\left(x-\\mu_{1}\\right)\\right\\}}{(1-\\phi) \\times(2 \\pi)^{-\\frac{d}{2}}|\\Sigma|^{-\\frac{1}{2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu_{0}\\right)^{T} \\Sigma\\left(x-\\mu_{0}\\right)\\right\\}}} \\\\ {=\\left(\\mu_{1} \\Sigma-\\mu_{0} \\Sigma\\right)^{T} x+\\frac{1}{2} \\mu_{0}^{T} \\Sigma \\mu_{0}-\\frac{1}{2} \\mu_{1}^{T} \\Sigma \\mu_{1}+\\ln \\frac{\\phi}{1-\\phi}} \\\\ {=w^{T} x+b}\\end{array}其中$w=\\mu{1} \\Sigma-\\mu{0} \\Sigma$，$b=\\frac{1}{2} \\mu{0}^{T} \\Sigma \\mu{0}-\\frac{1}{2} \\mu{1}^{T} \\Sigma \\mu{1}+\\ln \\frac{\\phi}{1-\\phi}$。 从结果可以看到，高斯判别分析是一个线性模型，属于判别式模型。","link":"/categories/machine-learning/naive-bayes/"},{"title":"科研小萌新的探索之路","text":"早在今年五月份左右的时候，师兄就跟我说要准备下AAAI。那时距离deadline还有3个多月，时间完全充足，我也是跃跃欲试。因为自己所处的实验室科研氛围非常浓厚，本来也是打算研一投稿一篇，无论中与否，也是一种锻炼。于是，跌跌撞撞也开始认真专注于自己的科研探索之路。 我的研究方向是哈希检索，虽然这个方向已经趋近饱和，但摸索大半年，自己也是想到了很多感觉可以做的idea。阅读文献——产生idea——实验验证，这个过程确实非常令人兴奋，印象在六月份的时候，自己无论是在走路、吃饭还是在睡觉前，脑海里都在不停地思考，想着这个idea可以如何改进、这个问题可以怎么解决？手机里的待办清单里还保留着自己很多半夜两三点突然想到的点子。那时有很多瞬间，自己都膨胀到感觉都可以发好多篇AAAI和CVPR了。但是，理想很丰满，现实很骨感，很多idea在不停得实验验证后，都宣告失败，但是依然有几个点子我觉得是可以去做的，只是一直没能找到比较好的解决方案。随着时间不停地推进，AAAI2020的dealine也即将临近，加上师兄的催促以及建议，自己也是暂时放弃了对先前一些想法的探究，然后在一天之内强行“逼”出了一个idea。虽然这个idea的实验效果很不错，但是创新点很小，而且也不是自己先前研究的重点，所以总有一种不是很待见的感觉。在七月下旬的时候，自己最终也是逐步确定了论文框架，并开始着手自己的第一篇会议论文。不过由于之前没有任何投稿经验，英语写作也是多年未碰，整个AAAI的准备过程可谓是磕磕碰碰，踩了许多的坑。 首先是论文的初稿写作。自己采取的方法是先构思整个论文的架构，包括针对的问题、贡献、比较的方法、要做的实验等等，在整体框架大致确定后，再进行写作。不过俗话说，万事开头难，多年没用过英语写作，在着手Introduction的时候，很多想法用中文大致表达得出来，但是用英语一句话也写不出来。最后憋得难受，只好不停得参阅现有论文的写法，然后借助万能的翻译软件，写完一段中文翻译完再修修补补放进论文，不过这样出来的结果真的是惨不忍睹，更有小伙伴表示有种高中英语写作的既视感，很直白。不过作为论文初稿，自己想也是蛮凑合着吧，大致思想表达正确后之后再进行大改。 然后中期阶段遇到最大的一个问题就是baseline的实验。由于深度哈希的backbone没有一个统一的标准，加上各种千奇百怪的实验设置，相同的方法在不同的论文下的数据真是天差地别，另外由于需要precision-recall curve的实验，只好自己重新跑一遍所有baseline的实验，这过程真的是痛苦万分。哈希领域发展了这么多年，但是所有方法的代码真的是杂乱无章，非深度的深度的，数据集的不同划分方式和不同的特征数据，基于matlab、caffe、pytorch、tensorflow各种框架的代码都有，有的官方提供的代码跑出来和论文上的数据差很多有的反而还高很多点，有的方法也没有在ImageNet上的实验还需要自己去调参，调完结果不理想还一度怀疑自己写的代码是否有问题。前前后后足足跑了两百多个实验，才勉强得到了论文的结果，但依旧感觉不是理想（其实是有些数据比原始论文低，感觉会被reviewer怼）。这个过程也说明了一个问题，就是我在哈希这个领域的基础打得不是很扎实，很多经典工作的实验都没有进行实验，而相比最近认识的一些同是研究哈希的小伙伴们，他们就会自己把一些经典的工作复现一遍，即锻炼了自己的编码能力，也对论文的方法有了更深的理解，这也是自己将来在新的方向上需要注意的一个点。另外也深感一个权威性的baseline对一个领域的重要性，这可以避免疯狂的灌水以及人为刷点的工作的出现。如果研二有时间的话，准备把整个深度哈希的论文整个到一个框架里，希望能成为在哈希领域另一个强有力的baseline。 最后，在论文的初稿和实验部分完成后，就是师兄和老师们的各种修改意见了。这个阶段也是很艰难，一个是论文的写作确实太粗糙，错误百出；另外一个就是方法的包装上和老师们的意见有着许多冲突。在文章的初稿阶段，idea的包装方法都是自己琢磨出来的，不过确实很不行，在师兄的意见下，对文章的框架以及针对的问题进行了一些修改。 而后到老师修改时，整个文章的框架又完成改变了，和自己之前的想法完全不同，有不少地方感觉也是有点怪怪的。不过自己水平比较低，还体会不到老师的境界，于是就按照老师的框架进行完善。然而最后到另外一个老师修改的时候又被完全否决了。那时感觉自己好几天的时间都是在做无用功，相比于实验室其他同学的论文已经都几乎定稿，自己的Introduction却还在一遍又一遍的重构着，心态真的是差点崩了。不过无论做什么事情都得尽善尽美，心态调整了回来后，仔细想了下老师们的意见，感觉也是有很多值得借鉴的，于是综合了老师们和师兄的意见，然后加了自己的一些想法，才确定了论文的最终框架。这个过程真正让我感受到一篇论文的来之不易，一字一句都要细细斟酌，虽然想法有点出入，但老师们和师兄（特别是鸭哥老师！）还是耐心得在指导着我，真的是非常感谢。 总结整个论文的准备过程，最大的心得体会就是： Introduction一定要反复打磨。先用中文给出整体框架，然后和师兄老师们进行充分得讨论之后确定了最终框架，再着手论文的写作。一个论文，最重要的部分就是Introduction，后面的无论是方法部分还是实验部分，都是Introduction的具体展开。这次投稿犯的最大错误就是Introduction没有充分讨论就进行写作，导致整个论文各个部分都不停得重写，耗费了许多精力。 基础工作一定要做好。论文的实验部分最重要的就是和现有方法的对比，之前自己在验证各种idea的时候，都是直接对比实验结果和先前论文的数据，但是在实际实验时，会发现论文的数据永远是理想的，即使利用官方代码也不一定得到和论文一致的结果，导致实验部分的数据对比和预期的不一致。比较正确的做法是，一个领域在研究时，对于经典文献，除了了解方法的核心思想，也要尽可能得复现其实验（或运行官方代码），确保论文的idea是确实有效的，而不是各种trick的叠加组合，这也能进一步启发我们未来的研究方向以及不同方法之间的公平对比。 有挑战性的idea再进行投稿。目前中国的学术圈，论文灌水现象非常严重，很多人都存在着一种急功近利的想法，没有关注于idea的深度，而只注重于idea的包装，导致大多工作都是没有实际意义的。当然，这也是目前我存在的问题。正如鸭哥老师所说，一个好的工作应当是在充分分析现有方法存在的问题的基础，结合实验结果得到自己的创新方法，而不是一拍脑袋的出来的结果，这样的工作更具有深度，更具有启发性。先前在研究哈希时，大部分的时间都是在关注于loss的设计，并非这次AAAI的idea，从而使得整个论文的撰写过程，我都有一种整个idea很一般被accept的可能性很小的感觉，一直想着赶紧结束就能继续研究先前的idea，这是一个非常不好的现象。因此，我觉得良好的科研应当是在有了一个确实可行的idea之后再进行投稿，而并非让dealine逼着你想出一个缺乏深度的idea，写出一篇对整个领域毫无意义的paper。 不要玩游戏！这个问题确实很尴尬，在论文的准备过程中偶尔会和大家一起玩一款手游，这导致自己的注意力无法长期集中、效率低下、心浮气躁，这个问题在自己本科期间准备毕设的时候也出现了，而且具体的表现也基本一致。总而言之，以后一定不要再玩游戏了！ 总而言之，这次AAAI的投稿被accept的可能性基本很小，虽然花费了很多心思有点可惜，但是自己确实也学习到了很多东西，包括论文的写作、idea的挖掘以及实验的设置等等，到下次再投稿的时候，自己应该能有一个质的变化。再次诚挚感谢各位老师和师兄的耐心指导！ （说个题外话，感觉科研圈都是太小了，大家都是在做着类似的工作，没有太多的创新点。这几天偶然发现上个月在arXiv上刚公布的有一篇论文的idea和自己几乎一模一样，无论是代码的书写逻辑还是实验的设置，都和自己现在的版本几乎一样，当然人家的包装和书写比我高了太多档次（虽然这篇论文先发的，但我真的没有抄袭！/狗头）） 贴几张这次准备过程的照片，权当纪念 (^__^)","link":"/categories/essay/first-paper-summary/"},{"title":"《Structuring Machine Learning Projects》笔记","text":"正交化定义：尽量使得因素之间正交，不互相影响，从而在对模型的某个方面进行优化时，不会影响到模型在其他方面的表现能力。 当你发现一个模型在dev数据集表现不好的时候，你可能会采取某些方法改进模型，但是采取的方法可能会同时影响到模型在其他数据集的表现。例如在dev数据集的表现变好了，但是却造成在test集的表现变差了，改变一个因素可能会同时影响到多个方面。因此，Orthogonalization便是想要使得每一个方法尽可能只影响到一个因素。 模型改进 Collect more data Collect more diverse training set Train algorithm longer with gradient descent Try Adam instead of gradient descent Try bigger network Try dropout Add $L_2$ regularization Network architecture Activation functions hidden units 评价指标模型评价的标准有很多，例如accuracy、precision、recall等，但是这些标准往往是互相影响的，例如在precision的表现很好，但是在recall上的表现却一般般，因此很能通过这两个指标去评判哪一个模型才是更好的。因此在评价模型的时候，尽量采用单一的统一标准，例如F1 Score。在模型在多个维度的误差时，也尽量使用平均误差。当然，根据现实需求不同，每一个标准对模型的影响权重也不一样。因此，我们也可以根据实际需求，选择权重大的作为评判标准。例如，模型的预测精度和所需运行时间之间的权衡。​假如针对猫的分类器A和B的分类误差分别是3%和5%，但是分类器A可能会把色情内容为猫误分类然后推送给用户，而分类器B却不会。因此在这种情况下，分类器B是一个更好的选择。针对这种情况，我们可以在误差项的计算里加入色情图片的权重，如果输入x是色情图片，那么相对应的误差惩罚会大得多，以此来使得色情图片不被误分类为猫而推送给用户。 当模型在所定义的指标以及dev/test set上表现很好，但是在实际的应用中表现却一般般，此时应该修改模型的评价指标或则修改dev/test set。 数据集选择 dev集和test集必需来自同样的分布，且最好能反映在未来的数据分布。而train集的分布可以允许和dev集、test集的分布不同（当然，分布能相同就尽可能得相同）； 当数据集较小时（例如100、1000、10000），train集和test集的比例可以为7：3，或则train集与dev集、test集的比例为6：2：2；当数据集很大时（例如1000000），则train集：dev集：test集=9.8：0.1：0.1 。当然，在满足上述条件的前提下，应使得测试集尽可能得大以给与模型的表现能力更高的置信度。 Human-level表现 当模型的表现超过human-level performance的时候，表现力的上升会慢慢减缓，但是决不可能超过Bayes optimal error。 人类在许多任务上已经可以达到很好的效果， 例如图片识别、语音识别等，如果模型的表现能力人类差时，可以考虑以下方法： Get labeled data from humans; Gain insight from manual error analysisi: Why did a persion get this right? Better analysis of bias / variance. 假设Training error为8%，Dev error为10%。当human-level performance（近似于Bayes error）为1%时，明显Training error距离human error还有着7%的差距，而距离Dev error只有2%的差距，因此在这种情况下，我们应该把模型的改进方案聚焦在偏差上，而不是方差；而当hu man-level performance为7.5%的时候，Training error距离human error只有0.5%的差距，而距离Dev error有2%的差距，因此此时我们应该聚焦在方差的改进，而不是偏差。 误差分析 检查在DEV集里被误分类的类别是什么、被误分类的原因是燊，并分析各自所占比重，再决定把精力花在解决哪个问题上面； 在数据集中，可能存在有一些数据集的标签是错误的。因此我们应该查看DEV集，确定是分类器的预测错误，还是由于数据集的标签错误； 当然被模型正确分类的数据集，也可能存在标签是错误的情况，但是此时你的模型却“正确分类”，这种情况也要考虑。但是一般情况所花费的精力会很多，所以对这种情况进行处理的一般较少； 在修正错误的标签集时，确保对dev集和test集同时操作，以保证数据分布的一致性； 数据分布不同原因与数据划分例：在猫分类器中，所用的训练数据来自于网上，这些图片的质量很非常好，但是这个分类器实际应用是在移动App上，而在移动App上，用户所拍摄的猫的图片往往质量没那么好，因此在训练集上表现很好的分类器，在实际应用中可能就表现很一般。且来自于网上的图片集量级A往往很大，例如1000000，而来自于用户所拍摄的图片集B可能只有10000。针对这种情况，有以下几种可能的解决方案： 将10000的数据集B加入1000000的数据集A中，然后打乱再重新按照98：1：1的比例分为train/dev/test，但是在这种处理下，很大可能dev和test所包含的数据集大部分来自于A，只有少部分来自于B，这跟设置dev集合的目的是相违背的。dev集和test集的数据分布应当与实际应用中的数据分布尽可能得一致——即与数据集B的分布一致，这样训练出来的模型才能在实际应用中表现很好。 训练集包含所有的数据集A和部分的数据集B，然后将剩下的数据集B划分为dev集和test集。虽然在这种情况下训练集和dev集、测试集的数据不一致，但是实际的表现效果会优于方案1。 验证集误差过高当训练集和验证机/测试集的分布不一致时，可以将训练集的一小部分抽取出来作为train-dev set，至此，整个数据集包含train、train-dev、dev、test四个集合，作用如下： train set：用于模型训练 train-dev set：具有和train set相同的数据分布，但不用于模型训练，只用于误差测定 若train-dev set的误差与dev set的误差相近，但比train set的误差高许多时，说明这是一个偏差问题；若若train-dev set的误差与train set的误差相近，但dev set的误差却比train-dev set的误差高许多时，说明这是一个由于训练集和验证集的数据分布不一致所造成的问题。 数据集关联分析假设训练误差为1%，验证误差为10%，如果训练集和验证机/测试集的数据分布一致，则说明这是一个高方差问题;但如果和上述例子类似，训练集和测试集的分布不一致的话，误差产生的原因可能只是因为训练集的图片质量较好分类器容易分辨，而验证集的图片质量普通从而导致分类器无法正确分类，造成了较大的验证误差。 数据分布不同解决方法 进行人工错误分析，发掘数据不匹配的缘由。通过分析train集和dev集之间的区别，尝试得到更多和dev集分布累计的train集； 采用人工数据合成的方法。比如在汽车内的语音识别系统，训练集为在安静环境下录制的10000小时的语音数据，但是实际的应用中，汽车内的语音识别系统的输入语音数据是包含的噪音的，比如汽车发送声、周围车辆的喇叭声、汽车内的回响等等。因此，假如你的拥有一小时的汽车噪音数据，为了train集和dev集尽可能得一致，可以通过人工数据合成的方法，把这一小时的噪音数据和10000小时的在安静环境下录制的语音数据进行合成，当然这有可能使得系统对这一小时的噪音数据过拟合。另外一个解决方法是录制10000小时的噪音数据，当然这个方法所耗费的精力会比较大。 Learning from multiple tasksTransfer learningpre-training和fine tune： 在深度神经网络训练中，使用先前已经训练好的模型的参数来作为初始化的模型参数，这便是pre-training；在之后的训练中更新模型的参数，这便是fine tuning; 迁移学习即把从一个数据集A学到的知识应用到另一个数据集B中。但是假如数据集A的量级比数据集B小，此时对B应用迁移学习并不是一个明智的做法，因为数据集A能给数据集B所提供的信息是很少的。此时应当对数据集B重新训练一个模型。 Muti-task learning定义：给定一个输入，可以同时对输入进行多个方面的判断。例如给定在自动驾驶中，给定一个图像，然后同时判断这个图像里是否有人行道、停止标志、骑车、红绿灯等。Y标签通常为[0, 1, 1, 0,……, 1]，其中 1 表示这副图像用于这个属性。 多分类学习与Muti-task learning的概念有点类似，但是多分类学习（如softmax、SVM）是指对一个输入，判定属于多个分类中的其中一个类别，例如给定一个动物图像，判定属于猫/狗/猪。在多分类学习下，Y标签通常为[0, 1, 2, 3, …..m]，其中m - 1为类别数。 在Muti-task learning中，训练集的Y标签是否存在某个属性有时是不确定的。例如给定一副图像，Y标签只告诉你这副图像有红绿灯和汽车，但对于是否有人行道、停止标志是不确定的，这些属性在Y标签被对应打上 ？ 号。因此，在计算损失函数时，这些属性未确定的损失是不计算进去的，只计算Y标签中已经指明为 0 或 1 的位置。 Muti-task learning只有在网络深度较大时的表现情况才比较好。因为深度神经网络可以学习到图像的某些低维特征，这些特征可以用来共享以同时进行多任务的学习。 对于需要对一个输入同时进行判定的问题，一个做法就是Muti-task learning，另一个就是针对每一个类别学习一个分类器。在每个类别的样本数据较少时，采用Muti-task learning是比较明智的做法，这样可以更好得学习到图像的特征；而当每个类别的样本数据都很多时，可以采用多分类模型，这样模型预测的准确度一般会较高； End-to-end learning当数据集足够大的时候，end-to-end leraning才适用。 Pros: Let the data speak. 在非End-to-end的模型中，常常人为得为模型定义一系列的网络层，这些网络层可以提取输入数据的某些特征，例如图像结构或则语音数据的发音等，但是这些提取出来的信息都是人为给定的，而在end-to-end模型中，直接学习从输入到输出的映射，可能可以发现数据里面更深层次潜在的特征信息； Less hand-designing of components needed. Cons: May need large amount of data; Excludes potentially useful hand-designed compoents.","link":"/categories/deep-learning/《Structuring-Machine-Learning-Projects》笔记/"},{"title":"《视频分析前沿》笔记","text":"Image FeaturesColor RGB：将RGB空间进行归一化，即可使得颜色信息独立于光强。 YIQ YUV HSI Edge和颜色特征相比，边缘特征对光照变化不敏感，具有较强的鲁棒性。 Optical Flow基于一个错误的假设下进行计算的。 Texture测定物体表面的强度变化，可得到图片的平滑性或粗糙性。 Object Tracking ApprochesPoint TrackingKalman FilterKernel FilterObject detectionBackground SubtractionSegmentationFrom a single Image(Machine Learning)Density ExtimationHistogramsThe bins of the histogram are defined as the intervals [x0 +mh, x0 +(m + 1)h), for m positive and negative integers, x0 is the origin and h the bin width. \\hat f(x) = \\frac{number of X_i in the same bin as x}{nh}分母中的h：对概率分布求积分，结果应该为1。对应到直方图即每个bin的面积和应该为1。在直方图中计算累计概率分布和时，每个bin的面积为长×宽，因此需要h。 Deawbacks： In procedures like cluster analysis and nonparametric discriminant analysis, using histogramresults in inefficient use of the data. The histogram is not continuous so trouble arises when derivatives are required. Choice of origin may have an effect in the interpretation. Representing bivariate or trivariate data by histogram is difficult. Naive EstimatorIf the random variable X has density f, then: f(x) = \\lim_{h\\to0}\\frac{1}{2h}P(x-h","link":"/categories/study/《视频分析前沿》笔记/"},{"title":"支持向量机","text":"最大间隔分类器函数间隔：$γ^{i} = y^{i}(w^{T} x + b)$， 改变w和b的量级，对分类结果不会产生任何影响，但是会改变函数间隔的大小。因此，直接对函数间隔求最大值是没有任何意义的。因为可以通过任意改变w、b的量级，使得函数间隔任意大。 Q1: 函数间隔的式子中，$w^{T} x + b$的结果在几何意义上代表什么？或则假设有两个样点$x_i$和$x_j$，如果$w^{T} x _i+ b$的值大于$w^{T} x_j + b$，是否表明$x_i$比$x_j$距离分类器更远，有更大的确信度分类正确？ A1: 第一问： 结果会随参数的量级的改变而改变。第二问：是的。 Q2: 假设有一个已经计算好参数的分类器，那么在函数间隔中，可以确保$w^(T)x + b$的符号和$y^(i)$的符号是一致的，即确保分类的正确性。此外，我们都知道当一个样点距离分类器越远的时候，则这个样点就有越大的确信度分类正确，但是在函数间隔中，$w^(T)x + b$的值是否也代表了这种确信度？ A2: 显然，$w^{T} x + b$并不能代表这种确信度。首先当我们任意改变w、b的量级时，分类器的方向和位置并不会发生任何改变，但是$w^{T} x + b$的值却会发生变化，因此无法表示分类正确的确信度。 几何间隔：在线性分类算法中，训练数据和拟合出来的直线之间的距离。 在logistic regression中，当$\\Theta^{x}x &gt;&gt; 0​$时，可认为样本有很大的概率和可信度被预测为1，反之亦成立。所以把这条结论应用到图像上时，最直观的解释就是数据到直线间的距离尽可能远。所以一个较好的拟合结果，就是确保直线和大部分训练数据的距离都尽可能远，即处于中间。 改变w和b的量级，对几何间隔没有任何影响。且，通过||w||，可以得到几何间隔和函数间隔之间的关系。 Q: 最大间隔分类器的目标就是要最大化几何间隔，那为什么要额外引入函数间隔? A: 函数间隔和几何间隔可以通过||w||进行转换，且函数间隔的量级会随参数的量级的改变而改变，这为之后最大间隔分类器的问题优化以及公式形式的转变提供了很好的帮助。 拉格朗日对偶拉格朗日对偶的重要作用是将w的计算提前并消除w。 核函数对于某个问题，如何确定具体使用的核函数形式？如何确保使用的这个核函数是可行的？ Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。我个人的体会是：使用libsvm，默认参数，RBF核比Linear核效果稍差。通过进行大量参数的尝试，一般能找到比linear核更好的效果。至于到底该采用哪种核，要根据具体问题，有的数据是线性可分的，有的不可分，需要多尝试不同核不同参数。如果特征的提取的好，包含的信息量足够大，很多问题都是线性可分的。当然，如果有足够的时间去寻找RBF核参数，应该能达到更好的效果。 L1 norm软间隔 参数C的作用？ C比较大的时候代表犯错越少越好，C比较小的时候代表找到的w越短越好，也就是在对的数据上的间隔越宽越好。 SVM完整推导过程 对于支持向量机，其主要作用的还是取决于距离线性平面最近的那几个点（亦称为支持向量），算法考虑了每一个支持向量，这也导致了当添加了一个额外的极端值之后，原本表现好的线性平面可能会受到影响，得到一个几何间隔更小的线性平面。 对于分类问题，假设数据是线性可分隔的，即可以找到一个超平面划分不同类的数据，那么某一样点距离超平面的距离越远，则分类正确的可信度越高。因此，可以证明，当超平面位于不同类数据的中间时，即最大化所有样本距离超平面的最小值，可得到最优间隔分类器。 函数间隔可以表示点到超平面的间隔。但是在函数间隔中，当参数w和b成线性变换时，分类结果不变，但是函数间隔的值却会改变。由此引入了一个新的概念：几何间隔。在几何间隔中，参数w和b线性变换时，几何间隔的值都不会发生改变。在通过固定间隔的值为1，去掉一些影响问题求解的参数后，整个问题变转化为： 为了求解这个问题，引入了拉格朗日对偶问题。通过对偶问题的求解以及KKT条件的限制，得到: 因此，原先的问题便转化为： 在低维度下，数据可能存在线性不可分的情况，而将特征映射到高维空间后，往往就可分了。因此，通过仿射函数将特征映射到高维空间。但此时又出现一个新的问题：在这个公式里，需要计算特征向量的内积，当特征向量非常多时且映射到高维空间后，直接计算所需要的时间复杂度O（n）很可能为$n^{2}$甚至更大。因此引入了核函数，可以证明，核函数可以化为两个仿射函数的乘积，从而通过直接计算核函数，而不是一一计算仿射函数，使得计算的复杂度缩小到O(n)。 核函数通过将低维数据映射到高维，在高维空间用超平面将数据分隔开来，可以解决原数据线性不可分隔的情况。 不过上述这个模型对于噪声点还是很敏感的，几个数据的变化都会导致整个超平面发生变化，如下图所示: 因此，为了避免这种情况的出现，在模型里添加了惩罚项C和$\\xi $,其中C比较大的时候代表犯错越少越好，C比较小的时候代表找到的w越短越好，也就是在对的数据上的间隔越宽越好。最终模型如下所示： 到这里，整个问题就转化成对拉格朗日参数$\\alpha$的优化求解，当$\\alpha$求解出来之后，w和b随之得解，于是便找到了我们所想要的超平面。那么如何对参数$\\alpha$进行求解呢？可以采用坐标上升法——即反复通过固定其他参数为固定值，对特定$\\alpha^{i}$进行优化，来优化求解所有的$\\alpha$值。但是在SVM中，因为条件 的限制，无法只对单一参数进行优化，因此导出了SMO算法。在SMO算法中，每次对两个不同的$\\alpha^{i}$和$\\alpha^{j}$同时进行优化，增加其中一个$\\alpha$的时候，对另一个值减少相同的量，以此满足限制条件 。对于SMO算法，每一次循环$\\alpha^{i}$和$\\alpha^{j}$的选择有一个启发式选择规则，即每次选择改变步长最大的j，可以使得算法更快得收敛。 至此，SVM模型中，所需要的所有参数都得以求解，我们得到了一个最优间隔分类器，将新的测试数据代入计算，通过结果大于0或则小于0，便可完成分类功能。 Coursera：SVM推导思路在之前的分类问题中采用的是logistic回归的方法，在logistic回归中，只要求分类器能够正确得分类不同的样本，使得对应正样本函数输出$\\theta^(T)X&gt;=0$，且对应负样本函数输出$\\theta^(T)X&lt;0$即可，而对于分类正确的确信度没有太大的要求。然而在SVM中，不仅仅要求找到分类器，且要尽可能使得对于每一个样本，分类器都有较大的概率分类正确，即最大化样本距离分类器的间距。 在SVM中，首先用两条直线近似logistic回归中的代价函数，然后用参数C替代正则化常数$\\lambda$，可以证明，这个推导过程是近似等价的。接着将参数C设置为很大的一个数（例如1000000），因此在最小化代价函数过程中，公式的第一项就要尽可能为0，等同于对于所有的正样本函数输出$\\theta^(T)X&gt;=$1，且对应负样本函数输出$\\theta^(T)X&lt;=-1$，由此推导出SVM的目标方程。 参数C设置很大的时候，等同于正则化常数$\\lambda$变小，此时的话函数易于过拟合，即函数对于样本中的异常点会异常敏感，一个样本的变动都会使得分类器改变，这显然不是我们想要的。因此，我们不能将C设置得过大，亦不能将C设置过小。（C类似于上面“四、L1 norm软间隔问题”中参数C的作用） 下面是一些普遍使用的准则（n 为特征数， m 为训练样本数。）： 如果相较于 m 而言， n 要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 如果 n 较小，而且 m 大小中等，例如 n 在 1-1000 之间，而 m 在 10-10000 之间，使用高斯核函数的支持向量机。 如果 n 较小，而 m 较大，例如 n 在 1-1000 之间，而 m 大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 Q：为什么支持向量机是一个最大间距分类器？（为什么如上的目标方程可以得到最大间距分类器？） A：SVM试图极大化$p^(i)$的范数，即训练样本到决策边界的举例。（参考笔记196-200页部分）","link":"/categories/machine-learning/支持向量机/"},{"title":"毕设选导系统：宣传文案","text":"产品概述 毕设导师智能分配系统是一个用来简化传统手工匹配繁琐操作的系统。本系统将学生报志愿、系负责人收集整理数据、相关人员进行手工分配、反馈选择结果等繁琐的操作转移到线上。把毕设导师互选的所有流程，传化对本系统的操作。减少了相关人员的工作量，降低了流程中由于手工操作而出现错误的可能。学生的志愿选择、导师分配、数据统计、结果查看及导出等操作均可在上系统完成，提高了毕设导师选择的效率。 产品功能用户功能学生用户 查看和修改个人信息 进行志愿的填报 查看专业导师列表以及导师的详细信息（包括职称、课题内容、学生数等） 查看分配结果，包括导师的主要信息以及同一个导师的其他学生信息 导师用户 查看和修改个人信息 设置所带的学生数以及课题研究内容 查看选择自己的学生列表以及学生的详细信息（包括绩点、排名、志愿序等） 查看分配结果，包括自己的所有学生的主要信息和联系方式 系负责人用户 查看和修改个人信息 通过Excel文件进行学生和老师信息的批量导入 手动添加或则删除学生或导师信息 对导师分配系统的整个流程进行相关数据的设置 可设置志愿数、学生人数（实验班和非实验班）、年级、各个时间段等信息 利用智能分配算法对未匹配到导师的学生进行一键分配 以Excel文件格式导出分配结果（包括导师和学生两种导向） 院负责人用户 查看和修改个人信息 设置每个系的系负责人 对分配结果进行微调操作 以Excel文件格式导出分配结果（包括导师和学生两种导向） 毕设互选运行流程 步骤 流程 1 院负责人设置各个系的负责人 2 系负责人进行匹配设置 3 导师填报学生人数和选题 4 学生填报志愿（第一轮） 5 导师选择学生（第一轮 6 学生填报志愿（第二轮） 7 导师选择学生（第二轮） 8 系负责人利用智能分配对未分配到的学生进行一键分配 9 院负责人对结果进行微调 10 系和院负责人导出分配结果 11 导师和学生查看各自结果 12 毕设互选完毕 产品的创新特色使用系统前 工作量庞大 ：学生志愿信息由系负责人手动收集整理，工作量庞大； 师生之间信息了解少 ：学生对导师的信息了解较少，需反复得登录学院网站进行查询；导师对学生信息的了解大多也只能通过系负责人等中间方式进行了解； 无师生互选 ：导师没有选择学生的权利，伯乐千里马难以相遇； 分配过程不透明 ：导师分配过程不透明，系负责人在对结果进行微调的时候也容易出现细节错误，任务繁重； 结果信息通知繁琐 ：分配结果信息通知过于单一，采用文件传送或邮件的方式进行通知，而且师生之间的联系方式也没有提供。 使用系统后 师生二轮互选 ：历史创新式得引入了师生二轮互选的流程，让选导过程透明化、高效化、简单化； 智能分配 ：系统引入了ACM大神编写的高效智能分配算法，在师生互选二轮结束后，系负责人可以通过智能分配算法一键对学生进行导师分配。且该算法经过多次真实数据的测试，学生的分配结果大都集中在志愿序第一或第二的导师，完美得契合了每个学生的导师预期； 查看专业导师信息 ：学生可以在专业导师界面查看到自己专业的所有导师信息，包括职称、课题等重要信息，为学生的志愿填报提供了一个很好的帮助； 设置课题和学生数 ：导师可以在系统设定自己的课题内容以及所期望带的学生数，更加灵活充分得提现了导师的意愿; 导师选择学生 ：导师可以在学生列表查看到填报自己的学生，并查看学生的详细信息，包括绩点排名、志愿序等重要信息，有助于导师选择心仪的学生； 匹配信息设置 ：系负责人可以在设置界面对二轮选择的时间段以及学生数、志愿数等信息进行设置，图形化的界面加上时间控件的人性化选择，让时间设置更加便捷； 分配结果查询 ：每个用户均可以看到对应的分配结果信息，学生用户可以看到和自己同一个导师的学生，导师可以看到自己的所有学生列表，且均有联系方式等详细信息提供，方便师生之间的互相联系； Excel文件导出 ：系负责人和院负责人可以对分配结果进行Excel文件的导出，且导出格式有两种，一种以学生为导向，一种以老师为导向（对于一个老师用户多名学生的情况，系统还会系统处理Excel单元，进行智能单元格的合并），两种导向的Excel文件让结果信息更加清楚。 其他创新特色 Web端系统 ：导师选择这种事情，每个学生整个大学只需要一次，而Web端的操作便捷简单，用户不需要再额外安装什么环境或插件，只需要一个浏览器、一颗认真的心，便可以选择到自己心仪的导师或则学生； 一站式服务 ：整合了报课系统，只要一个网址，便可完成报课和导师分配等功能。 推广方案潜在目标用户几乎各大高校各个学院都需要导师互选这个环节，所以理论上讲各大高校都能成为我们的目标用户，但是由于现实中各种环境以及资源因素的限制，我们的理想的潜在目标用户范围可缩小至福州大学城周边的各个学院。 系负责人用户：选导过程以往都是采用手工收集志愿信息再分配的方式，这会加剧系负责人的工作量，而这个系统很好得考虑到了系负责人的需求，系负责人只需要设置一下匹配信息，然后最后点击智能分配按钮，便可以完成整个毕设选导的过程； 导师用户：很多导师都希望自己拥有选择学生的权利，并且可以查看到学生的详细信息，这可以让导师选择到自己心仪的学生； 学生用户：相比以前，对于学生而言，这个系统的带来的好处主要是可以查看到本专业导师的详细信息和分配结果信息，让志愿填报更合理化； 院负责人用户：系统提供的Excel导出功能，能让院负责人对分配结果的获取更为便捷直观。 首批目标用户基于该系统的特殊性，当学院选择该系统进行毕设互选时，所有用户都必须使用该系统，而当学院没有选择该系统进行毕设互选时，基本上也没有用户会登录系统进行操作，所以这个系统的推广以及用户量很大一部分还取决于系统的完成度以及学院的政策。当然，作为系统的开发者，我们会尽一切努力让毕设互选系统更加得人性化和高效化，以求用代码改变世界，解放双手！ 具体推广方案如下： 张栋老师作为这个系统的指导老师，此外本身也是计算机实验班的系负责人，在了解清楚这个系统的完成度之后，可以以张栋老师为媒介向其他系负责人进行推广； 和学院负责人进行沟通，充分了解院负责人的需求，并展示系统模拟真实数据测试之后的匹配结果，以说明该系统的有效性和可用性； 编写系统的用户指南，在推广过程中，给予用户对应的用户指南，并通过简单的流程演示展示整个毕设互选系统的流程，以求让用户了解到该系统所带来的便捷性； 对系统简单的操作流程进行视频的录制，制作出毕设导师互选系统的使用视频，让用户对系统的了解程度更深。 通过以上的推广方案，拟实现2017年该系统便能上线运行，当然在上线之前，系统还需要进行多次的数据测试，才能保证在实际运用中不出现严重的功能性BUG，给用户良好的体验。 扩大用户群体为了扩大用户群体，拟采用线下渠道推广为主，线上渠道推广为辅，“本学院-&gt;本校其他学院-&gt;大学城其他学校-&gt;上线系统官网，面向全国高校”逐步逐层递进的方式。其中线下渠道推广主要是直接拜访各院校教学办负责人，然后根据实际情况对系统进行相应的个性化定制。而线上渠道推广则主要是在系统官方网站上进行宣传推广，并提供简化版的导师互选系统供潜在用户进行在线试用。 具体推广方案如下： 当系统在数计学院试运行成功后，对院负责人、系负责人、学生、导师等进行用户调研，收集她们对这个系统的反馈情况，并根据反馈情况对系统进行改进； 通过学院负责人的引导，会见其他学院的负责人，并向其他学院负责人展示该系统在第一次上线运行后所带来的便捷性和高效性，并将系统运行的数据整理成图标，以最直观的形式向其他学院难度负责人用户进行推广； 在福大内推广成功后，可向大学城周边的高校进行推广，以福大为正面例子，在树立良好口碑的基础之上，针对各个高校进行个性化的选导定制。 由于系统的复杂度相当之高，而当引入其他学院的需求之后，系统的重构是势在必行的事情！这对于我们团队而言是一个相当巨大的考验，不过我们会继续发扬软工实践的拼劲，把每一滴汗水带到真正的实际项目中，以求给用户更好的体验！ 我们坚信，代码能改变世界，更能改变自己！ 系统官网 Github项目链接 毕设导师互选系统项目链接 毕设导师互选系统官网链接","link":"/categories/software-engineering/毕设选导系统：宣传文案/"},{"title":"神经网络：动态部分","text":"训练技巧损失函数可视化损失函数一般都是定义在高维度的空间中，这样要将其可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，例如，随机生成一个权重矩阵W，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着此方向计算损失值，计算方法是根据不同的$\\alpha$值来计算$L(W+aW_1)$。这个过程将生成一个图表，其x轴是$\\alpha$值，y轴是损失函数值。同样的方法还可以用在两个维度上，通过改变a,b来计算损失值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，a,b可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示： 过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。 损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。 梯度检查 使用中心化公式。 用泰勒展开式，该公式的误差近似$O(h^2)$。 使用相对误差比较数值梯度和解析梯度的不同。若使用绝对值进行比较：当差值为1e-4，若梯度值在1.0左右，则差值较为合适；若梯度值为1e-5或更低时，则1e-4为非常大的差距，梯度实现就会被当成失败的。 在实践中： 相对误差&gt;1e-2：通常就意味着梯度可能出错。 1e-2&gt;相对误差&gt;1e-4：要对这个值感到不舒服才行。 1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。 1e-7或者更小：结果很好。 使用双精度。 目标函数的不可导点。例如在ReLU或SVM损失中，由于max函数的使用，导致某些数据在x = 0时为不可导点。这会使得解析梯度和数值梯度的计算结果出现一定的差距。 谨慎设置步长h。在实践中h并不是越小越好，因为当特别小的时候，就可能就会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将调到1e-4或者1e-6。 在操作的特性模式中梯度检查。梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。因此为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。 不要让正则化吞没数据。 正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。 关闭随机失活（dropout）和数据扩张（augmentation）。 检查少量的维度。在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。 合理性检查的提示与技巧 寻找特定情况的正确损失值。在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于Softmax分类器，一般期望它的初始损失值是2.302；对于SVM，假期望损失值是9（因为对于每个错误分类，边界值是1）。 提高正则化强度时导致损失值变大。 对小数据子集过拟合。最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然正则化会阻止让损失为0。 权重更新比例权重中更新值的数量和全部值的数量之间的比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。 可视化如果数据是图像像素数据，那么把第一层特征可视化会有帮助： 左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。 局部最小值在使用梯度下降等方法更新权重参数的时候，有时候参数可能会收敛于局部最小值。在小的神经网络中，不同的局部最小值对于模型的影响确实很大，但是在大的神经网络中，随着时间的推移，最优的局部最小值和最差的局部最小值之间的差异会越来愈小。因此在大的神经网络中，通常认为收练的局部最小值等同于全局最小值。 学习率的选择以及退火在开始阶段，应选择较高的lr，这样loss就会较快得下降。但是下降到某一节点之后可能无法达到最小值，此时需要降低lr，以用更低的学校效率到达最低的loss点。 随步数衰减：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 指数衰减。数学公式是，其中是超参数，是迭代次数（也可以使用周期作为单位）。 1/t衰减的数学公式是，其中是超参数，t是迭代次数。 在实践中，发现随步数衰减的随机失活更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比![k](http://www.zhihu.com/equation?tex=k)更有解释性。最后，如果有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 梯度下降SGD沿着负梯度方向改变参数（因为梯度指向的是上升方向，但是我们通常希望最小化损失函数）。假设有一个参数向量x及其梯度dx，那么最简单的更新的形式是： 12# 普通更新x += - learning_rate * dx 其中learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。 Momentum如果把梯度下降法想象成一个小球从山坡到山谷的过程，那么前面几篇文章的小球是这样移动的：从A点开始，计算当前A点的坡度，沿着坡度最大的方向走一段路，停下到B。在B点再看一看周围坡度最大的地方，沿着这个坡度方向走一段路，再停下。确切的来说，这并不像一个球，更像是一个正在下山的盲人，每走一步都要停下来，用拐杖来来探探四周的路，再走一步停下来，周而复始，直到走到山谷。而一个真正的小球要比这聪明多了，从A点滚动到B点的时候，小球带有一定的初速度，在当前初速度下继续加速下降，小球会越滚越快，更快的奔向谷底。momentum 动量法就是模拟这一过程来加速神经网络的优化的。 123# 动量更新v = mu * v - learning_rate * dx # 与速度融合x += v # 与位置融合 A为起始点，首先计算A点的梯度∇a ,然后下降到B点， 到了B点需要加上A点的梯度，这里梯度需要有一个衰减值 ,推荐取0.9。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。这样一步一步下去，带着初速度的小球就会极速的奔向谷底。 Nesterov动量法每下降一步都是由前面下降方向的一个累积和当前点的梯度方向组合而成。于是一位大神就开始思考，既然每一步都要将两个梯度方向（历史梯度、当前梯度）做一个合并再下降，那为什么不先按照历史梯度往前走那么一小步，按照前面一小步位置的“超前梯度”来做梯度合并呢？如此一来，小球就可以先不管三七二十一先往前走一步，在靠前一点的位置看到梯度，然后按照那个位置再来修正这一步的梯度方向。如此一来，有了超前的眼光，小球就会更加”聪明“, 这种方法被命名为Nesterov accelerated gradient 简称 NAG。 1234x_ahead = x + mu * v# 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)v = mu * v - learning_rate * dx_aheadx += v 123v_prev = v # 存储备份v = mu * v - learning_rate * dx # 速度更新保持不变x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式 Adagrad随着算法不断的迭代，cache会越来越大，整体的学习率会越来越小。所以一般来说adagrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。Adagrad的一个缺点是，在深度学习中学习率总是单调减小，被证明通常过于激进且过早停止学习。 123# 假设有梯度和参数向量xcache += dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) cache表示前t步梯度dx的累加； 用于平滑的式子eps（一般设为1e-4到1e-8之间）是防止出现除以0的情况； 平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多； cache的累加为梯度的平方，是因为接下去要进行平方根操作，以防平方根内出现负数。 RMSpropRMSprop用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均： 12cache = decay_rate * cache + (1 - decay_rate) * dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) decay_rate是一个超参数，通常取值为[0.9, 099, 0,999]。RMSpro和Adagrad主要的区别是在于cache的更新方式，可以将其拆为两个部分：其中0.9部分取决于之前的梯度和，剩下0.1部分取决于当前的梯度。如果当前梯度很大，学习率衰减得就会很快；如果当前梯度小，学习率衰减得就会慢点。利用这个特性，有效得解决了Adagrad方法的缺点。 AdamAdam是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。简化的代码是下面这样： 123456789101112m = beta1*m + (1-beta1)*dxv = beta2*v + (1-beta2)*(dx**2)t += 1########## 完整版 ## 偏置矫正，只有在前几次迭代时（当t较小时）作用比较大# 主要是一种针对m、v初始为0的补偿措施，将m、v变大m_bias = m / (1 - beta1**t)v_bias = v / (1 - beta2**t)#########x += - learning_rate * m_bias / (np.sqrt(v_bias) + eps)","link":"/categories/deep-learning/神经网络：动态部分/"},{"title":"结对编程1：需求分析与原型设计","text":"需求分析（NABCD模型）N（Need，需求） 师生互选，改变导师被动分配学生的局面 每个学生分配到预想的导师，避免分配了志愿以外的导师的情况出现 学生可以了解到所有导师的课题和研究方向 导师可以设定自己想要的学生数 导师所分配到的学生数尽可能平均 志愿填写和导师的学生数设置均可在系统里完成 系统可根据志愿情况以及导师需求，通过一定算法得出最后的分配结果 A（Approach，做法）导师选择每个学生大学四年只需做一次，固开发成单独的一个APP不是很理智的选择，我们小组采用的做法是开发一个基于Android端的导师选择系统，然后整合进现有的APP比如福大易班、福大教务通、福大助手等。这样在开发过程中便可以专注于主要功能模块的设计，而不用浪费精力在边边角角的界面上。 导师选择系统大致的运行流程如下： 学生和导师在第一次登录系统时便要完善个人信息，进行实名认证。特别要完善其对应的专业技能兴趣或则课题研究方向，系统将自动这些信息进行匹配，为学生计算出每个导师的推荐度，便于学生更有目的性得进行选择； 学生登录系统，在规定期限内填写并提交五个志愿的导师； 导师登录系统，在学生列表界面查看相应的学生信息，并选择自己想要的学生； 所有导师操作完毕之后，分配算法如下： 学生只有被一名导师所接受，将直接分配 多名导师选择同一学生，该学生将被分配给志愿优先级高的导师 剩余学生按照绩点进行排序，并依次检索其志愿，若对应志愿的导师已分配的学生数未满其设定值，将直接进行分配； 将导师按照已分配学生数升序排序，剩余学生依次分配给对应导师，以求尽可能让所有导师的学生数平均分布； 算法运行结束，导师分配完毕。 学生和导师登录导师选择系统，查看最终的分配结果，并联系其对应的导师或学生 B（Benefit，好处） 改变被动分配的局面，让每个导师参与学生的选择 推荐度的设置让学生选择导师时更理性化，更具指导性 导师分配结果大致符合学生和导师的预期 学生信息以及导师信息透明化，方便彼此的了解以及沟通 大大简化了年级负责人以及系负责人繁琐复杂的人工分配 每名导师所分配到的学生数量尽可能平均 互联网正在移动化，基于Android端的系统便于用户下载，实时查看导师信息和分配结果 学习成绩好的学生有更大机会分配到理想的导师，以此鼓励学生努力学习 C（Competitors，竞争） 目前学校的导师分配大都采用人工排序分配的做法，尚无成熟的系统可供使用，市场竞争小 潜在竞争对手可视为目前参加软工实践的学生，其中不乏有拥有丰富项目经验的优秀码农，他们全都在对“导师分配”这块大蛋糕垂涎欲滴 D（Delivery，推广） 当原型系统被采纳之后，立马投入精力进行开发 通过博客以及软工课程进行初期的推广，预期让所有修读软工的学生了解到该系统，并从学生角度出发期望得到建设性的意见 通过邮箱或则当面交谈，尽可能向学院所有导师推广这一个系统，并从中汲取到更为专业、更全面的建议并加以改进 结对过程我和我的对友在之前便有过合作的经历，我有过一定的Android开发经验，而对友对前端的掌握也很好，故作业发布后不久我们便结对了。在整个导师选择系统的设计过程中，我主要是负责画原型，队友则是进行初期的草稿以及文档的完善。结对让我们效率更为高效，短短几天时间便完成了作业。 原型设计 原型设计工具：Axure rp 7.0 学生端 主页界面——拥有志愿填写、导师信息、个人信息、结果查询四个功能模块； 个人信息——每名学生均要进行实名认证，并完善自己的个人信息，尤其专业兴趣。这些信息在导师选择学生的时候将会起到重要的作用。 志愿填写——学生可以在这个界面填写自己的志愿信息，这些信息将在导师选择学生时起到关键性的作用；另外，在规定期限截止之前，学生可以任意修改志愿信息； 结果查询——在规定期限之后，导师选择系统根据算法分配导师成功。每名学生均可在结果查询界面查询到自己的导师以及相同导师的同学，并通过上面的联系方式互相取得联系，更有利于之后毕设工作的指导进行。 导师信息——导师信息界面可以根据推荐度、研究方向、学生数等进行搜索排序，而每一个导师都会根据学生以及导师的兴趣方向自动计算出一个推荐值，可供学生更加理性得进行导师的选择； 导师简介界面有着每一位导师完整的资料以及联系方式，学生可以通过联系方式与心仪的导师事先取得沟通，更有利于之后的互选成功率。 导师端 主页界面——拥有学生列表、个人信息、结果查询等三个界面。 学生列表——拥有待选学生和已选学生两个界面。学生填报志愿完成后，在待选学生界面，每位导师均可查看选中自己的学生信息，并决定是否接受。当学生被导师给选中之后，相应信息会传递到已选学生界面，并在后台进行相应数据得更新，让分配算法的运行更具准确性。而在已选学生界面，导师亦可通过侧滑删除的方法，筛掉先前已接 受的学生。 个人信息——导师可以在这个界面完善自己的个人信息，设定想要的学生数量以及研究方向，这些信息都将为学生选择时提供关键的指导作用。 结果查询——在规定期限之后，导师选择系统根据算法分配导师成功。每位均可在结果查询界面查询到自己的学生，并通过上面的联系方式互相取得联系，更有利于之后毕设工作的指导进行。 效能分析 内容 时长 需求分析 1H 导师分配流程设计 2H 手绘原型草图 3H 用Axure RP进行原型设计 6H 用MD进行文档编写 4H 系统后期完善 1H 因为客户的需求都比较明确，且自身也经历过导师选择，所以前期的需求分析还是比较明朗的，而导师分配流程则是和对友在互相激烈的辩论出得出来的。整个导师分配系统流程确定后，较为头痛得便是原型的设计，我们在稿纸上画了好多版的原型之后，才最终确定了如上图所示的系统。之后便开始原型设计和文档的编写。 整个过程大致用了17个小时，基本都是集中在两三天之内，工作连续紧凑，目的性强，期间也参考了许多其他同学的想法。整体的效率还是蛮高的。 PSP PSP 计划 预估时长为一个月左右 开发 需求分析：师生互选，系统后台自动完成导师分配，减少人工工作量 生成设计文档：.md文档 设计复审：多次审核，共同讨论，确保原型的正确性和完整性 代码规范：大小驼峰命名法；杜绝中文命名；适当采用单词缩写；见名知意 具体设计：数据库设计、接口设计、界面编写、逻辑跳转等 具体编码：Android、JavaWeb、PHP 代码复审：在开发过程中不断对系统进行完善修改 测试：单元测试、黑白盒测试、BUG修正 测试报告 利用测试结果进行测试报告的编写 工作量计算 预估服务器端的任务比较艰巨，需要实时更新信息，并运行分配算法给出最终的分配结果；Android端因为有过开发经验，界面之间的逻辑也较为简单，工作量较小 事后总结 暂无 过程改进 暂无 总结导师选择是每个学生大学必将面临的问题，在几个月之前我们也完成了自己导师的选择，但是整个选择过程就如客户需求所述一样，高度不透明，繁琐复杂，分配结果也很难做到让每个人都满意。因此，开发一个透明化、智能化的导师选择系统是时势所趋，这次软工作业正好也给了我们一个时机去认真思考这个问题并努力解决。 在整个系统的设计过程中，结对的高效率得到了充分的提现，思想的碰撞让我们迅速得到的系统的大致分配流程，而各自的开发经验让我们对原型的设计以及文档的编写也是得心应手。互取其长，互补其短，我们在短短两天的时间内便完成了原型系统的设计。期间对Axure RP软件的使用以及项目的开发流程有了更深层次的理解。 虽然文档以至尾声，但我们并未将其认定为此次作业的结束，我们期待的是专属于我们的第三次作业的出现。斯是陋室，惟吾德馨。希望我们的原型系统能够得到您的认可！ 附件: 导师选择系统PDF文档 导师选择系统MD文档","link":"/categories/software-engineering/结对编程1：需求分析与原型设计/"},{"title":"结对编程2：毕设选导系统","text":"问题重述编码实现一个毕设导师的智能匹配的程序。提供输入包括：30个老师（包含带学生数的要求的上限，单个数值，在[0,8]内），100个学生（包含绩点信息），每个学生有5个导师志愿（志愿的导师可以重复但不能空缺）。实现一个智能自动分配算法，根据输入信息，输出导师和学生间的匹配信息（一个学生只能有一个确认导师，一个导师可以带少于等于其要求的学生数的学生）及未被分配到学生的导师和未被导师选中的学生。 问题分析问题描述中要求测试数据自写程序随机生成，现实中存在着许多影响因素，但为保证分配算法不至于太过复杂， 因此生成测试数据时只考虑绩点因素。而在简化版的毕设导师分配系统中，对分配结果起到决定性作用的也就只有两个因素——志愿顺序和优先级。为使成绩靠前的学生尽量学到自己满意的导师，我们拟定采取导师学生名单满后按照绩点顺序进行筛选的方法进行导师的分配。 核心算法在实际毕设导师选择的时候，若采用双向选择，则需考虑到众多因素。这里为了数据处理方便，在导师选择的时候只考虑绩点这一关键因素，即根据绩点排序进行学生的筛选。 整个算法大致步骤如下所示： 程序随机生成学生信息、导师信息等测试数据 算法进行五轮分配，在第i轮时，依次检索所有学生的第i个志愿填报情况 将该学生添加进第i个志愿所对应导师的学生名单 若该志愿所对应导师的学生名单未达其设定学生数上限，则进行下一个学生的检索 若该该志愿所对应导师的学生名单超过其设定学生数上限，则将其学生名单按照绩点进行排序 将绩点排名最后的学生移除该导师的学生名单，并设定该学生状态为未选中导师，在下一轮时再次对其进行分配 算法进行五轮分配完毕，输出分配结果，程序运行结束。 算法流程图： 代码分析实现语言：JAVA 开发平台：Eclipse 学生类 1234567891011public class Student { //学号 private int studentID; //绩点 private double score; //姓名 private String name; //导师工号 private int teaID; //五个志愿 private int[] chooseTeacher = new int[5]; 导师类 12345678910111213141516171819202122public class Teacher { //优先队列因子，绩点小的在对头 Comparator&lt;Student&gt; OrderIsdn = new Comparator&lt;Student&gt;(){ public int compare(Student s1, Student s2) { if(s1.getScore() &gt;= s2.getScore()) { return 1; } else if (s1.getScore() &lt; s2.getScore()) { return -1; } else { return 0; } } }; //保存导师的学生名单，优先队列结构可使学生根据绩点竞争筛选时更为简单 private Queue&lt;Student&gt; student = new PriorityQueue&lt;Student&gt;(10, OrderIsdn); //姓名 private String name; //导师工号 private int teaID; //学生数上限 private int stu_num_max; 随机生成学生数据，考虑实际情况，学生绩点的生成范围为1.0~4.5 123456789101112131415161718192021222324252627282930public void initStudentData() { for (int i = 0; i &lt; STU_NUMBERS; i ++) { students[i] = new Student(); students[i].setStudentID(i + 1); if (i &lt; 9) { students[i].setName(&quot;student_0&quot; + (i + 1)); } else { students[i].setName(&quot;student_&quot; + (i + 1)); } //根据现实情况，随机生成的绩点信息范围在1.0~4.5 students[i].setScore(((double)rand.nextInt(3500) + 1000) / 1000); int tempID; int[] tempTea = new int[5]; for (int k, j = 0; j &lt; 5; j ++) { tempID = rand.nextInt(TEA_NUMBERS) + 1; for (k = 0; k &lt; j; k ++) { if (tempTea[k] == tempID) { j --; break; } } if (k == j) { tempTea[j] = tempID; } } students[i].setChooseTeacher(tempTea); } } 随机生成导师数据，包括工号、姓名、学生数上限等信息 1234567891011121314public void initTeacherData() { for (int i = 0; i &lt; TEA_NUMBERS;i ++) { teachers[i] = new Teacher(); teachers[i].setTeaID(i + 1); if (i &lt; 10) { teachers[i].setName(&quot;teacher_0&quot; + (i + 1)); } else { teachers[i].setName(&quot;teacher_&quot; + (i + 1)); } teachers[i].setStu_num_max(rand.nextInt(9)); } } 导师分配系统核心算法 1234567891011121314151617public void distTeacher() { for (int i = 0; i &lt; 5; i ++) { for (int k = 0; k &lt; STU_NUMBERS; k ++) { if (students[k].getTeaID() == 0) { int index = students[k].getChooseTeacher()[i] - 1; students[k].setTeaID(index + 1); teachers[index].getStudent().add(students[k]); if (teachers[index].getStudent().size() &gt; teachers[index].getStu_num_max()) { students[teachers[index].getStudent().peek().getStudentID() - 1].setTeaID(0); teachers[index].getStudent().poll(); } } } } } 算法运行完成后，输出分配结果至data.txt文档 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public void printData() throws IOException { int count = 0; String temp; File file=new File(&quot;data.txt&quot;); if(file.exists()) { file.delete(); } else { file.createNewFile(); } FileWriter fileWriter=new FileWriter(file); @SuppressWarnings(&quot;resource&quot;) BufferedWriter bufferedWriter=new BufferedWriter(fileWriter); //输出学生信息，包括分配的导师 bufferedWriter.write(&quot;***********************\\r\\n&quot;); bufferedWriter.write(&quot;--------学生信息---------\\r\\n&quot;); bufferedWriter.write(&quot;***********************\\r\\n\\r\\n&quot;); for (int i = 0; i &lt; STU_NUMBERS; i ++) { bufferedWriter.write(&quot;学号：&quot; + students[i].getStudentID() + &quot;&quot; + &quot;\\r\\n&quot;); bufferedWriter.write(&quot;姓名：&quot; + students[i].getName() + &quot;\\r\\n&quot;); bufferedWriter.write(&quot;绩点：&quot; + students[i].getScore() + &quot;&quot; + &quot;\\r\\n&quot;); bufferedWriter.write(&quot;五个导师志愿：&quot;); for (int j = 0; j &lt; 5; j ++) { bufferedWriter.write(students[i].getChooseTeacher()[j] + &quot; &quot;); } bufferedWriter.write(&quot;\\r\\n&quot;); if (students[i].getTeaID() == 0) { temp = &quot;无&quot;; } else { temp = students[i].getTeaID() + &quot;&quot;; } bufferedWriter.write(&quot;已分配导师：&quot; + temp + &quot;\\r\\n&quot;); bufferedWriter.write(&quot;\\r\\n&quot;); } //输出导师信息，包括导师的学生名单 bufferedWriter.write(&quot;***********************\\r\\n&quot;); bufferedWriter.write(&quot;--------导师信息---------\\r\\n&quot;); bufferedWriter.write(&quot;***********************\\r\\n\\r\\n&quot;); for (int i = 0; i &lt; TEA_NUMBERS; i ++) { bufferedWriter.write(&quot;导师工号：&quot; + teachers[i].getTeaID() + &quot;&quot; + &quot;\\r\\n&quot;); bufferedWriter.write(&quot;姓名：&quot; + teachers[i].getName() + &quot;\\r\\n&quot;); bufferedWriter.write(&quot;学生数上限：&quot; + teachers[i].getStu_num_max() + &quot;&quot; + &quot;\\r\\n&quot;); bufferedWriter.write(&quot;学生名单：&quot; + &quot;\\r\\n&quot;); for (Student stu : teachers[i].getStudent()) { count ++; bufferedWriter.write(&quot; &quot; + stu.toString() + &quot;\\r\\n&quot;); } bufferedWriter.write(&quot;\\r\\n&quot;); } bufferedWriter.write(&quot;***********************\\r\\n\\r\\n&quot;); bufferedWriter.write(&quot;未分配人数：&quot; + (STU_NUMBERS - count) + &quot;\\r\\n&quot; + &quot;导师分配成功率为：&quot; + count + &quot;%&quot; + &quot;\\r\\n&quot;); bufferedWriter.flush(); System.out.println(&quot;导师分配系统运行成功！\\r\\n导师分配结果信息已保存至data.txt文档里!&quot;); } 结果分析运用程序进行了十次样本测试，测试结果如下： 次数 学生总数 导师总数 未分配学生数 未分配导师数 匹配率 第一次 100 30 3 4 97% 第二次 100 30 1 1 99% 第三次 100 30 9 3 91% 第四次 100 30 0 1 100% 第五次 100 30 3 6 97% 第六次 100 30 2 2 98% 第七次 100 30 6 4 94% 第八次 100 30 2 2 98% 第九次 100 30 7 4 93% 第十次 100 30 4 4 96% 平均 100 30 2.9 3.1 96.3% 分析: 从结果表中可以看出，平均未分配学生数为2.9，未分配到学生的导师数为3.1，且导师分配成功的学生比例高达96.3%！这充分表明了我们的的算法基本满足了题意要求。 示例：分配结果示例链接 程序评价本毕设导师智能分配程序的分配结果基本满足了题意要求，但是程序尚且存在着一些不足。在随机生成的学生信息和导师信息数据和现实生活中有所偏颇。一方面在导师志愿填写的时候，往往存在一些热门老师和冷门导师，这些导师受学生的关注度有着较大的差别，然而在程序随机生成数据的时候，并未考虑进这个因素，而是把所有导师在志愿填报上面的概率设为一致；另一方面，在实际中，学生的绩点信息往往会呈现出一种两端少，中间多的情况，而在生成数据的时候却还是把绩点生成的范围设为一致。这两个方面在一定程度上面对分配结果会产生一定的影响。 总结 031401433 张斯巍 两个人一起想算法，写代码真的比一个人单干有趣的多，特别是两个人的不同的想法提出来之后，通过分析优缺点以及讨论最终实现的方案，会比一个人的想法更加的全面，更加具有可行性。当一个想法不能持续下去时，队友之间能够互相打气，查找相关资料后，最终完成了我们的代码。这次结对作业针对实际问题要求用程序来实现，不仅需要导师匹配，还需要考虑一些其他的因素，我们讨论了一下想要用java实现，由于我的队友编程能力比较强，一些想法还有编程都是队友提供帮助的。 031402304 陈燊 本次作业的工作量相对上一次而言少了挺多，因为在上一次作业的需求分析中，我们对这个毕设智能分配系统已经做了一个充分的讨论，而本次算法的实现思路和我们之前的想法大致也是一样的，又加上我本身JAVA基础掌握得较为牢固，因此在几个小时之内便完成的程序的整体实现。整个程序设计过程中的难点，我觉得应该便是如何让随机生成的数据更贴合实际，在现实生活中，绩点信息和志愿填报情况往往都存在着一定的规律，比如绩点分布在2.0~3.2等中上水平的同学占绝大多数部分，而志愿填报时也经常存在热门导师，导师在选择学生的时候，也并非仅仅只参照绩点情况，还有专业兴趣技能特长等等，如何把这些因素综合考虑进去，生成一个基本贴合实际情况的数据，也是程序的一个要点。在整个过程当中，我仔细阅读其他组的数据随机生成方法，发现都没有考虑这些。因此我觉得即使算法实现得非常巧妙，但在缺乏真实合理的测试数据之下，所运行出来的结果并不具备多大的参考价值。 项目链接: 导师分配系统","link":"/categories/software-engineering/结对编程2：毕设选导系统/"},{"title":"MCMC等采样算法","text":"直接采样直接采样的思想是，通过对均匀分布采样，实现对任意分布的采样。因为均匀分布采样好猜，我们想要的分布采样不好采，那就采取一定的策略通过简单采取求复杂采样。假设y服从某项分布p(y)，其累积分布函数CDF为h(y)，有样本z~Uniform(0,1)，我们令 z = h(y)，即 y = h(z)^(-1)，结果y即为对分布p(y)的采样。 直接采样的核心思想在与CDF以及逆变换的应用。在原分布p(y)中，如果某个区域[a, b]的分布较多，然后对应在CDF曲线中，[h(a), h(b)]的曲线斜率便较大。那么，经过逆变换之后，对y轴（z）进行均匀分布采样时，分布多的部分（占据y轴部分多）对应抽样得到的概率便更大， 局限性实际中，所有采样的分布都较为复杂，CDF的求解和反函数的求解都不太可行。 拒绝采样拒绝采样是由一个易于采样的分布出发，为一个难以直接采样的分布产生采样样本的通用算法。既然 p(x) 太复杂在程序中没法直接采样，那么便一个可抽样的分布 q(x) 比如高斯分布，然后按照一定的方法拒绝某些样本，达到接近 p(x) 分布的目的。 计算步骤设定一个方便抽样的函数 q(x)，以及一个常量 k，使得 p(x) 总在 k*q(x) 的下方。（参考上图） x 轴方向：从 q(x) 分布抽样得到 a； y 轴方向：从均匀分布（0, k*q(a)) 中抽样得到 u； 如果刚好落到灰色区域： u &gt; p(a), 拒绝， 否则接受这次抽样； 重复以上过程。 计算步骤（BN） 根据网络指定的先验概率分布生成采样样本； 拒绝所有与证据不匹配的样本； 在剩余样本中对事件X=x的出现频繁程度计数从而得到估计概率、 局限性 拒绝了太多的样本！随着证据变量个数的增多，与证据e相一致的样本在所有样本中所占的比例呈指数级下降，所以对于复杂问题这种方法是完全不可用的。 难以找到合适的k*q(a)，接受概率可能会很低。 重要性采样（似然加权） Likelihood Weighting 重要性采样主要是用于求一个复杂分布p(x)的均值，最后并没有得到样本。 重要性采样的思想是借助一个易于采样的简单分布q(x)，对这个简单分布q(x)所得到的样本全部接受。但是以此得到的样本肯定不满足分布p(x)，因此需要对每一个样本附加相应的重要性权重。在重要性采样中，以p(x0)/q(x0)的值作为每个样本的权重。这样，当样本和分布p(x)相近时，对应的权重大；与分布p(x)相差过多时，对应的权重小。这个方法采样得到的是带有重要性权重的服从q(z)分布的样本，这个权重乘以样本之后的结果其实就是服从p(z)分布的。 通过上述公式，我们可以知道重要性采样可以用于近似复杂分布的均值。 吉布斯采样假设有一个例子：E：吃饭、学习、打球；时间T：上午、下午、晚上；天气W：晴朗、刮风、下雨。样本（E，T，W）满足一定的概率分布。现要对其进行采样，如：打球+下午+晴朗。 问题是我们不知道p(E,T,W)，或者说，不知道三件事的联合分布。当然，如果知道的话，就没有必要用吉布斯采样了。但是，我们知道三件事的条件分布。也就是说，p(E|T,W), p(T|E,W), p(W|E,T)。现在要做的就是通过这三个已知的条件分布，再用Gibbs sampling的方法，得到联合分布。具体方法：首先随便初始化一个组合,i.e. 学习+晚上+刮风，然后依条件概率改变其中的一个变量。具体说，假设我们知道晚上+刮风，我们给E生成一个变量，比如，学习→吃饭。我们再依条件概率改下一个变量，根据学习+刮风，把晚上变成上午。类似地，把刮风变成刮风（当然可以变成相同的变量）。这样学习+晚上+刮风→吃饭+上午+刮风。同样的方法，得到一个序列，每个单元包含三个变量，也就是一个马尔可夫链。然后跳过初始的一定数量的单元（比如100个），然后隔一定的数量取一个单元（比如隔20个取1个）。这样sample到的单元，是逼近联合分布的。 蓄水池采样蓄水池抽样（Reservoir Sampling ），即能够在o（n）时间内对n个数据进行等概率随机抽取，例如：从1000个数据中等概率随机抽取出100个。另外，如果数据集合的量特别大或者还在增长（相当于未知数据集合总量），该算法依然可以等概率抽样。 算法步骤： 先选取数据流中的前k个元素，保存在集合A中； 从第j（k + 1 &lt;= j &lt;= n）个元素开始，每次先以概率p = k/j选择是否让第j个元素留下。若j被选中，则从A中随机选择一个元素并用该元素j替换它；否则直接淘汰该元素； 重复步骤2直到结束，最后集合A中剩下的就是保证随机抽取的k个元素。 MCMC算法 随机采样和随机模拟：吉布斯采样Gibbs Sampling MCMC算法学习总结 【重点】采样方法（二）MCMC相关算法介绍及代码实现 马氏链收敛定理马氏链定理：如果一个非周期马氏链具有转移概率矩阵P,且它的任何两个状态是连通的，那么$\\lim{p\\to\\infty}P{ij}^n$存在且与i无关，记$\\lim{p\\to\\infty}P{ij}^n = \\pi(j)$，我们有： 其中$\\pi = [\\pi(1), \\pi(2), … , \\pi(j), …], \\sum_{i=0}^{\\infty}\\pi_i = 1, \\pi$称为马氏链的平稳分布。 所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的。 说明： 该定理中马氏链的状态不要求有限，可以是有无穷多个的； 定理中的“非周期“这个概念不解释，因为我们遇到的绝大多数马氏链都是非周期的； 细致平稳条件 针对一个新的分布，如何构造对应的转移矩阵？ 对于一个分布$\\pi(x)$，根据细致平稳条件，如果构造的转移矩阵P满足$\\pi(i)P{ij} = \\pi(j)P{ji}$，那么$\\pi(x)$即为该马氏链的平稳分布，因此可以根据这个条件去构造转移矩阵。 通常情况下，初始的转移矩阵$P$一般不满足细致平稳条件，因此我们通过引入接受率构造出新的转移矩阵$P’$，使其和$\\pi(x)$满足细致平稳条件。由此，我们便可以用任何转移概率矩阵（均匀分布、高斯分布）作为状态间的转移概率。 如果我们假设状态之间的转移概率是相同的，那么在算法实现时，接收率可以简单得用$\\pi(j)/\\pi(i)$表示。 Metropolis-Hastings采样对于给定的概率分布p(x),我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布， 于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x), 那么我们从任何一个初始状态x0出发沿着马氏链转移, 得到一个转移序列 x0,x1,x2,⋯xn,xn+1⋯,， 如果马氏链在第n步已经收敛了，于是我们就得到了 π(x) 的样本xn,xn+1⋯。 在马尔科夫状态链中，每一个状态代表一个样本$x_n$，即所有变量的赋值情况。 通过分析MCMC源码，可以知道：假设状态间的转移概率相同，那么下一个样本的采样会依赖于上一个样本。假设上一个样本所对应的原始分布概率$\\pi(x)$很小，那么下一个样本的接受率很大概率为1；反之如果上一个样本的原始分布概率$\\pi(x)$很大，那么下一个样本会有挺大概率被拒绝。这样的机制保证了生成的样本服从分布$\\pi(x)$。 从上述分析可以看出，假如初始状态的样本对应的分布概率很小，那么在算法刚开始运行时所产生的样本（即使是分布概率很小的样本）很大可能都会被接收，从而使得算法刚开始运行时采样的样本不满足原始分布$\\pi(x)$。只要算法采样到分布概率大的样本（此时即为收敛！），那么之后所采样得到的样本就会基本服从原始分布。当然，从初始状态遍历到分布概率大的状态时需要运行一段时间，这段过程即为收敛的过程。MCMC算法在收敛之后，保证了在分布概率$\\pi(x)$大的地方产生更多的样本，在分布概率$\\pi(x)$小的地方产生较少的样本。 一个马尔可夫链需要经过多次的状态转移过程采用达到一个稳定状态，这时候采样才比较接近真实的分布。此过程称为burn in。一般可通过丢弃前面的N个采样结果来达到burn in。 疑问 MCMC的收敛是什么意思？这个过程中是什么参数会更新导致收敛？如何确定何时收敛？ 收敛过程没有参数会更新，收敛的思想类似于大数定理。应用MCMC算法采样时，初始的样本量少，服从的分布可能和复杂分布$\\pi(x)$相差挺远，但随着状态转移数的增加（转移矩阵P的应用），根据上述定理的证明，最终的样本分布会逐渐服从复杂分布$\\pi(x)$。 $\\pi$是每个状态所对应的概率分布吗？如果是的话，初始选定一个状态后，这个$\\pi$如何设定？或则在MCMC证明过程中，初始$\\pi$的概率分布如何设置？ 在MCMC的证明过程中，$\\pi$是每个状态所对应的概率分布。证明中所给定的初始$\\pi$应该只是为了证明无论初始样本符合什么分布，在经过一定数量的转移之后，得到的样本会服从复杂分布$\\pi (x)$，在实际代码实现中，不用对这个$\\pi$进行设定。 代码1234import numpy as npimport randomimport matplotlib.pyplot as pltimport pandas as pdf Rejection Sampling12345678def f(x): if 0 &lt;= x and x &lt;= 0.25: y = 8 * x elif 0.25 &lt; x and x &lt;= 1: y = (1 - x) * 8/3 else: y = 0 return y 123456def g(x): if 0 &lt;= x and x &lt;= 1: y = 1 else: y = 0 return y 123456789def plot(fun): X = np.arange(0, 1.0, 0.01) Y = [] for x in X: Y.append(fun(x)) plt.plot(X, Y) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show() 12plot(f)plot(g) 1234567891011121314151617def rejection_sampling(N=10000): M = 3 cnt = 0 samples = {} while cnt &lt; N: x = random.random() acc_rate = f(x) / (M * g(x)) u = random.random() if acc_rate &gt;= u: if samples.get(x) == None: samples[x] = 1 else: samples[x] = samples[x] + 1 cnt = cnt + 1 return samples 1s = rejection_sampling(100000) 12345X = []Y = []for k, v in s.items(): X.append(k) Y.append(v) 1plt.hist(X, bins=100, edgecolor='None') MCMC SamplingMetropolis-Hastings Algorithm 参考：MCMC相关算法介绍及代码实现 123456789PI = 3.1415926def get_p(x): # 模拟pi函数 return 1/(2*PI)*np.exp(- x[0]**2 - x[1]**2)def get_tilde_p(x): # 模拟不知道怎么计算Z的PI，20这个值对于外部采样算法来说是未知的，对外只暴露这个函数结果 return get_p(x) 12def domain_random(): #计算定义域一个随机值 return np.random.random()*3.8-1.9 123456789def metropolis(x): new_x = (domain_random(),domain_random()) #新状态 #计算接收概率 acc = min(1,get_tilde_p((new_x[0],new_x[1]))/get_tilde_p((x[0],x[1]))) #使用一个随机数判断是否接受 u = np.random.random() if u&lt;acc: return new_x return x 12345678910111213141516def testMetropolis(counts = 100,drawPath = False): plt.figure() #主要逻辑 x = (domain_random(),domain_random()) #x0 xs = [x] #采样状态序列 for i in range(counts): xs.append(x) x = metropolis(x) #采样并判断是否接受 #在各个状态之间绘制跳转的线条帮助可视化 X1 = [x[0] for x in xs] X2 = [x[1] for x in xs] if drawPath: plt.plot(X1, X2, 'k-',linewidth=0.5) ##绘制采样的点 plt.scatter(X1, X2, c = 'g',marker='.') plt.show() 1testMetropolis(5000) 123456789def metropolis(x): new_x = domain_random() #计算接收概率 acc = min(1,f(new_x)/f(x)) #使用一个随机数判断是否接受 u = np.random.random() if u&lt;acc: return new_x return x 123456789101112def testMetropolis(counts = 100,drawPath = False): plt.figure() #主要逻辑 x = domain_random() xs = [x] #采样状态序列 for i in range(counts): xs.append(x) x = metropolis(x) #采样并判断是否接受 #在各个状态之间绘制跳转的线条帮助可视化 plt.hist(xs, bins=100, edgecolor='None') # plt.plot(xs) plt.show() 1testMetropolis(100000) Gibbs Sampling1234567891011121314151617def partialSampler(x,dim): xes = [] for t in range(10): #随机选择10个点 xes.append(domain_random()) tilde_ps = [] for t in range(10): #计算这10个点的未归一化的概率密度值 tmpx = x[:] tmpx[dim] = xes[t] tilde_ps.append(get_tilde_p(tmpx)) #在这10个点上进行归一化操作，然后按照概率进行选择。 norm_tilde_ps = np.asarray(tilde_ps)/sum(tilde_ps) u = np.random.random() sums = 0.0 for t in range(10): sums += norm_tilde_ps[t] if sums&gt;=u: return xes[t] 123456789def gibbs(x): rst = np.asarray(x)[:] path = [(x[0],x[1])] for dim in range(2): #维度轮询，这里加入随机也是可以的。 new_value = partialSampler(rst,dim) rst[dim] = new_value path.append([rst[0],rst[1]]) #这里最终只画出一轮轮询之后的点，但会把路径都画出来 return rst,path 1234567891011121314151617181920def testGibbs(counts = 100,drawPath = False): plt.figure() x = (domain_random(),domain_random()) xs = [x] paths = [x] for i in range(counts): xs.append([x[0],x[1]]) x,path = gibbs(x) paths.extend(path) #存储路径 p1 = [x[0] for x in paths] p2 = [x[1] for x in paths] xs1 = [x[0] for x in xs] xs2 = [x[1] for x in xs] if drawPath: plt.plot(p1, p2, 'k-',linewidth=0.5) ##绘制采样的点 plt.scatter(xs1, xs2, c = 'g',marker='.') plt.show() 1testGibbs(5000)","link":"/categories/study/MCMC等采样算法/"},{"title":"深度人脸识别综述","text":"Introduction人脸识别系统通常由以下构建模块组成： 人脸检测。人脸检测器用于寻找图像中人脸的位置，如果有人脸，就返回包含每张人脸的边界框的坐标。如图 3a 所示。 人脸对齐。人脸对齐的目标是使用一组位于图像中固定位置的参考点来缩放和裁剪人脸图像。这个过程通常需要使用一个特征点检测器来寻找一组人脸特征点，在简单的 2D 对齐情况中，即为寻找最适合参考点的最佳仿射变换。图 3b 和 3c 展示了两张使用了同一组参考点对齐后的人脸图像。更复杂的 3D 对齐算法（如 [16]）还能实现人脸正面化，即将人脸的姿势调整到正面向前。 人脸表征。在人脸表征阶段，人脸图像的像素值会被转换成紧凑且可判别的特征向量，这也被称为模板（template）。理想情况下，同一个主体的所有人脸都应该映射到相似的特征向量。 人脸匹配。在人脸匹配构建模块中，两个模板会进行比较，从而得到一个相似度分数，该分数给出了两者属于同一个主体的可能性。 人脸识别相关方法的发展如下图所示： 对于基于 CNN 的人脸识别方法，影响准确度的因素主要有三个：训练数据、CNN 架构和损失函数。 因为在大多数深度学习应用中，都需要大训练集来防止过拟合。一般而言，为分类任务训练的 CNN 的准确度会随每类的样本数量的增长而提升。这是因为当类内差异更多时，CNN 模型能够学习到更稳健的特征。但是，对于人脸识别，我们感兴趣的是提取出能够泛化到训练集中未曾出现过的主体上的特征。因此，用于人脸识别的数据集还需要包含大量主体，这样模型也能学习到更多类间差异。 人脸识别实验中常用到close-set和open-set： close-set：就是所有的测试集都在训练集中出现过。所以预测结果是图片的ID，如果想要测试两张图片是否是同一个，那么就看这两张图片的预测ID是否一样即可。 open-set：就是测试的图片并没有在训练集中出现过，那么每张测试图片的预测结果是特征向量，如果想要比较两张图片的人脸是否属于同一个人，需要测试图像特征向量的距离。 在光照较差，遮挡，形变（大笑），侧脸等诸多条件下，神经网络很难提取出与“标准脸”相似的特征，异常脸在特征空间里落到错误的位置，导致识别和验证失败。这是现代人脸识别系统的局限，一定程度上也是深度学习（深度神经网络）的局限。 面对这种局限，通常采取三种应对措施，使人脸识别系统能正常运作： 1. 工程角度：研发质量模型，对检测到人脸质量进行评价，质量较差则不识别/检验。 2. 应用角度：施加场景限制，比如刷脸解锁，人脸闸机，会场签到时，都要求用户在良好的光照条件下正对摄像头，以避免采集到质量差的图片。 3. 算法角度：提升人脸识别模型性能，在训练数据里添加更多复杂场景和质量的照片，以增强模型的抗干扰能力。 Euclidean Based Metric LearningContrastive loss DeepID1并不属于contrastive loss系列，只是作为DeepID系列的开山之作而放在这里。 DeepID1:《Deep Learning Face Representation from Predicting 10,000 Classes》 DeepID2:《Deep learning face representation by joint identification-verification》 DeepID2+:《Deeply learned face representations are sparse, selective, and robust》 DeepID3:《Deepid3: Face recognition with very deep neural networks》 \\operatorname{Ident}\\left(f, t, \\theta_{i d}\\right)=-\\sum_{i=1}^{n} p_{i} \\log \\hat{p}_{i}=-\\log \\hat{p}_{t} \\operatorname{Verif}\\left(f_{i}, f_{j}, y_{i j}, \\theta_{v e}\\right)=\\left\\{\\begin{array}{ll}{\\frac{1}{2}\\left\\|f_{i}-f_{j}\\right\\|_{2}^{2}} & {\\text { if } y_{i j}=1} \\\\ {\\frac{1}{2} \\max \\left(0, m-\\left\\|f_{i}-f_{j}\\right\\|_{2}\\right)^{2}} & {\\text { if } y_{i j}=-1}\\end{array}\\right.DeepID1: DeepID2: DeepID2+: DeepID3: Triplet Loss 《Facenet: A unified embedding for face recognition and clustering》 \\sum_{i}^{N}\\left[\\left\\|f\\left(x_{i}^{a}\\right)-f\\left(x_{i}^{p}\\right)\\right\\|_{2}^{2}-\\left\\|f\\left(x_{i}^{a}\\right)-f\\left(x_{i}^{n}\\right)\\right\\|_{2}^{2}+\\alpha\\right] Center Loss 《A Discriminative Feature Learning Approach for Deep Face Recognition》 \\begin{aligned} \\mathcal{L} &=\\mathcal{L}_{S}+\\lambda \\mathcal{L}_{C} \\\\ &=-\\sum_{i=1}^{m} \\log \\frac{e^{W_{y_{i}}^{T} \\boldsymbol{x}_{i}+b_{y_{i}}}}{\\sum_{j=1}^{n} e^{W_{j}^{T} \\boldsymbol{x}_{i}+b_{j}}}+\\frac{\\lambda}{2} \\sum_{i=1}^{m}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{c}_{y_{i}}\\right\\|_{2}^{2} \\end{aligned}$c_{y_i}$代表样本$i$对应类别$y_i$所属的类中心。理想情况下，这个类中心在每次迭代时都需要利用整个数据集的特征来更新，但是这会需要巨大的计算量。因此，在实际使用中，做了如下修改： 由整个训练集更新center改为mini-batch更新center； 避免错误分类的样本的干扰，使用scalar α 来控制center的学习率； 虽然center loss取得了良好的结果，但是也有一些不足之处。Center Loss考虑到了使得类内紧凑，却不能使类间可分；另外训练样本对的选择也较为麻烦。在论文中，作者也提到了，选取合适的样本对对于模型的性能至关重要，论文中采用的方法是每次选择比较难以分类的样本对重新训练，类似于hard-mining。同时，合适的训练样本还可以加快收敛速度。 Center-Invariant Loss 《Deep face recognition with center invariant loss》 \\begin{aligned} L=& L_{s}+\\gamma L_{I}+\\lambda L_{c} \\\\=&-\\log \\left(\\frac{e^{\\mathbf{w}_{y}^{T} \\mathbf{x}_{i}+b_{y}}}{\\sum_{j=1}^{m} e^{\\mathbf{w}_{j}^{T} \\mathbf{x}_{i}+b_{j}}}\\right)+\\frac{\\gamma}{4}\\left(\\left\\|\\mathbf{c}_{y}\\right\\|_{2}^{2}-\\frac{1}{m} \\sum_{k=1}^{m}\\left\\|\\mathbf{c}_{k}\\right\\|_{2}^{2}\\right)^{2} \\\\ &+\\frac{\\lambda}{2}\\left\\|\\mathbf{x}_{i}-\\mathbf{c}_{y}\\right\\|^{2} \\end{aligned} Range Loss 《Range loss for deep face recognition with long-tail》 \\mathcal{L}_{R}=\\alpha \\mathcal{L}_{R_{i n t r a}}+\\beta \\mathcal{L}_{R_{i n t e r}} \\mathcal{L}_{R_{i n t r a}}=\\sum_{i \\subseteq I} \\mathcal{L}_{R_{i n t r a}}^{i}=\\sum_{i \\subseteq I} \\frac{k}{\\sum_{j=1}^{k} \\frac{1}{\\mathcal{D}_{j}}} \\begin{aligned} \\mathcal{L}_{R_{\\text { inter }}} &=\\max \\left(m-\\mathcal{D}_{\\text {Center}}, 0\\right) \\\\ &=\\max \\left(m-\\left\\|\\overline{x}_{Q}-\\overline{x}_{\\mathcal{R}}\\right\\|_{2}^{2}, 0\\right) \\end{aligned} \\mathcal{L}=\\mathcal{L}_{M}+\\lambda \\mathcal{L}_{R}=-\\sum_{i=1}^{M} \\log \\frac{e^{W_{y_{i}}^{T} x_{i}+b_{v_{i}}}}{\\sum_{j=1}^{n} e^{W_{j}^{T} x_{i}+b_{j}}}+\\lambda \\mathcal{L}_{R} Summary基于欧式距离的Metric Learning符合人的认知规律，在实际应用中也取得了不错的效果，但是它有非常致命的两个问题： 模型需要很长时间才能拟合，contrastive loss和triplet loss的训练样本都基于pair或者triplet的，可能的样本数是O(N2)或者O(N3)的。当训练集很大时，基本不可能遍历到所有可能的样本，所以一般来说需要很长时间才能拟合； 模型好坏很依赖训练数据的sample方式，理想的sample方式不仅能提升算法最后的性能，更能略微加快训练速度。 Margin Based ClassificationSoftmax Loss L=\\frac{1}{N} \\sum_{i} L_{i}=\\frac{1}{N} \\sum_{i}-\\log \\left(\\frac{e^{f_{y_{i}}}}{\\sum_{j} e^{f_{j}}}\\right) L_{i}=-\\log \\left(\\frac{e^{\\left\\|\\mathbf{W}_{y_{i}}\\right\\|\\left\\|\\mathbf{x}_{i}\\right\\| \\cos \\left(\\theta_{y_{i}}\\right)}}{\\sum_{j} e^{\\left\\|\\mathbf{W}_{j}\\right\\|\\left\\|\\mathbf{x}_{\\mathbf{i}}\\right\\| \\cos \\left(\\theta_{j}\\right)})}\\right)这里的W可以理解为一组基向量，每一个维度对应一个类别。$W_iX_i$即将数据$X_i$映射到类别$i$对应的维度上，$X_i$与基向量$W_i$的夹角越小，则数据$X_i$属于类别$i$的概率越大。因此，对于一个二分类问题而言，只要$X_i$与其中一个基向量$W_i$的夹角小于45°，则$X_i$即被分类为类别$i$。 从上述分析可以看出，基于Softmax的分类只要求能够得到正确的分类结果，但是这种形式并不能够有效地学习得到使得类内较为紧凑、类间较离散的特征。 L-Softmax Loss 《Large-margin softmax loss for convolutional neural networks》 L_{i}=-\\log \\left(\\frac{e^{\\left\\|\\mathbf{W}_{y_{i}}\\right\\|\\left\\|\\mathbf{x}_{i}\\right\\| \\psi\\left(\\theta_{y_{i}}\\right)}}{\\left\\|\\mathbf{W}_{y_{i}}\\right\\|\\left\\|\\mathbf{x}_{i}\\right\\| \\psi\\left(\\theta_{y_{i}}\\right)+\\sum_{j \\neq y_{i}} e^{\\left\\|\\mathbf{W}_{j}\\right\\|\\left\\|\\mathbf{x}_{i}\\right\\| \\cos \\left(\\theta_{j}\\right)})}\\right)其中，$\\psi(\\theta)$可以表示为： \\psi(\\theta)=\\left\\{\\begin{array}{l}{\\cos (m \\theta), 0 \\leq \\theta \\leq \\frac{\\pi}{m}} \\\\ {\\mathcal{D}(\\theta), \\frac{\\pi}{m}","link":"/categories/computer-vision/deep-face-recognition/"},{"title":"Zero-shot Learning","text":"介绍在传统的分类模型中，为了解决多分类问题（例如三个类别：猫、狗和猪），就需要提供大量的猫、狗和猪的图片用以模型训练，然后给定一张新的图片，就能判定属于猫、狗或猪的其中哪一类。但是对于之前训练图片未出现的类别（例如牛），这个模型便无法将牛识别出来，而ZSL就是为了解决这种问题。在ZSL中，某一类别在训练样本中未出现，但是我们知道这个类别的特征，然后通过语料知识库，便可以将这个类别识别出来。 zero-shot learning的一个重要理论基础就是利用高维语义特征代替样本的低维特征，使得训练出来的模型具有迁移性。语义向量就是高维语义特征，比如一个物体的高维语义为“四条腿，有尾巴，会汪汪叫，宠物的一种”，那我们就可以判断它是狗，高维语义对它没有细节描述，但是能够很好的对其分类，分类是我们的目的，所以可以舍去低维特征，不需要“全面”。 DAP模型 《Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer》 DAP可以理解为一个三层模型：第一层是原始输入层，例如一张电子图片（可以用像素的方式进行描述）；第二层是p维特征空间，每一维代表一个特征（例如是否有尾巴、是否有毛等等）；第三层是输出层，输出模型对输出样本的类别判断。在第一层和第二层中间，训练p个分类器，用于对一张图片判断是否符合p维特征空间各个维度所对应的特征；在第二层和第三层间，有一个语料知识库，用于保存p维特征空间和输出y的对应关系，这个语料知识库是事先人为设定的（暂时理解是这样？）。 假设我们已经训练好了一个DAP模型，第一层和第二层间的分类器可以判断 是否黑眼圈、是否喜欢吃竹子 之类的特征，然后在语料知识库里面包含一个映射：黑眼圈 喜欢吃竹子—&gt; 熊猫，那么即使我们的模型在训练时没有见过熊猫的图片，在遇到熊猫的图片时，我们可以直接通过对图片的特征进行分析，然后结合知识语料库判断出这张图片是熊猫。假设即使语料知识库里面不包含 黑眼圈 喜欢吃竹子—&gt; 熊猫 的映射，我们也可以通过计算熊猫图片的特征与其他训练样本的特征的汉明距离度量，得到熊猫和什么动物比较类似的信息。整个DAP的运作思想就是类似于上述过程。 缺点 算法引入了中间层，核心在于尽可能得判定好每幅图像所对应的特征，而不是直接去预测出类别；因此DAP模型在判定属性时可能会做得很好，但是在预测类别时却不一定； 无法利用新的样本逐步改善分类器的功能； 无法利用额外的属性信息（如Wordnet等) ALE模型 《Label-Embedding for Attribute-Based Classification》 概要在分类问题中，每个类别被映射到属性空间中，即每个类别可用一个属性向量来表示（例如：熊猫—&gt;（黑眼圈，爱吃竹子，猫科动物……））。ALE模型即学习一个函数F，该函数用于衡量每一幅图像和每个属性向量之间的匹配度. ALE模型确保对于每幅图像，和分类正确的类别的相容性比和其他类别的匹配度高。通过在AWA和CUB数据集上的实验表明，ALE模型比DAP等模型在ZSL问题上的表现更好，而且ALE可以利用额外的类别信息来提高模型表现，以及可以从零样本学习迁移到其他拥有大量数据的学习问题中。 在CV领域，大部分的工作都集中与如何从一幅图像里抽取出合适的特征信息（即上图左侧），而在ALE中，研究的重心在于如何把一个类别映射到合适的欧几里得属性空间。 模型推导定义f(x; w）为预测函数，定义F(x, y; w)为输入x和类别y之间的匹配度。则当给定一个需要预测类别的数据x时，预测函数f所做的便是从所有类别y中，找到一个类别y使得F(x, y; w)的值最大。 对于模型的参数w的求解过程为：对于所有样本（x, y），尽可能得最大化 $\\frac{1}{n} \\sum_{n=1}^{N}{F(x,y;\\omega)}$ ，但是对于这个目标函数，在图像分类问题中无法直接优化得出我们的最终目标（具体为什么我也不知道。。。）因此作者从WSABIE算法中得到灵感，并借此来实现ALE算法。 Zero-shot learning： 文中借鉴WSABIE算法，得到我们的目标函数为：$\\frac {1}{N} \\sum{n=1}^{N} \\max{y∈Y}l(x_n, y_n, y)$.该目标函数和SSVM很类似，对此解释如下： 对于每一个样本，计算对应每个类别的得分。然后从其他所有不是正确类别的得分中找出最大的得分； 逐样本累加后即得到损失函数的值，然后利用SGD等方法对参数进行更新即可； 算法的核心思想和SVM很像，即让错误分类的得分得尽可能得比正确分类的得分小。 Few-shots learning： 在上述的Zero-shot learning下对应的模型中，每个类别通过映射$\\phi(y)$得到语义空间的值是实现通过先验信息固定的，但是在使用模型预测的过程中，可能会逐步遇到之前训练样本中不存在对应类别的数据，那么ALE就具有能逐步利用新的训练样本来改善模型的作用。在此问题下，模型的目标函数变为： \\frac {1}{N} \\sum_{n=1}^{N} \\max_{y∈Y}l(x_n, y_n, y) + \\frac{\\mu }{2}\\begin{Vmatrix} \\Phi - \\Phi ^{\\Lambda }\\end{Vmatrix}^{2}在上述公式中，参数$\\Phi$为在一定维度随机初始化的参数。在使用SGD等方法进行参数更新的时候，为使该损失函数的值尽可能得小，显然$\\Phi$要尽可能得接近$\\Phi^{\\Lambda}$，同时也利用了训练样本中存在的部分信息。从而使得AEL模型达到可以逐步利用新的训练样本（之前的训练样本中不存在的类别）的信息来改善模型。 ALE模型如何利用额外的属性等信息来源？ DAP模型针对每一个属性训练一个分类器，再从属性向量空间里面找到和测试样本最接近的类别。然后ALE并没有特别针对语义向量空间的每一个维度进行学习，ALE直接学习了从特征空间到语义向量空间的映射。其中$\\theta(x)$表示从图像得到的特征，$\\varphi (y)$则是从类别到语义向量空间的映射。 因为$\\varphi (y)$是独立于训练数据的，因此可以根据需要变换为其他种类的先验信息，例如HLE模型，而在DAP模型中，因为模型限定了只能对单一属性进行训练，因为就无法利用其他种类的先验信息了。 模型如何对ZS类进行预测ALE模型利用属性等语义信息，将每个类别标签嵌入到维度为85的语义空间内（假设使用85个属性描述类别）。ALE模型所做的就是学习到一个从特征空间到语义空间的映射，该映射确保得到语义空间内的点与正确类别的语义向量更近，尽可能得与错误类别的语义向量更远。这样，在对ZS进行预测时，映射所得到的在语义空间上的点与在训练时出现过的那些类别所对应点会很远，从而使得与正确的ZS类对应的语义向量更近，从而进行预测。 DAP与ALE针对每个测试样本，DAP利用分类器得到一个属性向量，然后直接从预定义好的属性空间里找到一个与该属性向量最近的类别作为预测结果输出。而ALE在训练时利用映射函数衡量每个输入与语义向量之间的得分，损失函数的训练过程确保正确类别的得分都比错误类别的得分高出一定值（类似SVM），因此ALE的训练过程是直接以类别预测为指导的。 DAP的模型结构其实和ALE模型的结构大体上是一致的，甚至连映射矩阵、语义空间的矩阵维度都是相同的，但是区别在于两者的监督方式不同。DAP针对是在属性上进行 监督学习的，针对每个属性维度学习一个分类器；而ALE是针对类别进行监督学习的，从而导致映射层的参数不同。 DAP的监督方式没有学习到属性之间的相关性，而ALE训练之后确保类别相似的在语义空间内都具有类似的分布，且类别相似大都具有相同的特征，导致了具有不同特征的类别在语义空间内具有不同的分布，从而学习到属性间的相关性。 SAE模型 《Semantic Autoencoder for Zero-Shot learning》 概要传统的ZSL问题，经常遇到映射领域漂移问题，在SAE模型中，为了解决这个问题，要求输入x经过变换生成的属性层S，拥有恢复到原来输入层x的功能，并且添加了映射层必须具有具体的语义意思的限制，通过加入这个限制，确保了映射函数必须尽可能得保留原输入层的所有信息。 映射领域漂移（Projection domain shift） 对于zero-shot learning问题，由于训练模型时，对于测试数据类别是不可见的，因此，当训练集和测试集的类别相差很大的时候，比如一个里面全是动物，另一个全是家具，在这种情况下，传统zero-shot learning的效果将受到很大的影响。 为什么SAE模型可以解决映射领域漂移问题? 介绍如今CV的研究方向已经逐步朝着大规模的分类问题发展了，但是由于在大数据集（如ImageNet）的21814个类别里，有296个类别的图像仅仅只有一个图像数据，因此可扩展性还是一个严峻的问题。 SAE模型的输入特征空间是通过GoogleNet或则AlexNet从图像数据里提取出来的特征，而输出语义空间是头ing过Skip-gram在Wikipedia训练得到的word2vec向量。 问题 SAE模型在预测的时候，是从unseen classes的语义空间向量里找到一个和当前测试样本距离最近的作为预测结果。那么在实际应用中，当一个新的样本进来，我们怎么确定这个样本是属于seen classes还是unseen classes的语义空间里进行预测呢？ SCoRE 《Semantically Consistent Regularization for Zero-Shot Recognition》 Deep-RIS要学习样本X到属性向量S的映射W，设训练集： \\mathcal{D}=\\left\\{\\left(\\mathbf{x}^{(i)}, \\mathbf{s}^{(\\hat{i})}\\right)_{i=1}^{N}\\right\\}其中x为输入样本，y为样本标签；属性集合为： \\mathbf{s}^{(i)}=\\left(s_{1}^{(i)}, \\ldots, s_{Q}^{(i)}\\right)对于Q个属性，为每个属性建立一个CNN，作为属性分类器： a_{k}\\left(\\mathbf{x} ; \\mathbf{t}_{k}, \\Theta\\right)=\\sigma\\left(\\mathbf{t}_{k}^{T} \\theta(\\mathbf{x} ; \\Theta)\\right)其中，$\\sigma$为sigmoid函数。损失函数可以定义为交叉熵损失。 Deep-RULE假设有Q个属性，用二值表示；有C个类别，则可以将每个类别转换为one-hot的形式： \\phi_{k}(y)=\\left\\{\\begin{array}{ll}{1} & {\\text { if class } y \\text { contains attribute } k} \\\\ {-1} & {\\text { if class } y \\text { lacks attribute } k}\\end{array}\\right.即为类别y的语义编码，则分类器可以定义为： h(\\mathbf{x} ; \\mathbf{T}, \\Theta)=\\Phi^{T} a(\\mathbf{x})=\\Phi^{T} \\mathbf{T}^{T} \\theta(\\mathbf{x} ; \\Theta)其中$ \\theta(x ; \\Theta) $表示输入图片的特征表示；T为一个映射矩阵，$\\Theta^{T}$表示类别的one-hot表示。若使用神经网络结构实现上述目标函数，只需要先用CNN提取图片的特征，通过一个全连接层将特征映射到语义空间中（也就是学习映射矩阵T），输出时为类别y的形式即可。损失函数直接采用网络输出和真实样本之间的交叉熵损失即可。 Deep-RIS和Deep-RULE之间的关系可以从两者的损失函数，来说明它们之间的关系。Deep-RIS对每一个单独的属性ak(x)进行了监督学习，这是一种很强的约束，它把属性向量的表达限制在一个固定的范围内；反观Deep-RULE，它建立了一个语义空间SV，将样本和类别属性化之后，投影到该语义空间中。这样做的结果是：属性向量之间有了相关关系，使得属性向量的表达更加丰富；但同时，会产生冗余空间，这个冗余空间的问题在于，它会加重semantic domain shift的问题。更详细的解释是：设训练集中样本的属性向量所构成的语义空间XV的往往要小于SV，令NV=SV-XV，如果在测试集中，测试样本的属性向量处于NV中，则由训练样本训练出来的分类器将无法对该测试样本进行分类。在Deep-RIS中，对于样本到属性向量之间的映射有着非常强的约束，使得XV和SV相近。在同样的属性定义的前提下，由于NV的存在，使得Deep-RULE更有可能无法处理测试集中的类别。 论述：但同时，会产生冗余空间，这个冗余空间的问题在于，它会加重semantic domain shift的问题 解释：假设有a b c d四个特征，训练集的类别A和B分别具有ab和ac特征，即使测试集的类别是bc，也能进行预测，因为训练时已经都对abc的语义空间进行了监督学习，但是如果测试集包含d特征，这是类别A和B都不具有的特征（领域漂移问题），这就会使得模型无法正常得进行预测。 通过上面的论述其实可以知道，如果NV很小，几乎没有测试集类别的属性向量位于其中，则Deep-RULE的方法会取得更好的效果；如果NV很大，测试集中很多类别的属性向量位于其中，则Deep-RIS的方法可能会更好。 通过对上述两类方法的分析，可以发现，两种方法存在互补性。如果能够将Deep-RIS对于单个属性的约束融入到Deep-RULE中，就可以一定程度上使得NV变小，从而得到更好的结果。 以DAP和ALE两种典型的模型为例，当采用属性作为语义空间的表达时，两者属性空间的构成基本是一致的，都是由一系列的属性构成（例如有尾巴、会飞等）。最核心的不同在于DAP针对每一个属性监督学习一个分类器（如SVM），因此训练样本所包含的属性空间信息往往和整个属性空间差别很小。DAP对于每一个测试类别，输出一个维度为D（属性的个数）的向量。且DAP没有学习到属性之间的依赖关系，比如“水中动物”和“有翅膀”这两个属性就有着很强的负相关，则表明了我们可以利用更少的属性维度去表达和之前一样的信息。 而ALE直接从输入的特征空间学习一个到属性空间的映射，在训练过程中采用rank函数——确保所有正确类别的得分都比错误类别高。模型的输出是一个维度为C（类别个数）的变量，表示测试数据在每一个类别上的得分情况。因此ALE是从类别C的维度去进行监督学习的。在ALE训练完成后，训练样本里存在的类别在测试时对应的得分也会最大，但是对于训练样本中不存在的类别（ZS类），因为在训练时没有数据可以用于ZS类的监督学习，所有在测试时ZS类的得分表现就没有那么好。 因为属性空间是利用先验知识对所有类别映射而成的，因此存在一些属性只有在ZS类别里存在，而在训练类别里不存在。 以googlenet、CUB、attribute为例。pool5的输出是1024维，相当于visual feature；然后接一个312维的fc1，（fc1参数shape为1024 312，相当于公式中的T），fc1的输出fc_sem，用来做属性预测，也就是RIS; fc_sem再接一个150维的 fc2，(fc2参数shape为312150，相当于公式中的W），fc2的输出fc_obj，用来做类别预测，也就是RULE。fc2的参数，才是wc，用来跟事先定义好的codeword做L2约束。如果wc，直接使用事先定义好的codeword，W不学习，只学习T，那么这个流程将会是是经典的套路：visual space投影到semantic space，然后投影后的visual feature和semantic feature，使用dot product度量类别之间的相似性。根据Train set上的类别信息，使用分类loss，学习T。这篇工作相当于把用于dot product的codeword，也改成可以学习的了，放松了约束，可以看成是一种正则，用于增加模型在测试集上的泛化能力。 问题 Abstract: The latter addresses this issue but leaves part of the semantic space unsupervised. ALE和DAP中两个参数W之间的区别？ 迁移学习参考 cs231n-(9)迁移学习和Fine-tune网络 迁移学习与fine-tuning有什么区别？ 什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？ GMIS 2017大会杨强演讲：迁移学习的挑战和六大突破点 目前ZSL还是不能摆脱对其他模态信息的依赖：比如标注的属性，或者用wordvec去提语义特征，多数做法是将视觉特征嵌入到其他模态空间，或者将多个模态特征映射到一个公共latent空间，利用最近邻思想实现对未见类的分类，本质上也是一种知识迁移。 Transfer Learning涉及的范围就很大了，最近我也在看，涉及的细分领域比如Domain Adaptation等等，许多Transfer Learning中的技术也用于提高ZSL的性能，比如将Self-taughting Learning，Self-Paced Learning的思想可以用到Transductive ZSL中提高ZSL的算法性能。","link":"/categories/computer-vision/Zero-shot Learning/"},{"title":"《计算智能》笔记","text":"最优化算法广度优先搜索总是在某一深度上先搜索所有节点，之后搜索下一个深度的节点，能保证一定可以得到最优解，但需要生成大量节点，并且有可能导致组合爆炸，搜索效率低。 深度优先搜索总是扩展深度大的节点，直到找到目标节点。在存储空间上渐进最优，但是对于一棵无穷树，可能永远找不到最优解。搜索效率从一定程度上取决于运气，总体来说不高。 启发式搜索通过合适的启发函数f(n)=g(n)+h(n),h(n)尽量取下界最大值，可以通过生成比较少的节点求得最佳路径，搜索效率高。 alpha​-beta剪枝优点：节约了机器开销，减少搜索范围，提高了搜索效率。 缺点：严重依赖于算法的寻找顺序。 模拟退火算法优点： 具有摆脱局部最优解的能力，能够找到目标函数的全局最小点，已被证明有渐进收敛性； 简单、通用、易实现； 具有并行性。 缺点： 对参数（如初始温度）的依赖性较强； 优化过程长，效率不高。 流程图： 蚁群算法优点： 在求解性能上，具有很强的鲁棒性和搜索较优解的能力； 蚁群算法是一种基于种群的进化算法，具有并行性； 搜索过程采用分布式计算方式，大大提高了算法的计算能力和运行效率。 蚁群算法很容易与多种启发式算法结合，以解决陷入局部最优的问题。 缺点： 蚁群算法中初始信息素匮乏； 收敛速度慢、易陷入局部最优； 蚁群算法复杂度高，需要的搜索时间较长。 流程图： 遗传算法优点： 算法独立于求解域，具有快速随机的搜索能力； 群体搜索算法，具有潜在的并行性，鲁棒性高； 搜索使用评价函数启发，过程简单； 适合于求解复杂的优化问题。 缺点： 编程实现较复杂，需要对问题进行编码和解码； 算法参数多，会影响解的品质，而目前参数的选择基本是依靠经验； 容易产生早熟收敛的问题，易于陷入局部最优解； 算法对初始种群的选择有一定的依赖性，能够结合一些启发算法进行改进。 流程图： 粒子群算法优点： 具有相当快的逼近最优解的速度； 个体充分利用自身经验和群体经验调整自身的最优解； 无集中约束控制，具有很强的鲁棒性。 缺点： 数学基础薄弱，不能严格证明它在全局最优点上的收敛性； 容易产生早熟收敛，陷入局部最优，主要归咎于种群在搜索空间中多样性的丢失； 由于缺乏精密搜索方法的配合 ，最优往往得不到最优解。 流程图： 时序逻辑推理滤波 P(X_t|Y_{1:t})即根据现在及以前的所有测量数据，估计当前的状态。在雨伞那个例子中，根据目前为止过去进屋的人携带雨伞的所有观察数据，计算今天下雨的概率，这就是滤波。 预测 P(X_{t+k}|Y_{1:t}), k> 0即根据现在及现在以前的所有测量数据，估计未来某个时刻的状态。在雨伞的例子中，根据目前为止过去进屋的人携带雨伞的所有观察数据，计算从今天开始若干天后下雨的概率，这就是预测。 平滑 P(X_k|Y_{1:t}), 0 < k < t即根据现在及现在以前的所有测量数据，估计过去某个时刻的状态。在雨伞的例子中，意味着给定目前为止过去进屋的人携带雨伞的所有观察数据，计算过去某一天的下雨概率。 最可能解释 arg \\max_{x_{1:t}}P(X_{1:t}|Y_{1:t}))即给出现在及现在以前的所有测量数据，找到最能最可能生成这些测量数据的状态序列。例如，如果前三天每天都出现雨伞，但第四天没有出现，最有可能的解释就是前三天下雨了，而第四天没下雨。最可能解释也被称为解码问题，在语音识别、机器翻译等方面比较有用，最典型的方法是隐马尔可夫模型。 评估 P(Y_{1:t}|X_{1:t}, \\lambda)这里面的$\\lambda$是指模型。这个公式意味着在该模型下，给定到目前为止的状态序列，计算输出特定观测序列的可能性。这其实是个评估问题，可以评估模型的好坏，概率越高，意味着模型越能反映观测序列与状态序列之间的联系，模型就越好。 学习学习$\\lambda $，也即状态转移概率和观测概率 P(X_{t+1}|X_{t}), P(Y_t|X_t)学习的目的是根据历史数据得到合理的模型，一般是根据一个目标函数，对模型进行迭代更新，例如使（5）中要计算的值最大便可以作为一个目标。 前向递归算法 应用于滤波问题。 前向-后向算法 应用于平滑问题。 维特比算法 m_{t+1} = P(e_{t+1}|X_{t+1})\\max_{X_t}(P(X_{t+1}|X_t) m_{1:t})应用于最大可能解释问题。 精确推理Noisy-OR模型对于其中一个变量依赖于 k 个父节点的噪声逻辑关系，可以用 O(k)而不是 O(2k)个参数来描述其完全条件概率表。 变量消元算法变量消元算法保存了中间计算的结果，避免了重复计算，大大提高了计算效率。 动态贝叶斯网络近似推理（离散型）离散型的近似推理，即在贝叶斯网络下的近似推理。 各种采样算法的一致性估计。 直接采样直接采样可用于计算联合概率分布。 算法流程为： 拒绝采样拒绝采样可用于计算条件概率。先通过直接采样生成一定量的样本，然后拒绝掉与证据变量不一致的样本，再计算剩余样本中查询变量所占的比例。 算法流程为： 似然加权 Likelihood Weighting 回顾拒绝采样的算法过程，一个疑问就是为什么不在采样过程中就指定证据变量的值呢？这便是似然加权（重要性采样）所解决的问题。在似然加权采样过程中，如果变量位于证据变量的子节点，那么就在指定证据变量的值的概率下进行采样；如果变量是证据变量的父节点，由于在采样过程中无法实现知道证据变量的值，但是证据变量的值对父节点的概率分布是有影响的，因此我们在计算到证据节点时，需要将父节点采样的值对应的概率分布加入权重中。如果证据节点的条件概率高，说明父节点的取值是大概率的，反之则是小概率的。 算法流程为： 粒子滤波动态贝叶斯网络和马尔可夫模型可以相互转移。当给定证据变量想要计算隐含变量的概率时，我们可以通过“无限”复制时间片到动态贝叶斯网络中，然后再采用维特比算法等精确推理的方法去计算条件概率。但是，当网络的状态变量非常多的时候，即使维特比算法等精确推理的方法可以达到“常数级”的计算效率，但计算效率依旧不尽人意。 在复杂的动态贝叶斯网络下，可以采用似然加权的方法来近似得到条件概率。在传统的似然加权采样中，每次只产生一个样本，最后通过统计符合证据的样本频率来近似得到概率。替代地，我们可以一次性处理N个样本，并将其前向传播处理。但是似然加权采样存在的明显问题就是如果证据变量处在下游，那么算法的精度就会受损，前面的状态变量的采样无法从后面的证据变量得到指导。因此随着N个样本向后传播，就会有不少样本逐渐与证据变量的相似性变低。一个明显的做法就是去除掉与证据变量相似性低的样本，使得保留下来的样本大部分都是与证据变量相似性高的样本。 以上的整个过程，就是所谓的粒子滤波算法在动态贝叶斯网络中的应用。完整算法流程为： 粒子滤波和维特比算法的计算复杂度对比 近似推理（连续型）直接采样直接采样的思想是，通过对均匀分布采样，实现对任意分布的采样。因为均匀分布采样好猜，我们想要的分布采样不好采，那就采取一定的策略通过简单采取求复杂采样。假设y服从某项分布p(y)，其累积分布函数CDF为h(y)，有样本z~Uniform(0,1)，我们令 z = h(y)，即 y = h(z)^(-1)，结果y即为对分布p(y)的采样。 直接采样的核心思想在与CDF以及逆变换的应用。在原分布p(y)中，如果某个区域[a, b]的分布较多，然后对应在CDF曲线中，[h(a), h(b)]的曲线斜率便较大。那么，经过逆变换之后，对y轴（z）进行均匀分布采样时，分布多的部分（占据y轴部分多）对应抽样得到的概率便更大， 局限性 实际中，所有采样的分布都较为复杂，CDF的求解和反函数的求解都不太可行。 拒绝采样拒绝采样是由一个易于采样的分布出发，为一个难以直接采样的分布产生采样样本的通用算法。既然 p(x) 太复杂在程序中没法直接采样，那么便一个可抽样的分布 q(x) 比如高斯分布，然后按照一定的方法拒绝某些样本，达到接近 p(x) 分布的目的。 计算步骤 设定一个方便抽样的函数 q(x)，以及一个常量 k，使得 p(x) 总在 k*q(x) 的下方。（参考上图） x 轴方向：从 q(x) 分布抽样得到 a； y 轴方向：从均匀分布（0, k*q(a)) 中抽样得到 u； 如果刚好落到灰色区域： u &gt; p(a), 拒绝， 否则接受这次抽样； 重复以上过程。 局限性 拒绝了太多的样本！随着证据变量个数的增多，与证据e相一致的样本在所有样本中所占的比例呈指数级下降，所以对于复杂问题这种方法是完全不可用的。 难以找到合适的k*q(a)，接受概率可能会很低。 重要性采样重要性采样（似然加权）主要是用于求一个复杂分布p(x)的均值，最后并没有得到样本。 重要性采样的思想是借助一个易于采样的简单分布q(x)，对这个简单分布q(x)所得到的样本全部接受。但是以此得到的样本肯定不满足分布p(x)，因此需要对每一个样本附加相应的重要性权重。在重要性采样中，以p(x0)/q(x0)的值作为每个样本的权重。这样，当样本和分布p(x)相近时，对应的权重大；与分布p(x)相差过多时，对应的权重小。这个方法采样得到的是带有重要性权重的服从q(z)分布的样本，这个权重乘以样本之后的结果其实就是服从p(z)分布的。 通过上述公式，我们可以知道重要性采样可以用于近似复杂分布的均值。 吉布斯采样假设有一个例子：E：吃饭、学习、打球；时间T：上午、下午、晚上；天气W：晴朗、刮风、下雨。样本（E，T，W）满足一定的概率分布。现要对其进行采样，如：打球+下午+晴朗。 问题是我们不知道p(E,T,W)，或者说，不知道三件事的联合分布。当然，如果知道的话，就没有必要用吉布斯采样了。但是，我们知道三件事的条件分布。也就是说，p(E|T,W), p(T|E,W), p(W|E,T)。现在要做的就是通过这三个已知的条件分布，再用Gibbs sampling的方法，得到联合分布。具体方法：首先随便初始化一个组合,i.e. 学习+晚上+刮风，然后依条件概率改变其中的一个变量。具体说，假设我们知道晚上+刮风，我们给E生成一个变量，比如，学习→吃饭。我们再依条件概率改下一个变量，根据学习+刮风，把晚上变成上午。类似地，把刮风变成刮风（当然可以变成相同的变量）。这样学习+晚上+刮风→吃饭+上午+刮风。同样的方法，得到一个序列，每个单元包含三个变量，也就是一个马尔可夫链。然后跳过初始的一定数量的单元（比如100个），然后隔一定的数量取一个单元（比如隔20个取1个）。这样sample到的单元，是逼近联合分布的。 蓄水池采样蓄水池抽样（Reservoir Sampling ），即能够在o（n）时间内对n个数据进行等概率随机抽取，例如：从1000个数据中等概率随机抽取出100个。另外，如果数据集合的量特别大或者还在增长（相当于未知数据集合总量），该算法依然可以等概率抽样。 算法步骤： 先选取数据流中的前k个元素，保存在集合A中； 从第j（k + 1 &lt;= j &lt;= n）个元素开始，每次先以概率p = k/j选择是否让第j个元素留下。若j被选中，则从A中随机选择一个元素并用该元素j替换它；否则直接淘汰该元素； 重复步骤2直到结束，最后集合A中剩下的就是保证随机抽取的k个元素。 MCMC算法 随机采样和随机模拟：吉布斯采样Gibbs Sampling MCMC算法学习总结 【重点】采样方法（二）MCMC相关算法介绍及代码实现 马氏链收敛定理 马氏链定理：如果一个非周期马氏链具有转移概率矩阵P,且它的任何两个状态是连通的，那么$\\lim{p\\to\\infty}P{ij}^n$存在且与i无关，记$\\lim{p\\to\\infty}P{ij}^n = \\pi(j)$，我们有： 其中$\\pi = [\\pi(1), \\pi(2), … , \\pi(j), …], \\sum_{i=0}^{\\infty}\\pi_i = 1, \\pi$称为马氏链的平稳分布。 所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的。 说明： 该定理中马氏链的状态不要求有限，可以是有无穷多个的； 定理中的“非周期“这个概念不解释，因为我们遇到的绝大多数马氏链都是非周期的； 细致平稳条件 针对一个新的分布，如何构造对应的转移矩阵？ 对于一个分布$\\pi(x)$，根据细致平稳条件，如果构造的转移矩阵P满足$\\pi(i)P{ij} = \\pi(j)P{ji}$，那么$\\pi(x)$即为该马氏链的平稳分布，因此可以根据这个条件去构造转移矩阵。 通常情况下，初始的转移矩阵$P$一般不满足细致平稳条件，因此我们通过引入接受率构造出新的转移矩阵$P’$，使其和$\\pi(x)$满足细致平稳条件。由此，我们便可以用任何转移概率矩阵（均匀分布、高斯分布）作为状态间的转移概率。 如果我们假设状态之间的转移概率是相同的，那么在算法实现时，接收率可以简单得用$\\pi(j)/\\pi(i)$表示。 Metropolis-Hastings采样对于给定的概率分布p(x),我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布， 于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x), 那么我们从任何一个初始状态x0出发沿着马氏链转移, 得到一个转移序列 x0,x1,x2,⋯xn,xn+1⋯,， 如果马氏链在第n步已经收敛了，于是我们就得到了 π(x) 的样本xn,xn+1⋯。 在马尔科夫状态链中，每一个状态代表一个样本$x_n$，即所有变量的赋值情况。 通过分析MCMC源码，可以知道：假设状态间的转移概率相同，那么下一个样本的采样会依赖于上一个样本。假设上一个样本所对应的原始分布概率$\\pi(x)$很小，那么下一个样本的接受率很大概率为1；反之如果上一个样本的原始分布概率$\\pi(x)$很大，那么下一个样本会有挺大概率被拒绝。这样的机制保证了生成的样本服从分布$\\pi(x)$。 从上述分析可以看出，假如初始状态的样本对应的分布概率很小，那么在算法刚开始运行时所产生的样本（即使是分布概率很小的样本）很大可能都会被接收，从而使得算法刚开始运行时采样的样本不满足原始分布$\\pi(x)$。只要算法采样到分布概率大的样本（此时即为收敛！），那么之后所采样得到的样本就会基本服从原始分布。当然，从初始状态遍历到分布概率大的状态时需要运行一段时间，这段过程即为收敛的过程。MCMC算法在收敛之后，保证了在分布概率$\\pi(x)$大的地方产生更多的样本，在分布概率$\\pi(x)$小的地方产生较少的样本。 一个马尔可夫链需要经过多次的状态转移过程采用达到一个稳定状态，这时候采样才比较接近真实的分布。此过程称为burn in。一般可通过丢弃前面的N个采样结果来达到burn in。 疑问 MCMC的收敛是什么意思？这个过程中是什么参数会更新导致收敛？如何确定何时收敛？ 收敛过程没有参数会更新，收敛的思想类似于大数定理。应用MCMC算法采样时，初始的样本量少，服从的分布可能和复杂分布$\\pi(x)$相差挺远，但随着状态转移数的增加（转移矩阵P的应用），根据上述定理的证明，最终的样本分布会逐渐服从复杂分布$\\pi(x)$。 $\\pi$是每个状态所对应的概率分布吗？如果是的话，初始选定一个状态后，这个$\\pi$如何设定？或则在MCMC证明过程中，初始$\\pi$的概率分布如何设置？ 在MCMC的证明过程中，$\\pi$是每个状态所对应的概率分布。证明中所给定的初始$\\pi$应该只是为了证明无论初始样本符合什么分布，在经过一定数量的转移之后，得到的样本会服从复杂分布$\\pi (x)$，在实际代码实现中，不用对这个$\\pi$进行设定。 神经网络反向传播算法1. 网络初始化： 初始化权重为(0,1)内的随机数，并给定α和$\\eta$，α取(0.1,0.4)，$\\eta$取0.9左右。误差精度值$\\varepsilon$。 2. 数据加载： 提供训练样本集${xn, y_n}{n-1}^{N}$。 3. 前向传播： 计算网络的实际输出以及各隐层单元的状态（即前向过程）。 上述的$I { \\mathrm { p } ^ { i } } ^ { ( l ) }$即为本层的输入，$O { j } ^ { ( l - 1 ) }$为上一层的输出，L、S代表两层的神经元个数，其实L==S,f(x)为阈值函数。 4. 反向传播： 反向计算误差，由以下公式计算完成： 对于输出层，误差计算公式如下： 对于隐含层，误差计算公式如下： 即对于隐含层，其误差取决于上一层的误差值，K、M、J分别为第二层，第三层，第一层的神经元的个数，其实K=M=J. 5. 更新权重： 修改权重和阈值 6. 中止判断： 判断输出层误差是否满足$\\varepsilon $的要求，若满足则则停止训练，否则转向（3）,继续执行。 Hebb学习（无监督）Hebb学习规则与“条件反射”机理一致，并且已经得到了神经细胞学说的证实。 巴甫洛夫的条件反射实验：每次给狗喂食前都先响铃，时间一长，狗就会将铃声和食物联系起来。以后如果响铃但是不给食物，狗也会流口水。受该实验的启发，Hebb的理论认为在同一时间被激发的神经元间的联系会被强化。比如，铃声响时一个神经元被激发，在同一时间食物的出现会激发附近的另一个神经元，那么这两个神经元间的联系就会强化，从而记住这两个事物之间存在着联系。相反，如果两个神经元总是不能同步激发，那么它们间的联系将会越来越弱。Hebb学习律可表示为： W_{ij}(t+1)=W_{ij}(t)+a⋅y_i⋅y_j其中$W{ij}$表示神经元j到神经元i的连接权，$y_i$与$y_j$表示两个神经元的输出，a是表示学习速率的常数，如果$y_i$与$y_j$同时被激活，即$y_i$与$y_j$同时为正，那么$w{ij}$将增大。如果$yi$被激活，而$y_j$处于抑制状态，即$y_i$为正$y_j$为负，那么$w{ij}$将变小。 Delta学习规则（有监督）Delta学习规则是一种简单的有监督学习算法，该算法根据神经元的实际输出与期望输出差别来调整连接权，其数学表示如下： W_{ij}(t+1)=W_{ij}(t)+a⋅(d_i−y_i)x_j(t)其中$W{ij}$表示神经元j到神经元i的连接权，$d_i$是神经元i的期望输出，$y_i$是神经元i的实际输出，$x_j$表示神经元j状态，若神经元j处于激活态则$x_j$为1，若处于抑制状态则$x_j$为0或-1（根据激活函数而定）。a是表示学习速度的常数。假设$x_i$为1，若$d_i$比$y_i$大，那么$W{ij}$将增大，若$di$比$y_i$小，那么$W{ij}$将变小。 Detla规则简单来讲就是：若神经元实际输出比期望输出大，则减少输入为正的连接的权重，增大所有输入为负的连接的权重。反之，则增大所有输入为正的连接权的权重，减少所有输入为负的连接权的权重。 支持向量机基本问题： \\begin{array} { c l } { \\min _ { \\gamma , w , b } } & { \\frac { 1 } { 2 } \\| w \\| ^ { 2 } + C \\sum _ { i = 1 } ^ { m } \\xi _ { i } } \\\\ { \\text { s.t. } } & { y ^ { ( i ) } \\left( w ^ { T } x ^ { ( i ) } + b \\right) \\geq 1 - \\xi _ { i } , \\quad i = 1 , \\ldots , m } \\\\ { } & { \\xi _ { i } \\geq 0 , \\quad i = 1 , \\ldots , m } \\end{array}对偶问题： \\begin{array} { c l } { \\max _ { \\alpha } } & { W ( \\alpha ) = \\sum _ { i = 1 } ^ { m } \\alpha _ { i } - \\frac { 1 } { 2 } \\sum _ { i , j = 1 } ^ { m } y ^ { ( i ) } y ^ { ( j ) } \\alpha _ { i } \\alpha _ { j } \\left\\langle x ^ { ( i ) } , x ^ { ( j ) } \\right\\rangle } \\\\ { \\text { s.t. } } & { 0 \\leq \\alpha _ { i } \\leq C , \\quad i = 1 , \\ldots , m } \\\\ { } & { \\sum _ { i = 1 } ^ { m } \\alpha _ { i } y ^ { ( i ) } = 0 } \\end{array}","link":"/categories/study/《计算智能》笔记/"},{"title":"《计算机视觉》笔记","text":"图像处理滤波空域滤波 频域滤波双边滤波 参考 双边滤波（Bilateral filter） 双边滤波器（Bilateral filter）是一种可以保边去噪的滤波器。可以滤除图像数据中的噪声，且还会保留住图像的边缘、纹理等（因噪声是高频信号，边缘、纹理也是高频信息，高斯滤波会在滤除噪声的同时使得边缘模糊）。它和我们普通的高斯滤波器一样，也是使用一个卷积核（模板矩阵），叠加到待处理像素点上，使用对应邻域像素点的加权求和来作为新的输出像素点的值一种方法，简单来说，双边滤波和高斯滤波一样，不同只在于模板矩阵的不同。双边滤波器的模板系数矩阵由高斯模板矩阵点乘（元素级相乘）值域系数获得。 原理：在平坦区域，像素差值较小，对应值域权重r接近于1，此时空域权重d起主要作用，相当于直接对此区域进行高斯模糊，在边缘区域，像素差值较大，值域系数下降，导致此处核函数下降（因w=r*d），当前像素受到的影响就越小，从而保持了边缘的细节信息。 思想：抑制与中心像素值差异较大的像素（即使你们空域相距较近）。 计算方法：对每一个邻域像素点，计算出其对应的空域系数和值域系数，相乘得到总的系数，然后进行加权求和。 傅里叶变换频谱 参考 如何理解 图像傅里叶变换的频谱图 图像频域滤波与傅里叶变换 图像的二维傅里叶变换频谱图特点研究 How to Do a 2-D Fourier Transform in Matlab Fourier transform visualization using windowing 二维频谱中的每一个点都是一个与之一一对应的二维正弦/余弦波。 一幅图像经过傅里叶变换之后的结果包含频率和相位信息。频率信息说明了原图像中包含的低频和高频部分的分布情况，而相位信息则说明了这些频率信息是如何组成的。同一组正弦曲线和余弦曲线，经过不同的相位组合起来会形成不同的曲线，因此说明相位是决定一幅图像表征的重要信息。 此外，在原始图像中，频率的分布区域是不同的，有时同样的频率部分会分布在图像的不同区域。但是在频谱中，这些位置信息并不会记录，频谱只是将相同的频率部分累加起来，而相位才记录了这些频率部分的分布信息。 我们将一幅图像A的频率部分和另一幅图像B的相位部分组合起来，经过反傅里叶变换之后得到的图像大多体现的是图像B的特征。 傅里叶变换之后的频谱，中间是高频部分，四周是低频部分；而经过频谱居中（fftshift）处理后的频谱，中间是低频部分，四周是高频部分。 频谱居中后的频谱图： 频谱未居中的频谱图： 下图分别为图a、图b、图c、图d。图b去掉了原图像中的水平部分的频率信息，因此在反傅里叶变换后图像水平部分的边缘信息都丢失了；同理，图c和图d分别去掉了对角线位置的频率信息，是否去掉的成都不同，对应的反傅里叶变化拿到图像对角线位置的边缘信息也去除掉了。 为什么频谱图是对称的？ 傅里叶分析傅里叶变换特点 Aliasing problem模板匹配图像金字塔滤波组和纹理图像压缩算法JPEGPNG特征检测与匹配边缘检测介绍目标：识别图像中的突变区域。 定义：边缘是图像中强度值急剧变化的区域。 噪声影响噪声会对边缘检测产生一定的干扰。差分滤波对噪声的反应很强烈。 可以在进行边缘检测前，对图像进行平滑来减轻噪声的影响。 根据公式$\\frac {d}{dx}(fg) = f \\frac{d}{dx}g$，我们可以直接对平滑函数g先求导，再作用在f上。 平滑操作可以移除噪声，但同时也会对边缘的检测造成一定的影响。因此需要权衡好平滑和定位。 边缘检测的设计边缘算子好的标准： Good detection：能够发现所有的真实边缘，忽略噪声等。 Good localization：检测到的边缘尽可能靠近真实的边缘；对每个真实边缘点，仅检测返回一个边缘点。 边缘检测的可用信息： 边界上颜色、强度、纹理的差别； 连续性和闭包； High-level知识。 Canny边缘算子 参考 Canny边缘检测算法的实现 边缘检测的三大准则： 低错误率的边缘检测：检测算法应该精确地找到图像中的尽可能多的边缘，尽可能的减少漏检和误检。 最优定位：检测的边缘点应该精确地定位于边缘的中心。 图像中的任意边缘应该只被标记一次，同时图像噪声不应产生伪边缘。 算法流程如下： 高斯模糊 计算梯度幅值和方向 非最大值抑制 双阀值 滞后边界跟踪 边缘检测研究现状 局部边缘检测效果很好，但是由于光照和纹理等变化，导致了许多false positives的出现； 部分方法将轮廓信息也考虑进去，取得了较好的结果； 很少有方法从数据里学习边缘特征信息； 没有很好得利用到high-level的物体信息。 兴趣点和角点兴趣点匹配流程： Detection：找到具有区分度的兴趣点； Description：提取兴趣点周围的局部特征描述子 Matching：根据一定的策略匹配局部特征描述子。 表现权衡： 高质特征的特点： Repeatability：相同的特征可以找原始图像和经过几何变换等后的图像找到。 Saliency：每个特征间的区分度高。 Compactness and efficiency：特征数量远小于像素点数量。 Locality：特征占据图像很小的区域，robust to clutter and occlusion。 Harris角点 参考 Harris角点检测算法详解 harris角点检测器 Harris角点检测是特征点检测的基础，提出了应用邻近像素点灰度差值概念，从而进行判断是否为角点、边缘、平滑区域。Harris角点检测原理是利用移动的窗口在图像中计算灰度变化值，其中关键流程包括转化为灰度图像、计算差分图像、高斯平滑、计算局部极值、确认角点。 算法过程： 求出I(x,y)在x、y方向上的梯度Ix、Iy 分别求出在x、y方向上的梯度乘积，I2x=Ix∗Ix,I2y=Iy∗Iy,Ixy=Ix∗Iy 对I2x，I2y，Ixy进行高斯加权，从而产生A，B，C三个元素，如下：A=g(I2x)=I2x∗wB=g(I2y)=I2y∗wC=g(I2xy)=I2xy∗w 求出每个像素的Harris响应值R，令小于阈值（阈值一般为0.01 * np.max(R)）的响应值R为0; 进行3 3邻域非极大值抑制，即如果该点的值比3 3邻域的其它角点小，则删除该角点; 记录下角点在原图像的位置，即图像角点所在的位置。 注意：应先计算差分图像再进行高斯平滑，否则先高斯平滑可能会导致原图像中的角点无法精确得位于边缘处，而往内部偏移。具体表现如下二图所示： Harris角点性质如下： 阈值决定检测点数量 增大$\\alpha$的值，将减小角点响应值$R$，降低角点检测的灵性，减少被检测角点的数量；减小$\\alpha$值，将增大角点响应值$R$，增加角点检测的灵敏性，增加被检测角点的数量。 Harris角点检测算子对亮度和对比度的变化不敏感 这是因为在进行Harris角点检测时，使用了微分算子对图像进行微分运算，而微分运算对图像密度的拉升或收缩和对亮度的抬高或下降不敏感。换言之，对亮度和对比度的仿射变换并不改变Harris响应的极值点出现的位置，但是，由于阈值的选择，可能会影响角点检测的数量。 Harris角点检测算子具有旋转不变性 Harris角点检测算子使用的是角点附近的区域灰度二阶矩矩阵。而二阶矩矩阵可以表示成一个椭圆，椭圆的长短轴正是二阶矩矩阵特征值平方根的倒数。当特征椭圆转动时，特征值并不发生变化，所以判断角点响应值也不发生变化，由此说明Harris角点检测算子具有旋转不变性。 Harris角点检测算子不具有尺度不变性 如下所示，当上侧的山峰图像被放大时，在检测窗口尺寸不变的前提下，在窗口内所包含图像的内容是完全不同的。上侧的图像可能被检测角点，而下侧的图像则可能被检测为边缘或曲线。 Hessian &amp; Harris： pB boundary detector Martin D R, Fowlkes C C, Malik J. Learning to detect natural image boundaries using local brightness, color, and texture cues[J]. IEEE transactions on pattern analysis and machine intelligence, 2004, 26(5): 530-549. 人类在识别物体边缘时，不仅会利用到强度信息，还会利用到颜色、纹理、亮度等多种信息。有些图像的边缘信息在强度上没有明显区别，难以检测出来，但是可能在颜色或则纹理空间上就可以明显得进行区分，这启发了我们可以结合颜色、纹理、亮度、强度等多种信息来进行边缘信息的检测。 如下图所示，I、B、C、T分别表示强度、亮度、颜色、纹理。在Boundaries列表里，我们可以看到第一幅图在边缘上强度变化明显，但是颜色信息却没有明显的变换；而第三幅图则是边缘上强度变化不明显，但是颜色和纹理等信息都有明显的变化，因此可以通过颜色和纹理信息检测出这个图像的边缘信息。 多个图像在不同特征信息下的边缘检测结果如下： 应用 图像校准 运动跟踪 机器导航 数据库检索 物体识别 3D重建 局部特征在不同尺度空间下，同一幅图像的兴趣点所构成的局部特征信息很可能是不同的，难以进行匹配。因此我们所采用的局部特征描述子需要具有尺度不变性，能够根据所处的尺度选择对应的局部特征表示。 DoGDoG应该是LoG的近似。 为什么DoG三维图中的最大值和最小值点是角点? 在不同的尺度下，检测到的图像边缘信息是不同的。由于噪声的存在，尺度越大，检测后噪声会被去掉，但是同时边缘信息也会丢失一部分。在相邻的三个尺度空间下检测到的边缘信息，如果某个位置的值都是最大的，说明这个点不会因为尺度的变化而变化，具有尺度不变性，可作为图像的角点。 Harris-LaplaceMSERHistogramsSIFT 参考 SIFT特征匹配算法介绍——寻找图像特征点的原理 SIFT特征提取分析 SIFT详解 SIFT算法的特点有： SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性； 独特性（Distinctiveness）好，信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配； 多量性，即使少数的几个物体也可以产生大量的SIFT特征向量； 高速性，经优化的SIFT匹配算法甚至可以达到实时的要求； 可扩展性，可以很方便的与其他形式的特征向量进行联合。 步骤一：构建尺度空间 尺度空间理论目的是模拟图像数据的多尺度特征。可以利用高斯卷积核中不同的$\\sigma$设置，实现对图像的不同尺度变换。σ大小决定图像的平滑程度，大尺度对应图像的概貌特征，小尺度对应图像的细节特征。大的σ值对应粗糙尺度(低分辨率)，反之，对应精细尺度(高分辨率)。 图像金字塔的建立： 对于一幅图像I,建立其在不同尺度(scale)的图像，也成为子八度（octave），这是为了scale-invariant，也就是在任何尺度都能够有对应的特征点，第一个子八度的scale为原图大小，后面每个octave为上一个octave降采样的结果，即原图的1/4（长宽分别减半），构成下一个子八度（高一层金字塔）。 在上图中，i为octave的塔数（第几个塔），s为每塔的层数。2^{i-1}和k=2^{1/s}$值的设定都是必需，这保证了整个尺度空间从下往上是连续递增的（即高斯函数的标准差单调变大，可以计算证明），这也确保了图像从直观上来看越往上越模糊。 由图片的size决定建几个塔，每塔几层图像（s一般为3-5层）。其中，如果在检测极值点前对原始图像进行高斯平滑会导致图像丢失高频信息，所以一般在建立尺度空间前首先对原始图像长宽扩展一倍，以保留原始图像信息，增加特征点数量。0塔的第0层是原始图像(或你double后的图像)，往上每一层是对其下一层进行Laplacian变换（为什么不是gaussian变换），塔间的图片是降采样关系，例如1塔的第0层可以由0塔的第3层down sample得到，然后进行与0塔类似的高斯卷积操作。 找到极值点后，需要取出边缘信息？ 结果中包含很多边缘信息，而这些边缘信息很多只在某个方向上具有较大的梯度，不是角点，因此需要利用Harri角点类似的矩阵进行去除。 在生成关键点的描述子的时候，需要将坐标轴旋转为关键点的方向，以确保旋转不变性。这步的原理是什么？ 在图像发生旋转之后，关键点领域内的像素点的梯度值仍然是相同的，且关键点的主方向和领域像素点间的梯度方向是相对不变的，可以表征原来图像的信息。但是在生成描述子时需要统计8个方向的梯度方向直方图，而梯度方向和图像X轴是相关的，只有将坐标轴旋转为关键点的主方向，在不同旋转图像下得到的特征向量才会是一致的，从而保证的旋转不变形。 为什么高斯函数可分离性可以减小计算量？或则两个一维的高斯计算过程是如何的？ SUFT 参考 SUFT算法解析 Shape ContextGeometric BlurSelf-similarity DescriptorHOG 参考 深入浅出理解HOG特征—-梯度方向直方图 HOG特征（Histogram of Gradient）学习总结 图像特征提取三大法宝：HOG特征，LBP特征，Haar特征 在HOG中，需要统计每一个像素的幅度和角度，而在计算时角度的范围是0度到360度，但是因为角度箭头方向和与之相对的180度箭头方向是相同的（值一样），因此在HOG中，一般将计算得到的所有角度都变化为0度到180度的范围。而经验也表明使用[0, 180]的表达比[0， 360]更适用于行人检测。 步骤如下： 标准化gamma空间和颜色空间 计算图像梯度 为每个细胞单元构建梯度方向直方图 把细胞单元组合成大的块（block），块内归一化梯度直方图 收集HOG特征 疑问1：为什么[0, 180]的表现会更好？ 疑问2：为什么在HOG中，需要将图像分成小的连通区域，对细胞单元单独计算特征向量再合并起来？ 由于HOG是在图像的局部方格单元上操作，所以它对图像几何的和光学的形变都能保持很好的不变性，这两种形变只会出现在更大的空间领域上。其次，在粗的空域抽样、精细的方向抽样以及较强的局部光学归一化等条件下，只要行人大体上能够保持直立的姿势，可以容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。因此HOG特征是特别适合于做图像中的人体检测的。","link":"/categories/study/《计算机视觉》笔记/"},{"title":"《高级人工智能》笔记","text":"绪论图灵测试定义： 图灵测试指测试者与被测试者（一个人和一台机器）隔开的情况下，通过一些装置（如键盘）向被测试者随意提问。进行多次测试后，如果有超过30%的测试者不能确定出被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。 意义： 图灵测试推动了计算机科学和人工智能的发展。人工智能大体分为弱人工智能和强人工智能，目前大部分的研究成果都属于弱人工智能，如果有机器能通过图灵测试，那将标志着强人工智能的出现。 Occam’s RazorOccam’s Razor（奥卡姆剃刀），这个原理称为“如无必要，勿增实体”，即“简单有效原理”。强调简约主义，切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。 智能Agent理性Agent： 对每一个可能的感知序列，根据已知的感知序列提供的证据和Agent具有的先验知识，理性Agent应该选择能使其性能度量最大化的行动。 PEAS Performance（性能） Environment（环境） Actuators（执行器） Sensors（传感器） 简单反射Agent 基于模型的反射Agent 基于目标的Agent 基于效用的Agent 搜索算法启发式搜索A*算法的定义： A*算法是一种静态路网中求解最短路径最有效的直接搜索方法，也是解决许多搜索问题的有效算法。 公式表示为： f(n)=g(n)+h(n) f(n) 是从初始状态经由状态n到目标状态的代价估计， g(n) 是[状态空间中从初始状态到状态n的实际代价， h(n) 是从状态n到目标状态的最佳路径的估计代价。 可允许的启发式函数： h(n)是一个可采纳的启发式，即h(n)从不会过高估计到达目标的耗散。可采纳启发式是最优的，因为其认为求解问题的耗散是低于实际耗散的。 如果$h^(n)$为从节点n到目标节点的实际最优路径，h是可允许的（admissible）当且仅当：$h(n) \\leqslant h^(n)$ 对抗搜索MAX-MIN搜索方法： 执行深度优先搜索，产生完备的状态空间搜索树； 使用效用函数评估每个节点的的分布； 在MIN节点，从子节点选择得分最低的节点扩展； 在MAX节点，从子节点选择得分最高的节点扩展。 特点： 每一步的执行结果是确定的； 两个游戏者轮流进行操作； 游戏者之间进行零和博弈； 所有状态空间可知。 alpha​-beta剪枝定义： $\\alpha$-$\\beta$剪枝用于裁剪搜索树中对决策没有影响的不需要搜索的分枝，以提高运算速度。 定义$\\alpha$ := 目前为止路径上发现的MAX的最佳选择；$\\beta$ := 目前为止路径上发现的MIN的最佳选择。 搜索过程中不断更新$\\alpha$和$\\beta$的值，且当某个结点的值分别比目前的MAX的$\\alpha$或MIN的$\\beta$的值更差时，则裁剪此此节点剩下的分支。 问题： 在求解过程中，每个节点都要写明alpha和beta值？还是只要写出MAX节点的alpha值和MIN节点的beta值? 下棋和打牌的不同点： 下棋是公开明面的博弈，对于算法而言当前的状态都是已知的；而牌存在着许多未知的情况，即有不确定性，在这种情况下进行搜索会产生巨大的状态空间，难以求解。 约束满足问题时序模型概率公式全概率公式： P(A|B) = \\frac{P(A)*P(B|A)}{P(B)}贝叶斯公式： P(A_i|B) = \\frac{P(A_i)*P(B|A_i)}{\\sum_{i=1}^nP(A_i)P(B|A_i)}$P(A)$和$P(B)$为先验概率，可以根据以往经验和分析得到的概!率，不考虑与其他事件的联系。 $P(A|B)$则称为B发生后A的后验概率，注意式中事件B往往被认为是“因”，是给定的，也就是说P(B)往往是一个常数，分母P(B)可以去掉，贝叶斯公式又可以被表示为： P(A|B) = \\alpha P(A)*P(B|A)上式中的$\\alpha$表示归一化处理，保证概率和是1。 一般时序模型模型简介： 我们建立一个时序模型如下，包含了我们所要解决的所有问题。 这个模型包含两个序列：一个是状态序列，用X表示；一个是观测序列（证据序列）用Y表示。状态序列反应了系统的真实状态，一般不能被直接观测，即使被直接观测也会引进噪声；观测序列是通过测量得到的数据，它与状态序列之间有规律性的联系。 举个例子，假设有一个人待在屋子里不知道外边有没有下雨，他于是观察进屋子里的人是否带伞，这里有没有下雨就是状态，有没有带伞就是观测。 两个基本假设： 马尔科夫假设：假设当前状态只与上一个状态有关，而与上一个状态之前的所有状态无关。公式为： P(X_{t+1}|X_{1:t}) = P(X_{t+1}|X_{t})观测假设：假设当前观测值只依赖于当前状态，与其他时刻的状态无关。公式为： P(Y_{t}|X_{1:t}) = P(Y_{t}|X_{t})时序模型中的推理时序模型所要解决的是以下几个问题： （1）滤波 P(X_t|Y_{1:t})即根据现在及以前的所有测量数据，估计当前的状态。在雨伞那个例子中，根据目前为止过去进屋的人携带雨伞的所有观察数据，计算今天下雨的概率，这就是滤波。 （2）预测 P(X_{t+k}|Y_{1:t}), k> 0即根据现在及现在以前的所有测量数据，估计未来某个时刻的状态。在雨伞的例子中，根据目前为止过去进屋的人携带雨伞的所有观察数据，计算从今天开始若干天后下雨的概率，这就是预测。 （3）平滑 P(X_k|Y_{1:t}), 0 < k < t即根据现在及现在以前的所有测量数据，估计过去某个时刻的状态。在雨伞的例子中，意味着给定目前为止过去进屋的人携带雨伞的所有观察数据，计算过去某一天的下雨概率。 （4）最可能解释 arg \\max_{x_{1:t}}P(X_{1:t}|Y_{1:t}))即给出现在及现在以前的所有测量数据，找到最能最可能生成这些测量数据的状态序列。例如，如果前三天每天都出现雨伞，但第四天没有出现，最有可能的解释就是前三天下雨了，而第四天没下雨。最可能解释也被称为解码问题，在语音识别、机器翻译等方面比较有用，最典型的方法是隐马尔可夫模型。 （5）评估 P(Y_{1:t}|X_{1:t}, \\lambda)这里面的$\\lambda$是指模型。这个公式意味着在该模型下，给定到目前为止的状态序列，计算输出特定观测序列的可能性。这其实是个评估问题，可以评估模型的好坏，概率越高，意味着模型越能反映观测序列与状态序列之间的联系，模型就越好。 （6）学习 学习$\\lambda $，也即状态转移概率和观测概率 P(X_{t+1}|X_{t}), P(Y_t|X_t)学习的目的是根据历史数据得到合理的模型，一般是根据一个目标函数，对模型进行迭代更新，例如使（5）中要计算的值最大便可以作为一个目标。 推理算法（1）前向递归算法 应用于滤波问题。 （2）前向-后向算法 应用于平滑问题。 （3）维特比算法 m_{t+1} = P(e_{t+1}|X_{t+1})\\max_{X_t}(P(X_{t+1}|X_t) m_{1:t})应用于最大可能解释问题。 隐马尔可夫模型定义： 隐马尔可夫模型（Hidden Markov Model，HMM）是处理序列问题的统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其目的是从可观察的参数中确定该过程的隐含参数，然后利用这些参数来作进一步的分析。 三个问题： 给定一个模型，如何计算某个特定的输出序列的概率？ Forward-Backward算法（如何求解？） 给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列？ 维特比算法 给定足够量的观测数据，如何估计隐含马尔可夫模型的参数？ 训练隐含马尔可夫模型更实用的方式是仅仅通过大量观测到的信号O1，O2，O3，….就能推算模型参数的P(St|St-1)和P(Ot|St)的方法（无监督训练算法），其中主要使用鲍姆-韦尔奇算法。 卡尔曼滤波思想： 以K-1时刻的最优估计$x{k-1}$为准，预测K时刻的状态变量$x{k/k-1}$，同时又对该状态进行观测，得到观测变量$Z_k$，再在预测与观测之间进行分析，或则说是以观测量对预测量进行修正，从而得到K时刻的最优状态估计$x_k$。 Kalman滤波算法的本质就是利用两个正态分布的融合仍是正态分布这一特性进行迭代。 五个核心方程： 下述解答参考自[1]。上述的例子为一维情况，实际每个状态往往包含多个维度，此时对应的公式换成方差矩阵计算即可。 预测偏差和观测偏差计算协方差的原理？ 假设预测结果满足正态分布$N_0$，观测结果满足正态分布$N_1$，然后当前状态的分布为$N_0$和$N_1$的结合，新的分布也满足正态分布。预测偏差和观测偏差结合计算的实际上是新的正态分布下的Kalman增益，Kalmen增益描述了原始分布在新的分布下所对应的权重。相关公式如下： 最优值偏差的计算原理？ 当前状态的最优值偏差，计算的即新分布下的方差，根据上述公式即可得到。同样，当前状态的位置即当前分布的均值，同样可由上述公式求得。 预测误差的计算方法 动态贝叶斯网络定义： 动态贝叶斯网络（Danymic Bayesian Networks, DBN）在任何时刻t，都可以具有任何数量的状态变量和证据变量。 优点： 如果把一个复杂系统的状态分解成几个变量，那么整个模型所需要的参数将会大大减少。假设DBN有20个Boolean型的变量，每个变量只和上一时刻的三个节点有关联。那么在DBN中，所需要的参数为$202^3= 160$个；而在HMM中，每个时刻的隐含状态数量为$2^{20}$，因此转移矩阵的参数量为$2^{20}2^{20}$。显然，DBN所需要的参数量大大减少。 参考 学习笔记|卡尔曼滤波，粒子滤波，动态贝叶斯网络 简书：卡尔曼滤波 一文搞懂HMM（隐马尔可夫模型） 简单理解viterbi算法 HMM模型和Viterbi算法 简单决策效用理论定义： The utility function rates states and thus formalizes the desirability of a state by the agent.U(S) denotes the utility of state S for the agent. Expected Utility： EU(A | E) = \\sum_i P (Result_i(A) | Do(A); E) × U(Result_i(A))The principle of maximum expected utility (MEU) says that a rational agent should choose an action that maximizes $EU(A | E).$ Lottery Scenario： 我们可以用Lottery方案来模拟MEU，假设有$L = [p1; C1; p2; C2; : : : ; pn; Cn] $，分别表示获得金钱C1、C2, … , Cn的概率，这等同于在某个状态下所做的一个决策。假设每个人有多种彩票L可以选择，那么他一定会选择能让自己最后能获得金钱最大的彩票。 The Axioms of utility theory： Orderability Transitivity Continuity Substitutability Monotocicity Decomposability 复杂决策时间效用有限期：在有限期条件N下，给定状态的最优行动会随时间变化。有限期的最优策略是非静态的，N之后的所有行动，对整体效用没用影响。 无限期：在无限期条件下，最优行动只依赖于当前状态，其最优策略是静态的。 奖励函数累加回报：$U_h([s_0,s_1,s_2, …]) = R(s_0) + R(s_1) + R(s_2) + …$ 折扣回报：$U_h([s_0,s_1,s_2, …]) = R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + …$ 奖励因子$\\gamma$的解释：今天的一元钱在明天一般都会贬值。所以当某个状态s较晚到达时，要控制奖励因子使得获得的价值减少。 当环境不包含终止状态时，整个系统的效用值通常是无限大。可通过以下三种方法解决： 使用折扣回报； 环境包含终止状态； 根据每个时间步所获得的平均回报进行比较。 Bellman方程 U ( s ) = R ( s ) + \\gamma \\max _ { a \\in A ( s ) } \\sum _ { s ^ { \\prime } } P \\left( s ^ { \\prime } | s , a \\right) U \\left( s ^ { \\prime } \\right)价值迭代 策略迭代 样例学习学习策略监督学习无监督学习半监督学习协同训练（Co-Training） 假设数据有两种特征表达，比如图像特征（X-1, Y-1）和文本特征（X-2, Y-2）。对于未标注数据同样有两种View。协同训练（Co-Training）的过程如下： 从（X-1, Y-1），（X-2, Y-2）分别训练得到两个个分类模型F-1，F-2 分别使用F-1与F-2对未标注数据进行预测 将F-1所预测的前K个置信度最高的样本加入F-2的训练数据集 将F-2所预测的前K个置信度最高的样本加入F-1的训练数据集 回到第1步 直推式学习（transduction learning） 直推式学习可以看成是半监督学习的一个子问题，直推式学习假设未标记的数据就是最终要用来测试的数据，其目的是在这些有限的未标记数据上取得最佳泛化能力；而半监督学习并不知道未来的测试数据是哪些。 弱监督学习深度学习迁移学习零样本学习集成学习 参考 分类器组合方法Bootstrap, Boosting, Bagging, 随机森林（一） 分类器组合方法Bootstrap, Boosting, Bagging, 随机森林（二） BootstrappingBootstrapping是一种从给定训练集中有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。 过程： 采用重复抽样的方法每次从n个原始样本中抽取m个样本（m自己设定）； 对于m个样本计算统计量； 重复步骤（1）（2）N次（N一般大于1000），这样就可以算出N个统计量； 计算这N个统计量的方差。 BaggingBagging是boostrap aggregation的缩写，是一种根据均匀概率分布从数据集中重复抽样（有放回的）的技术。子训练样本集的大小和原始数据集相同。在构造每一个子分类器的训练样本时，由于是对原始数据集的有放回抽样，因此同一个训练样本集中可能出现多次同一个样本数据。 过程: 每次从原始数据集中有放回的随机抽样n个样本形成自助训练集，重复S次后得到S个新的训练集。 对每个自助训练集应用弱分类器，这样就得到了S个弱分类器； 将预测数据放在这S个弱分类器上计算，计算结果采用投票方式（分类问题）和简单求平均（回归问题）即可。 BoostingBoosting是一个迭代的过程，用来自适应的改变训练样本的分布，使得分类器聚焦在那些很难分的样本上。 之前Bagging的过程，抽取自主样本集的时候是简单随机抽样，每个样本被抽到的概率是一样的，而Boosting算法中给每一个样本赋予一个权值，每一轮在抽取自助样本集、训练完基分类器之后会对原始样本集做一个分类，然后跟效果比对，然后对分错的训练样本，自动的调节该样本的权值（增大）。权值可以用在以下方面： 可以用作抽样分布，从原始数据集中选出自助样本集 基分类器训练时更倾向于用用高权值的样本进行训练。 例如，有10个样本，开始时简单随机抽样，每个样本被抽到的概率是一样的，都是1/10，然后第一轮结束后用得到的模型对该轮数据集中的样本分类，发现样本4被分错了，就增加它的权值，这样，第二轮抽取的时候4被抽到的概率增加了，第三轮也是一样。这样做，就可以让基分类器聚焦于难被分类的样本4，增加4被正确分类的几率。 Adaboost算法框架： 权重调整： AdaBoost “focused on” the informative or “difficult” examples. 决策树信息熵定义为： \\mathrm { H } ( \\mathrm { X } ) = - \\sum _ { i = 1 } ^ { n } p _ { i } \\log _ { 2 } p _ { i ^ { \\prime } }注意：决策树的熵计算中，log以2为底。 概率模型最大似然估计定义： 最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。最大似然估计中采样需满足一个很重要的假设，就是所有的采样都是独立同分布的。 应用场景： 在模型已经确定，但是模型的参数未知的时候，我们可以利用ML来评估模型参数。 最大后验估计定义： 最大后验估计是根据经验数据获得对难以观察的量的点估计。与最大似然估计类似，但是最大的不同时，最大后验估计的融入了要估计量的先验分布在其中。故最大后验估计可以看做规则化的最大似然估计。 优点： 在评估模型参数的时候融入了模型的先验分布，使得结果更贴合实际情况，更为准确。 ML vs MAP当模型的参数本身的概率是均匀的，即该概率为一个固定值的时候，MAP=MLE。 频率学派和统计学派的区别： 这个区别说大也大，说小也小。往大里说，世界观就不同，频率派认为参数是客观存在，不会改变，虽然未知，但却是固定值；贝叶斯派则认为参数是随机值，因为没有观察到，那么和是一个随机数也没有什么区别，因此参数也可以有分布，个人认为这个和量子力学某些观点不谋而合。 往小处说，频率派最常关心的是似然函数，而贝叶斯派最常关心的是后验分布。我们会发现，后验分布其实就是似然函数乘以先验分布再normalize一下使其积分到1。因此两者的很多方法都是相通的。贝叶斯派因为所有的参数都是随机变量，都有分布，因此可以使用一些基于采样的方法（如MCMC）使得我们更容易构建复杂模型。频率派的优点则是没有假设一个先验分布，因此更加客观，也更加无偏，在一些保守的领域（比如制药业、法律）比贝叶斯方法更受到信任。 EM算法朴素贝叶斯朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。 朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间相互条件独立。 强化学习状态空间的定义： 函数描述状态的utility的优点： 自然语言处理 文本特征抽取的向量空间模型（VSM）和TF/IDF方法 向量空间模型向量空间模型（Vector Space Model, VSM），即将文档根据词袋模型等方法表达为一个向量，对文本内容的处理简化为向量空间中的向量运算，并且它以空间上的相似度表达语义的相似度。 词袋模型词袋模型，即将文本内容看作一系列词的组合，将不同的词汇用不同的袋子装起来，袋子里统计的是对应词汇出现的次数。 TF*IDFTF：词频，即一个词汇在一篇文档中出现的频率。 IDF：DF为文档频率，即一个词汇在整个文档集中出现的频率。IDF为DF的逆log函数。 公式为： \\frac { - N _ { i } \\log \\left( D _ { i } \\right) } { K } = \\frac { N _ { i } } { K } \\log \\left( \\frac { 1 } { D _ { i } } \\right)参考 《Artificial Intelligence: A Modern Approach》主页 《Artificial Intelligence: A Modern Approach》英文电子版 《Artificial Intelligence: A Modern Approach》中文电子版 《Artificial Intelligence: A Modern Approach》课后习题答案 《Artificial Intelligence: A Modern Approach》配套python源码 《Artificial Intelligence: A Modern Approach》PPT，University of Freiburg","link":"/categories/study/《高级人工智能》笔记/"},{"title":"软工实践个人总结","text":"驻足当下课程期望对比开学初的课程期望——软件工程的实践项目课程的自我目标： 对项目的整个开发流程有一个全面专业的了解 掌握《构建之法》的精髓，懂得如何管理一个开发团队 代码能力以及代码规范得到进一步的提升 做出一个具有网站端又有移动端的项目 项目能在各类比赛中大放异彩，并进一步商业化开发，投入市场 实践课保持依旧的魔鬼强度，又有所新的创新，让我们在敲代码的时候享受到快乐 课程期望履行情况： 从实际项目的最终Beta版本可以看出，现实和期望基本还是挺吻合的： 软工实践整个过程，作为团队的PM，在经历过多次团队摩擦、意见不合之后，学会了如何处理组员之间的沟通，更加合理高效得管理一个团队，并通过一系列的任务计划与开会讨论，让整个团队的编码阶段有条不紊得进行中，让组员在编码时有着一个轻松愉快的氛围 我们团队开发了一个Web端的毕设导师互选系统，而这个系统的安卓版同时也有另外一个小组在开发。 因为系统的完成程度较高，所以很可能在下学期系统便会在学院内部上线运行，而后再尝试着将系统推行到其他学院甚至其他高校，进一步进行商业化的开发，投入使用 整个软工实践过程中，因为团队协作和课程需要，我扮演的基本都是一个PM的角色，负责项目进度的把控以及博文的撰写，所以期望中代码能力并没有得到显著的提升，这也是我整个软工实践过程中最大的遗憾，当然，团队要走得更前，必须有人做出牺牲，虽然说得好像有点假惺惺了，但确实挺可惜的。 自我提升新软件： Typora：MarkDown编辑软件 XMind：思维导图的设计 Axure RP：原型设计软件 Sublime：高效的代码编辑器 PowerDesigner：数据库设计软件 ShadowsocksR：翻墙神器，顺畅浏览github GifCam：动态图制作神器！无需安装！只有1.5M！ 新工具： Git的使用 JUnit单元测试框架的使用 Wampserver64：Web项目的开发环境 SourceTree的Github上的项目进行管理控制 利用燃尽图对项目进度进行把控 新语言： 对PHP、JS、CSS、HTML都有了进一步的了解，但因为充当的不是程序猿的角色，对语言的掌握能力并没有显著的提升 新平台： 全程使用Github平台进行项目的托管，包括Project管理面板的利用、issues的沟通、里程碑的使用等，有效得提高团队协作开发的效率 博客园。一开始让我写博文其实我是拒绝的，但是随着博文越写越多，才懂得写好一篇博文也是一门艺术。一开始接触MD时，内心除了吐槽还是吐槽，标题大小、缩进、排版等等都是一个头疼的问题，但随着不停得百度和查阅资料，对MD的使用日益熟练，也养成了属于自己的博文风格，而且博客作业也多次收到助教们的好评，证明自己的努力还是有收获的。 新方法： 撰写博文的习惯。如今项目开发过程遇到什么问题，都会习惯性得在博客里面记录下来，这对自己的学习和以后的回顾都是有着一个很好的帮助。 通过issues进行项目粒度的划分以及完成度把控。团队整个开发过程中基本没有在群里互传过资料，都是通过issues全员可视化的形式进行管理，包括每一次的总结以及每个任务的完成情况，这样组员看得到彼此的进度，对自己也有着一个很好的督促作用。 代码完成量： 又说到这个尴尬的点了，因为充当PM的角色，在团队开发过程中基本都是对组员完成的最后情况进行项目的审核，偶尔也会根据commit记录审核下代码，但次数较少，因为自己的精力大都是放在整体项目进度的推进以及博文的撰写，所以在代码上花费的精力较少。 其他提升： 团队协作管理能力 良好的沟通能力以及采纳意见 文档编写能力 全局观念以及项目粒度的划分 物尽其长，能够合理得根据组员的擅长进行任务的分配 从团队的角度来看，整个软工实践课还是非常成功的，小组的项目完成度高，组员的分数基本也都是在前十，小组目前也已经有两名成员获得了黄色领骑衫（估计还能领一件），这些都离不开每一个组员辛勤的付出。 人月神话项目经验总结以及例证分析在整个软工实践过程中，我所负责的基本都是项目管理方面的工作，所以我从PM的角度出发，讲一下项目的经验总结。 团队沟通 ：作为PM，最关键最重要的点就是处理好和队员之间的沟通！一个良好的团队氛围是成功的一半。在软工实践课刚开始时，我们团队其实因为选题原因出现过不少的矛盾和摩擦，队员们都很有自己的想法，我们的选题也因此换了三次，此外在对作业的要求上我可能比较严格，很多细节方面都是精益求精，导致团队初期队员们的反应较为强烈。我其实一开始挺担心团队是否能够顺利得走下去的，但是之后团队开了多次的会议，经过大家的沟通以及协商，才逐渐确定了彼此的分工和职责，而我自身也把精力放在全局项目的推进上，对于细节方面改成冲刺最后再完善，此外还买了几次夜宵给队员，软工实践课结束后大家也一起聚餐，整个团队的氛围可谓说是越来越好，才造就了我们高效的开发进度。 任务分配 ：要做好一个PM，要学会如何第一时间从布置的任务中提取关键信息，并细化粒度分配到各个组员。同样的，在团队初期，因为对队员的擅长了解不多，导致了前几次团队作业一两个组员的任务量十分大，任务分配不均匀。但是随着团队的配合，逐渐发现了各个组员的擅长点，比如扬涛的的审美、齐民的尽责、伟炜的强大编码能力以及妹纸们的心细，并据此分配好各个任务，尽量让每个组员的任务量尽可能得平均，物尽其长。比如在软工实践的随堂小测上，我在和老师理清了作业的需求之后，便根据每个人的擅长点进行了任务分配。我们有四个人会JAVA，每个人负责一个功能模块，剩下三个人一个文档、一个测试、一个图表制作，在短短的一个上午里便完成了高质量的作业。此外还严格要求每个编码的组员都按照注释规范给代码加上注释，以保证作业的完成质量。 团队结构 ：我们团队 的结构属于1 + 3 + 3模式，即一个PM加3个后台加3个前端，PM负责统筹全局项目推进，然后3个后台前端两两配合、结对编程，负责对应的功能模块。此外团队博客的撰写全部由我负责，数据库的设计由齐民负责，原型的改进由许玲玲负责，算法的编写以及文档晚上、图标设计上面由杨涛负责，此外齐民和伟炜由于出色的编码能力，团队的技术指导基本由他们两个负责，当然因为我们团队的前端方面不是特别出色，我们又是也会请前端大神——立昊来当我们的技术顾问（感谢立昊(^__^) ）。因此，良好的团队结构才造就了我们出色的配合默契。 项目管理 :项目管理是一个PM的主要职责，我们团队和其他团队主要不同的地方在于我们的项目管理基本都是在github上面完成的，包括github上project和issues的利用，团队群里基本没有互传过文件。无论是博客里每个组员的心得体会，亦或任务的分配以及项目进度的审核，都是在github完成。这样做的好处是把团队的所有工作记录集中在唯一的一个地方，当组员在查看issues时也可以顺便看到代码的commit记录，还可以在project看到整个项目的完成度，这样能加深每个人对项目进展的了解度。 issues的利用 ：issues的利用我想单独当作一个点来讲，因为issues确实是项目管理的一个很好的辅助工具。先说说以往我们是如何进行团队合作的：任务分配都在群里或则群公告发布，然后定期根据commit记录审核项目，对未完成的点QQ私聊组员通知，心得体会也是群文件或则邮箱收集，这样的团队模式会导致文件太过杂乱或则消息过多，组员可能会遗漏到自己的任务点。反观依照老师的建议后，我们全程采用issues进行项目的管理，根据项目的功能模块，发布对应的issues，并注明标签分配到每个人，被分配到issues的组员github会自动邮件通知，确保了每个人都能及时了解到自己的任务。而在issues的开头我还会注明这个issues具体的任务要求是如何，每当组员完成对应的任务点时，便把完成情况提交到对应的issues下（文件/演示动态图/界面截图等），然后我会把项目拉下来，根据实际的运行对完成情况进行审核，如果有做的不好的地方，我便会在issues里面继续注明。此外，issues的一个关键作用是组员可以看到彼此之间的任务进展，当组员看到其他人的issues一个接一个得关闭，无形之中对自己也有着一种督促作用，毕竟若是issues都是自己的未关闭issues，站立式会议时自己肯定也免不了尴尬。 结队编码 ：在Alpha阶段和Beta阶段时，我们小组经常在活动室一起敲代码，项目要求上疑惑的点PM可以当场解答，技术上的难点团队的技术大牛们也能帮忙，整个团队互帮互助，互相督促，整个开发的效率简直高得可怕！在第一次活动室结队编码之后，组员便一直反应说希望多些这样的编码方式，而在Beta阶段我们便在活动室一起编码了好几次，大大促进了项目的进展。众人拾柴火焰高，以后的团队合作里，还会多多采用这样的编码环境。 冲刺时间的安排 ：软工实践课的强大确实是挺大的，这导致了挺多小组频繁出现通宵熬夜的情况。作为一名程序猿，虽然说熬夜是基本的技能，但是频繁得通宵熬夜可能还会适得其反。我们小组在整个软工冲刺里，就熬过一次夜(Alpha版本验收的前一个晚上，因为项目合并出现的一些冲突导致了熬夜)，很明显的我可以感受到在那次唯一的一次熬夜里，队员们包括我的情绪都变得烦躁了起来，开发效率大大降低，因此之后的话我便再也没有把冲刺时间段放到晚上。Beta版本的冲刺我们基本都是在白天，晚上最迟10点就结束冲刺，这样的话也能让组员有时间休息（毕竟妹纸最多的组，妹纸们还是需要美容养颜觉的），虽然没有熬夜，但是开发效率却逐步加快，使得我们团队在验收前的一个礼拜左右就基本完成了Beta版本。 经验建议对下一届实践的建议 作业分值的处理上尽量合理点，有很多团队作业的强度和工作量明显不同，但是最后的分值还都是15分。建议采用加权处理，根据预估的工作量分配分值权重； 可以增加一个技术问题记录的作业，博客这种东西，说实话没有作业的督促，很少有人会自己花时间去写技术问题的记录博客。下届实践课可以让每个学生都养成写技术记录博客的好习惯，这样几十个人的经验彼此分享，是一个非常宝贵的知识库！ 听闻下届软工实践可能采取跳槽的新玩法？想法是很好，但现实很残酷。大学的课堂还是和公司很不一样的，同学之间没有那么多的利益争纷，若采取跳槽的这种制度，是否有考虑到被投票出局的同学的心理感受？因为一个软工课破坏了彼此间的友谊，我想这是所有人都不想预见的。所以依我看来，跳槽这种制度绝不可取！否则这将会成为软工实践课被黑得最惨最厉害的一个里程碑！ 对后来人的期许 选了软工实践课，就要做好付出无数时间精力的准备。软工实践课的收获在分数上是远远体现不出来了，它是公司里团队合作的一个缩影，对我们之后的职业生涯都是有着极大的帮助。 组建团队时一定要考虑周全，团队里最好至少有一个有项目精力的技术大牛，此外队员们的积极性的话也得充分调动，不然让每个人对作业都养成一种随便敷衍了事的态度，这对团队而言是一个恶循环。 在团队组建后的初期，要做好团队之间的沟通，明确好彼此的分工，这对之后每一次的团队合作都是至关重要的准备工作。 团队分析 《构建之法》里面提到的关于团队发展的阶段共有四个，分别是：萌芽阶段、磨合阶段、规范阶段、创造阶段。 萌芽阶段 ：在实践课初期，考虑到团队的人员配置以及项目的可发展性，我们经历了多次选题变更，最终才确定了“毕设导师互选系统”这个项目，并由Android端转型为Web端。根据系统的特殊性（每个学生大学只用一次），我们确定了我们的项目是基于Web端的，目标用户是学院的院负责人、系负责人、导师、学生等用户，并且当项目在学院试用情况良好后，可能还会向其他学院进行推广。 附：预则立&amp;&amp;他山之石 磨合阶段 ：确立了选题之后，我们根据用户给定的需求以及自身的实际经验，进行了项目的思维导图、数据库以及原型的设计，并定稿的项目的《软件需求规格说明书》，在文档里面对项目的主要功能模块进行了详细得介绍，并确定了项目的验收标准，以保证之后的开发都能严格按照验收标准进行。此外，我们团队构建了Github的团队协同环境，确定了编码规范以及开发工具版本的统一，以避免在团队开发中出现的代码冲突。我们还确定了项目的体系结构，采用MVC设计模式以及ThinkPHP框架进行开发，确保项目的完成效率。 附：需求规格说明书 体系结构与系统设计 规范阶段 ：在整个软工实践过程中，我们团队经历过两次重要的里程碑——Alpha版本和Beta版本。在Alpha版本里，我们团队完成了毕设选导的主要核心功能模块，并在验收时成功演示了整个选导过程；而在Beta版本时，我们主要的工作是修复Alpha版本时出现的BUG以及完善系统剩余的功能模块，此外因为Beta版本临时产生的需求变更，导致我们的系统部分界面需要重新设计。但是因为Alpha阶段的完成度较高，使得我们Beta版本总体而言的工作量并不会特别大，在验收前的一个礼拜左右基本就完成了Beta版本的所有功能模块。 附：【Alpha版本】十天冲刺集结令 【Beta版本】七天冲刺集结令 创造阶段 ：在Beta版本完成之后，我们团队的主要的工作重点便是放在所有模块细节方面的完善，比如Logo的设计，提示的规范化，界面排版布局的美化等等。此外因为我们的系统很可能会在今后上线使用，我们对实际用户进行了充分的调研，包括学生用户、系负责人用户以及院负责人用户，并根据用户的反馈意见对系统进行修补完善。我们还采用了真实数据对系统进行了模拟测试。 附：用户试用与调研报告 经过上述的团队分析，显然我们的团队最后应该达到了创造阶段，并很好得完成了项目。 文献笔记 论文 ：《Code quality analysis in open source software development》 摘要 ：开放源码软件开发的支持者认为，与传统的封闭模型相比，使用这种模型产生更好的软件。然而，很少有经验证据支持这些要求。在本文中，我们提出的试点案例研究的结果，目的是：（一）了解结构质量的影响；（二）计算出的结构质量分析的代码由开源风格开发交付的好处。为此，我们测量了100个应用程序编写的Linux的质量特性，使用的软件测量工具，并与工业标准，该工具所提出的结果进行了比较。本案例研究的另一个目标是调查的问题，在开源的模块化，这种特性被认为是至关重要的开源这种软件开发类型的支持者。我们有经验评估的应用程序组件的大小和交付的质量通过用户满意度测量之间的关系。我们已经确定，在一定程度上，应用程序的平均组件大小与此应用程序的用户满意度呈负相关。 在大一时最开始写代码时，自己的代码质量简直是惨不忍睹，变量的定义都是随随便便什么abcd，缩进什么的也都没有考虑，导致后来回头再审视自己的代码时，都不知道自己在写什么。之后大一的暑假，根据《马士兵的JAVA教学视频》自学了JAVA，更重要的是对代码质量第一次有了全新的认识，学会了大小驼峰命名法、注释的规范写法以及接口的抽象和面向对象设计的思想，整个代码质量有了一个突飞猛进的变化。自己现在日常在写代码时，都有着属于自己的编码风格，并会习惯性得加上注释，方便日后可能的再利用。 如何审核代码质量关键有一下几点：1. 代码注释；2. 编码规范（各种命名）3. 代码的可扩展性和可移植性 项目分析符合用户需求我们的项目定位比较明确——毕设导师互选系统。整个项目的开发都是基于用户（栋哥）所给定的需求，而对于需求变更也进行了相应的处理，以保证最终的产品完美得符合用的需求。此外，因为项目的性质比较特殊，若当学院采用了我们的系统进行毕设选导，那么用户量基本不成问题，政策要求的话，所有老师和每一级的学生都会成为我们的用户，而且当系统试用情况不错之后，我们还会向其他学院甚至其他高校推广我们的系统，以进一步得扩大用户群体。 附：用户试用与调研报告 项目产品宣传推广方案 时效性从上述的团队分析里可以看出，我们团队有着完整规划性的开发流程，从项目规划=&gt;需求=&gt;设计=&gt;实现=&gt;发布都依次经历过，但是系统因为还未上线，所以维护阶段暂时没有精力。我们团队采用github进行项目的托管以及团队的协同开发，并利用每次冲刺会议里展示燃尽图以及项目的进度，而在项目的issues里面也可以看到每一个任务点对应的完成时间点以及完成情况，这充分表明了我们团队能在规定的期限里很好得完成项目的开发。 Alpha版本燃尽图 Beta版本燃尽图 项目Beta版本的部分issues 可维护性和可发展性整个系统项目托管在开源的github平台上面，以便后来者可以在我们项目的基础之上进行发展和开发。此外，项目相关的文档也均汇总在github项目下的doc文件夹里面，项目使用者可以根据文档内容对整个系统有着一个充分全面的了解。我们团队在项目开发过程中，后台部分采用了接口的设计，增加了项目的可移植性的灵活性，利于之后的维护和进一步的发展。 此外，我们团队还推出了系统的官方网站，这样用户是需要一个网址，便可以在官网上面可以了解到这个系统的大概功能以及运行流程，也可以看到关于我们团队的相关信息。用户若是对系统感兴趣，可以通过官网上面的联系方式直接联系到我们团队。 自我风采我叫陈燊（shen 第一声！），来自于“我说的都队”团队，是团队的PM兼职业软文写手。因为身为组长的原因，我在实践课上“抛头露面”的次数比较多，所以感觉同学们和老师应该得记得我（至少知道我名字怎么念！）我是一个“完美主义者”，组员们的眼里的“强迫症患者”（其实是认真！负责！严谨！好吗！），无论任何事情都要尽可能得做到最好，最直观得体现就是所写的博客上面即使多一行少一行空格我都感觉不舒服，一定得让整个版式规整统一！虽然“完美主义”确实耗费了我不少的时间精力，但是也直接保证了我们团队每一次的作业 完成质量！ 再者呢，就像我个性签名说说的那样，我这个人呢，比较矛盾，喜欢宅在宿舍敲代码却又向往健身和打球，喜欢各种美食却迫于某些原因不得克制自己的吃货欲望，喜欢音乐舞蹈却天生五音不全（不过幸好高中时有在晚会上跳舞唱过歌，也没留下什么遗憾了）。 总而言之，很幸运遇上软工实践，很幸运遇到你们，很幸运遇到“我说的都队”，人生不说再见，青春永不谢幕。往后的日子，我一直都在！ “我说的都队”——拍摄于文楼，一群要搞事的程序猿 附录 Github项目链接 毕设导师互选系统项目链接 毕设导师互选系统官网链接","link":"/categories/software-engineering/软工实践个人总结/"}],"tags":[{"name":"哈希","slug":"hash","link":"/tags/hash/"},{"name":"深度哈希","slug":"deep-hash","link":"/tags/deep-hash/"},{"name":"无监督哈希","slug":"unsupervised-hash","link":"/tags/unsupervised-hash/"},{"name":"Leetcode","slug":"Leetcode","link":"/tags/Leetcode/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"CUDA","slug":"CUDA","link":"/tags/CUDA/"},{"name":"Jupyter","slug":"Jupyter","link":"/tags/Jupyter/"},{"name":"神经网络","slug":"neural-network","link":"/tags/neural-network/"},{"name":"POJ","slug":"POJ","link":"/tags/POJ/"},{"name":"贪心算法","slug":"greedy-lgorithm","link":"/tags/greedy-lgorithm/"},{"name":"动态规划","slug":"dynamic-planning","link":"/tags/dynamic-planning/"},{"name":"搜索","slug":"search","link":"/tags/search/"},{"name":"图论","slug":"graph","link":"/tags/graph/"},{"name":"监督哈希","slug":"supervised-hash","link":"/tags/supervised-hash/"},{"name":"循环神经网络","slug":"rnn","link":"/tags/rnn/"},{"name":"排序","slug":"sort","link":"/tags/sort/"},{"name":"卷积神经网络","slug":"cnn","link":"/tags/cnn/"},{"name":"朴素贝叶斯","slug":"naive-bayes","link":"/tags/naive-bayes/"},{"name":"支持向量机","slug":"svm","link":"/tags/svm/"},{"name":"时序推理，隐马尔可夫","slug":"时序推理，隐马尔可夫","link":"/tags/时序推理，隐马尔可夫/"},{"name":"目标跟踪","slug":"object-track","link":"/tags/object-track/"},{"name":"语义分割","slug":"semantic-segmentation","link":"/tags/semantic-segmentation/"},{"name":"均值漂移","slug":"mean-shift","link":"/tags/mean-shift/"},{"name":"滤波","slug":"filter","link":"/tags/filter/"},{"name":"粒子滤波","slug":"particle-filter","link":"/tags/particle-filter/"},{"name":"核函数","slug":"kernel-function","link":"/tags/kernel-function/"},{"name":"梯度下降","slug":"gradient-descent","link":"/tags/gradient-descent/"},{"name":"采样","slug":"samples","link":"/tags/samples/"},{"name":"人脸识别","slug":"人脸识别","link":"/tags/人脸识别/"},{"name":"零样本学习","slug":"zero-shot-learning","link":"/tags/zero-shot-learning/"},{"name":"卡尔曼滤波","slug":"kalman-filter","link":"/tags/kalman-filter/"},{"name":"时序推理","slug":"temporal-reasoning","link":"/tags/temporal-reasoning/"},{"name":"贝叶斯网络","slug":"bayesian-network","link":"/tags/bayesian-network/"},{"name":"傅里叶变换","slug":"fourier-transform","link":"/tags/fourier-transform/"},{"name":"边缘检测","slug":"edge-detection","link":"/tags/edge-detection/"},{"name":"图像特征","slug":"image-feature","link":"/tags/image-feature/"},{"name":"强化学习","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"自然语言处理","slug":"nlp","link":"/tags/nlp/"}],"categories":[{"name":"随笔","slug":"essay","link":"/categories/essay/"},{"name":"计算机视觉","slug":"computer-vision","link":"/categories/computer-vision/"},{"name":"算法与数据结构","slug":"algorithms","link":"/categories/algorithms/"},{"name":"技术","slug":"technique","link":"/categories/technique/"},{"name":"机器学习","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"深度学习","slug":"deep-learning","link":"/categories/deep-learning/"},{"name":"软件工程","slug":"software-engineering","link":"/categories/software-engineering/"},{"name":"学习","slug":"study","link":"/categories/study/"}]}